<?xml version="1.0"?>
<doc>
    <assembly>
        <name>MyCaffe</name>
    </assembly>
    <members>
        <member name="T:MyCaffe.common.BatchInput">
            <summary>
            The BatchInput class stores the mini-batch index and input data.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BatchInput.#ctor(System.Int32,System.Object)">
            <summary>
            The BatchInput constructor.
            </summary>
            <param name="nIdxBatch">Specifies the mini-batch index.</param>
            <param name="objInput">Specifies the mini-batch data.</param>
        </member>
        <member name="P:MyCaffe.common.BatchInput.BatchIndex">
            <summary>
            Returns the batch index.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BatchInput.InputData">
            <summary>
            Returns the input data.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BBoxUtility`1">
            <summary>
            The BBox class processes the NormalizedBBox data used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log)">
            <summary>
            The constructor.
            </summary>
            <param name="cuda">Specifies the connection to Cuda and cuDNN.</param>
            <param name="log">Specifies the output log.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Dispose">
            <summary>
            Clean up all resources.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ComputeAP(System.Collections.Generic.List{System.Tuple{System.Single,System.Int32}},System.Int32,System.Collections.Generic.List{System.Tuple{System.Single,System.Int32}},MyCaffe.basecode.ApVersion,System.Collections.Generic.List{System.Single}@,System.Collections.Generic.List{System.Single}@)">
            <summary>
            Compute the average precision given true positive and false positive vectors.
            </summary>
            <param name="rgTp">Specifies a list of scores and true positive.</param>
            <param name="nNumPos">Specifies the number of true positives.</param>
            <param name="rgFp">Specifies a list of scores and false positive.</param>
            <param name="apVersion">Specifies the different ways of computing the Average Precisions.
            @see [Tag: Average Precision](https://sanchom.wordpress.com/tag/average-precision/)
              
            Versions:
            11point: The 11-point interpolated average precision, used in VOC2007.
            MaxIntegral: maximally interpolated AP. Used in VOC2012/ILSVRC.
            Integral: the natrual integral of the precision-recall curve.
            </param>
            <param name="rgPrec">Returns the computed precisions.</param>
            <param name="rgRec">Returns the computed recalls.</param>
            <returns>The Average Precision value is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.CumSum(System.Collections.Generic.List{System.Tuple{System.Single,System.Int32}})">
            <summary>
            Calculate the cumulative sum of a set of pairs.
            </summary>
            <param name="rgPairs"></param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetTopKScoreIndex(System.Collections.Generic.List{System.Single},System.Collections.Generic.List{System.Int32},System.Int32)">
            <summary>
            Create the TopK ordered score list.
            </summary>
            <param name="rgScores">Specifies the scores.</param>
            <param name="rgIdx">Specifies the indexes.</param>
            <param name="nTopK">Specifies the top k items or -1 for all items.</param>
            <returns>The items listed by highest score is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetMaxScoreIndex(System.Collections.Generic.List{System.Single},System.Single,System.Int32)">
            <summary>
            Create the max ordered score list.
            </summary>
            <param name="rgScores">Specifies the scores.</param>
            <param name="fThreshold">Specifies the threshold of score to consider.</param>
            <param name="nTopK">Specifies the top k items or -1 for all items.</param>
            <returns>The items listed by highest score is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ApplyNMSFast(System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Single},System.Single,System.Single,System.Single,System.Int32,System.Collections.Generic.List{System.Int32}@)">
            <summary>
            Do a fast non maximum supression given bboxes and scores.
            </summary>
            <param name="rgBBoxes">Specifies a set of bounding boxes.</param>
            <param name="rgScores">Specifies a seto of corresponding confidences.</param>
            <param name="fScoreThreshold">Specifies the score threshold used in non maximum suppression.</param>
            <param name="fNmsThreshold">Specifies the nms threshold used in non maximum suppression.</param>
            <param name="fEta">Specifies the eta value.</param>
            <param name="nTopK">Specifies the top k picked indices or -1 for all.</param>
            <param name="rgIndices">Returns the kept indices of bboxes after nms.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ApplyNMS(System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Single},System.Single,System.Int32)">
            <summary>
            Do non maximum supression given bboxes and scores.
            </summary>
            <param name="rgBBoxes">Specifies a set of bounding boxes.</param>
            <param name="rgScores">Specifies a seto of corresponding confidences.</param>
            <param name="fThreshold">Specifies the threshold used in non maximum suppression.</param>
            <param name="nTopK">Specifies the top k picked indices or -1 for all.</param>
            <returns>The indices of the bboxes after nms are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ApplyNMS(System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Single},System.Single,System.Int32,System.Boolean,System.Collections.Generic.Dictionary{System.Int32,System.Collections.Generic.Dictionary{System.Int32,System.Single}}@)">
            <summary>
            Do non maximum supression given bboxes and scores.
            </summary>
            <param name="rgBBoxes">Specifies a set of bounding boxes.</param>
            <param name="rgScores">Specifies a seto of corresponding confidences.</param>
            <param name="fThreshold">Specifies the threshold used in non maximum suppression.</param>
            <param name="nTopK">Specifies the top k picked indices or -1 for all.</param>
            <param name="bReuseOverlaps">Specifies whether or not to use and update overlaps (true) or alwasy compute the overlap (false).</param>
            <param name="rgOverlaps">Returns the overlaps between pairs of bboxes if bReuseOverlaps is true.</param>
            <returns>The indices of the bboxes after nms are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetDetectionResults(System.Single[],System.Int32,System.Int32)">
            <summary>
            Get detection results from rgData.
            </summary>
            <param name="rgData">Specifies a 1 x 1 x nNumDet x 7 blob data.</param>
            <param name="nNumDet">Specifies the number of detections.</param>
            <param name="nBackgroundLabelId">Specifies the label for the background class which is used to do a
            sanity check so that no detection contains it.</param>
            <returns>The detection results are returned for each class from each image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetPrior(System.Single[],System.Int32,System.Collections.Generic.List{System.Collections.Generic.List{System.Single}}@)">
            <summary>
            Get the prior boundary boxes from the rgPriorData.
            </summary>
            <param name="rgPriorData">Specifies the prior data as a 1 x 2 x nNumPriors x 4 x 1 blob.</param>
            <param name="nNumPriors">Specifies the number of priors.</param>
            <param name="rgPriorVariances">Specifies the prior variances need by prior bboxes.</param>
            <returns>The prior bbox list is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ComputeConfLoss(System.Single[],System.Int32,System.Int32,System.Int32,System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType)">
            <summary>
            Compute the confidence loss for each prior from rgConfData.
            </summary>
            <param name="rgConfData">Specifies the nNum x nNumPredsPerClass * nNumClasses blob of confidence data.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPredsPerClass">Specifies the number of predictions per class.</param>
            <param name="nNumClasses">Specifies the number of classes.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="loss_type">Specifies the loss type used to compute the confidence.</param>
            <returns>The confidence loss values are returned with confidence loss per location for each image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ComputeConfLoss(System.Single[],System.Int32,System.Int32,System.Int32,System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}})">
            <summary>
            Compute the confidence loss for each prior from rgConfData.
            </summary>
            <param name="rgConfData">Specifies the nNum x nNumPredsPerClass * nNumClasses blob of confidence data.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPredsPerClass">Specifies the number of predictions per class.</param>
            <param name="nNumClasses">Specifies the number of classes.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="loss_type">Specifies the loss type used to compute the confidence.</param>
            <param name="rgAllMatchIndices">Specifies all match indices storing a mapping between predictions and ground truth.</param>
            <param name="rgAllGtBoxes">Specifies all ground truth bboxes from the batch.</param>
            <returns>The confidence loss values are returned with confidence loss per location for each image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ComputeConfLoss(System.Single[],System.Int32,System.Int32,System.Int32,System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType,System.Collections.Generic.List{System.Collections.Generic.Dictionary{System.Int32,System.Collections.Generic.List{System.Int32}}},System.Collections.Generic.Dictionary{System.Int32,System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}})">
            <summary>
            Compute the confidence loss for each prior from rgConfData.
            </summary>
            <param name="rgConfData">Specifies the nNum x nNumPredsPerClass * nNumClasses blob of confidence data.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPredsPerClass">Specifies the number of predictions per class.</param>
            <param name="nNumClasses">Specifies the number of classes.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="loss_type">Specifies the loss type used to compute the confidence.</param>
            <param name="rgAllMatchIndices">Specifies all match indices storing a mapping between predictions and ground truth.</param>
            <param name="rgAllGtBoxes">Specifies all ground truth bboxes from the batch.</param>
            <returns>The confidence loss values are returned with confidence loss per location for each image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetConfidenceScores(System.Single[],System.Int32,System.Int32,System.Int32)">
            <summary>
            Calculate the confidence predictions from rgConfData.
            </summary>
            <param name="rgConfData">Specifies the nNum x nNumPredsPerClass * nNumClasses blob of confidence data.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPredsPerClass">Specifies the number of predictions per class.</param>
            <param name="nNumClasses">Specifies the number of classes.</param>
            <returns>The confidence scores are returned as the confidence predictions which contains a confidence prediction for an image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetLocPredictions(System.Single[],System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Create a set of local predictions from the rgLocData.
            </summary>
            <param name="rgLocData">Specifies the nNum x nNumPredsPerClass * nNumLocClasses * 4 blbo with prediction initialization data.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPredsPerClass">Specifies the number of predictions per class.</param>
            <param name="nNumLocClasses">Specifies the number of local classes  It is 1 if bShareLocation is true; and it is equal to the number
            of classes needed to predict otherwise.</param>
            <param name="bShareLocation">Specifies whether or not to share the location.  If true, all classes share the same location prediction.</param>
            <returns>A list of created location predictions is returned as a list of LabelBBox items where each item contains a location prediction
            for the image.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetGroundTruth(System.Single[],System.Int32,System.Int32,System.Boolean)">
            <summary>
            Create a set of ground truth bounding boxes from the rgGtData.
            </summary>
            <param name="rgGtData">Specifies the 1 x 1 x nNumGt x 7 blob with ground truth initialization data.</param>
            <param name="nNumGt">Specifies the number of ground truths.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="bUseDifficultGt">Specifies whether or not to use the difficult ground truth.</param>
            <returns>A dictionary containing the ground truth's (one per image) is returned with the label of each bbox stored in the NormalizedBBox.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.GetGroundTruthEx(System.Single[],System.Int32,System.Int32,System.Boolean)">
            <summary>
            Create a set of ground truth bounding boxes from the rgGtData.
            </summary>
            <param name="rgGtData">Specifies the 1 x 1 x nNumGt x 7 blob with ground truth initialization data.</param>
            <param name="nNumGt">Specifies the number of ground truths.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="bUseDifficultGt">Specifies whether or not to use the difficult ground truth.</param>
            <returns>A dictionary containing the ground truths per label is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Match(System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter.MatchType,System.Single,System.Boolean,System.Collections.Generic.List{System.Int32}@,System.Collections.Generic.List{System.Single}@)">
            <summary>
            Find matches between a list of two bounding boxes.
            </summary>
            <param name="rgGtBboxes">Specifies a list of ground truth bounding boxes.</param>
            <param name="rgPredBboxes">Specifies a list of predicted bounding boxes.</param>
            <param name="nLabel">Specifies the label.</param>
            <param name="match_type">Specifies the matching type.</param>
            <param name="fOverlapThreshold">Specifies the overlap.</param>
            <param name="bIgnoreCrossBoundaryBbox">Specifies whether or not to ignore corss boundary bounding boxes.</param>
            <param name="rgMatchIndices">Specifies the list where the indexes of matches are placed.</param>
            <param name="rgMatchOverlaps">Specifies the list where the overlaps of matches are placed.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.IsCrossBoundary(MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Returns whether or not the bbox is overlaps outside the range [0,1]
            </summary>
            <param name="bbox">Specifies the bounding box to test.</param>
            <returns>If the bbox overlaps, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.DecodeAll(System.Collections.Generic.List{MyCaffe.basecode.LabelBBox},System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Collections.Generic.List{System.Single}},System.Int32,System.Boolean,System.Int32,System.Int32,MyCaffe.param.ssd.PriorBoxParameter.CodeType,System.Boolean,System.Boolean)">
            <summary>
            Decode all bboxes in a batch.
            </summary>
            <param name="rgAllLocPreds">Specifies the batch of local predictions.</param>
            <param name="rgPriorBboxes">Specifies the set of prior bboxes.</param>
            <param name="rgrgfPrioVariances">Specifies the prior variances.</param>
            <param name="nNum">Specifies the number of items in the batch.</param>
            <param name="bShareLocation">Specifies whether or not to share locations.</param>
            <param name="nNumLocClasses">Specifies the number of local classes.</param>
            <param name="nBackgroundLabelId">Specifies the background label.</param>
            <param name="codeType">Specifies the coding type.</param>
            <param name="bVarianceEncodedInTarget">Specifies whether or not the variance is encoded in the target or not.</param>
            <param name="bClip">Specifies whether or not to clip.</param>
            <returns>The decoded Bboxes are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Decode(System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Collections.Generic.List{System.Single}},MyCaffe.param.ssd.PriorBoxParameter.CodeType,System.Boolean,System.Boolean,System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox})">
            <summary>
            Decode a set of bounding box.
            </summary>
            <param name="rgPriorBbox">Specifies an list of prior bounding boxs.</param>
            <param name="rgrgfPriorVariance">Specifies the list of prior variance (must have 4 elements each of which are > 0).</param>
            <param name="code_type">Specifies the code type.</param>
            <param name="bEncodeVarianceInTarget">Specifies whether or not to encode the variance in the target.</param>
            <param name="bClip">Specifies to enable/disable the clipping.</param>
            <param name="rgBbox">Specifies a list of bounding boxs.</param>
            <returns>A list of decoded bounding box is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Decode(MyCaffe.basecode.NormalizedBBox,System.Collections.Generic.List{System.Single},MyCaffe.param.ssd.PriorBoxParameter.CodeType,System.Boolean,System.Boolean,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Decode a bounding box.
            </summary>
            <param name="prior_bbox">Specifies the prior bounding box.</param>
            <param name="rgfPriorVariance">Specifies the prior variance (must have 4 elements each of which are > 0).</param>
            <param name="code_type">Specifies the code type.</param>
            <param name="bEncodeVarianceInTarget">Specifies whether or not to encode the variance in the target.</param>
            <param name="bClip">Specifies whether or not to enable clip or not.</param>
            <param name="bbox">Specifies the bounding box.</param>
            <returns>The decoded bounding box is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Encode(MyCaffe.basecode.NormalizedBBox,System.Collections.Generic.List{System.Single},MyCaffe.param.ssd.PriorBoxParameter.CodeType,System.Boolean,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Encode a bounding box.
            </summary>
            <param name="prior_bbox">Specifies the prior bounding box.</param>
            <param name="rgfPriorVariance">Specifies the prior variance (must have 4 elements each of which are > 0).</param>
            <param name="code_type">Specifies the code type.</param>
            <param name="bEncodeVarianceInTarget">Specifies whether or not to encode the variance in the target.</param>
            <param name="bbox">Specifies the bounding box.</param>
            <returns>The encoded bounding box is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.MeetEmitConstraint(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox,MyCaffe.param.ssd.EmitConstraint)">
            <summary>
            Check if a bbox meets the emit constraint w.r.t the src_bbox.
            </summary>
            <param name="src_bbox">Specifies the source Bbox.</param>
            <param name="bbox">Specifies the Bbox to check.</param>
            <param name="emit_constraint">Specifies the emit constraint.</param>
            <returns>If the emit constraint is met, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Coverage(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Compute the coverage of bbox1 by bbox2.
            </summary>
            <param name="bbox1">Specifies the first BBox.</param>
            <param name="bbox2">Specifies the second BBox.</param>
            <returns>The coverage of bbox1 by bbox2 is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Locate(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Locate bbox in the coordinate system of the source Bbox.
            </summary>
            <param name="srcBbox">Specifies the source Bbox.</param>
            <param name="bbox">Specifies the bbox to locate.</param>
            <returns>The bbox located within the source Bbox is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.JaccardOverlap(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox,System.Boolean)">
            <summary>
            Calculates the Jaccard overlap between two bounding boxes.
            </summary>
            <param name="bbox1">Specifies the first bounding box.</param>
            <param name="bbox2">Specifies the second bounding box.</param>
            <param name="bNormalized">Specifies whether or not the bboxes are normalized or not.</param>
            <returns>The Jaccard overlap is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Output(MyCaffe.basecode.NormalizedBBox,System.Drawing.SizeF,MyCaffe.param.ssd.ResizeParameter)">
            <summary>
            Output the predicted bbox on the actual image.
            </summary>
            <param name="bbox">Specifies the bbox.</param>
            <param name="szImg">Specifies the image size.</param>
            <param name="p">Specifies the resize parameter.</param>
            <returns>The predicted bbox is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Project(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox@)">
            <summary>
            Project one bbox onto another.
            </summary>
            <param name="src">Specifies the source bbox.</param>
            <param name="bbox">Specifies the second bbox.</param>
            <param name="proj_bbox">Returns the projected bbox here.</param>
            <returns>The new project bbox is returned if a projection was made, otherwise the original bbox is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Extrapolate(MyCaffe.param.ssd.ResizeParameter,System.Int32,System.Int32,MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Extrapolate the transformed bbox if height_scale and width_scale are explicitly
            provied, and the FIT_SMALL_SIZE resize mode is specified.
            </summary>
            <param name="param">Specifies the resize parameter.</param>
            <param name="nHeight">Specifies the height.</param>
            <param name="nWidth">Specifies the width.</param>
            <param name="crop_bbox">Specifies the crop Bbox.</param>
            <param name="bbox">Specifies the Bbox to be updated.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Intersect(MyCaffe.basecode.NormalizedBBox,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Create the intersection of two bounding boxes.
            </summary>
            <param name="bbox1">Specifies the first bounding box.</param>
            <param name="bbox2">Specifies the second bounding box.</param>
            <returns>The intersection of the two bounding boxes is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Clip(MyCaffe.basecode.NormalizedBBox,System.Single,System.Single)">
            <summary>
            Clip the BBox to a set range.
            </summary>
            <param name="bbox">Specifies the input bounding box.</param>
            <param name="fHeight">Specifies the clipping height.</param>
            <param name="fWidth">Specifies the clipping width.</param>
            <returns>A new, clipped NormalizedBBox is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Scale(MyCaffe.basecode.NormalizedBBox,System.Int32,System.Int32)">
            <summary>
            Scale the BBox to a set range.
            </summary>
            <param name="bbox">Specifies the input bounding box.</param>
            <param name="nHeight">Specifies the scaling height.</param>
            <param name="nWidth">Specifies the scaling width.</param>
            <returns>A new, scaled NormalizedBBox is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.Size(MyCaffe.basecode.NormalizedBBox,System.Boolean)">
            <summary>
            Calculate the size of a BBox.
            </summary>
            <param name="bbox">Specifies the input bounding box.</param>
            <param name="bNormalized">Specifies whether or not the bounding box falls within the range [0,1].</param>
            <returns>The size of the bounding box is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.FindMatches(System.Collections.Generic.List{MyCaffe.basecode.LabelBBox},MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}},System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Collections.Generic.List{System.Single}},MyCaffe.param.ssd.MultiBoxLossParameter,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Single}}}@,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}}@)">
            <summary>
            Find matches between prediction bboxes and ground truth bboxes.
            </summary>
            <param name="rgAllLocPreds">Specifies the location prediction, where each item contains location prediction for an image.</param>
            <param name="rgAllGtBboxes">Specifies the ground truth bboxes for the batch.</param>
            <param name="rgPriorBboxes">Specifies all prior bboxes in the format of NormalizedBBox.</param>
            <param name="rgrgPriorVariances">Specifies all the variances needed by prior bboxes.</param>
            <param name="p">Specifies the parameter for the MultiBoxLossLayer.</param>
            <param name="rgAllMatchOverlaps">Returns the jaccard overlaps between predictions and ground truth.</param>
            <param name="rgAllMatchIndices">Returns the mapping between predictions and ground truth.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.CountNumMatches(System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},System.Int32)">
            <summary>
            Counts the number of matches in the list of maps.
            </summary>
            <param name="rgAllMatchIndices">Specifies the list of match indices.</param>
            <param name="nNum">Specifies the number of items.</param>
            <returns>The total matches found in the number of items is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.IsEligibleMining(MyCaffe.param.ssd.MultiBoxLossParameter.MiningType,System.Int32,System.Single,System.Single)">
            <summary>
            Returns whether or not mining is eligible given the mining type and match index.
            </summary>
            <param name="miningType">Specifies the mining type.</param>
            <param name="nMatchIdx">Specifies the matching index.</param>
            <param name="fMatchOverlap">Specifies the matching overlap.</param>
            <param name="fNegOverlap">Specifies the negative overlap.</param>
            <returns>If mining is allowed, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.MineHardExamples(MyCaffe.common.Blob{`0},System.Collections.Generic.List{MyCaffe.basecode.LabelBBox},MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}},System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Collections.Generic.List{System.Single}},System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Single}}},MyCaffe.param.ssd.MultiBoxLossParameter,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},System.Collections.Generic.List{System.Collections.Generic.List{System.Int32}},System.Int32@)">
            <summary>
            Mine the hard examples from the batch.
            </summary>
            <param name="blobConf">Specifies the confidence prediction.</param>
            <param name="rgAllLocPreds">Specifies the location prediction, where each item contains the location prediction for an image.</param>
            <param name="rgAllGtBBoxes">Specifies the ground truth bboxes for the batch.</param>
            <param name="rgPriorBboxes">Specifies the prior bboxes in the format of NormalizedBBox.</param>
            <param name="rgrgPriorVariances">Specifies the variances needed by the prior bboxes.</param>
            <param name="rgAllMatchOverlaps">Specifies the jaccard overlap between predictions and the ground truth.</param>
            <param name="p">Specifies the parameters for the MultiBoxLossLayer.</param>
            <param name="rgAllMatchIndices">Specifies the mapping between predictions and the ground truth.</param>
            <param name="rgAllNegIndices">Specifies the indices for negative samples.</param>
            <param name="nNumNegs">Specifies the numberof negative indices.</param>
            <returns>The number of matches is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.EncodeLocPrediction(System.Collections.Generic.List{MyCaffe.basecode.LabelBBox},MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}},System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},System.Collections.Generic.List{System.Collections.Generic.List{System.Single}},MyCaffe.param.ssd.MultiBoxLossParameter,MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0})">
            <summary>
            Encode the localization prediction and ground truth for each matched prior.
            </summary>
            <param name="rgAllLocPreds">Specifies the location prediction, where each item contains the location prediction for an image.</param>
            <param name="rgAllGtBboxes">Specifies the ground truth bboxes for the batch.</param>
            <param name="rgAllMatchIndices">Specifies the mapping between predictions and the ground truth.</param>
            <param name="rgPriorBboxes">Specifies the prior bboxes in the format of NormalizedBBox.</param>
            <param name="rgrgPriorVariances">Specifies the variances needed by the prior bboxes.</param>
            <param name="p">Specifies the parameters for the MultiBoxLossLayer.</param>
            <param name="blobLocPred">Specifies the location prediction results.</param>
            <param name="blobLocGt">Specifies the encoded location ground truth.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.EncodeConfPrediction(System.Single[],System.Int32,System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},System.Collections.Generic.List{System.Collections.Generic.List{System.Int32}},MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox}},MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0})">
            <summary>
            Encode the confidence predictions and ground truth for each matched prior.
            </summary>
            <param name="rgfConfData">Specifies the num x num_priors * num_classes blob.</param>
            <param name="nNum">Specifies the number of images.</param>
            <param name="nNumPriors">Specifies the number of priors (predictions) per image.</param>
            <param name="p">Specifies the parameters for the MultiBoxLossLayer.</param>
            <param name="rgAllMatchIndices">Specifies the mapping between predictions and the ground truth.</param>
            <param name="rgAllNegIndices">Specifies the indices for negative samples.</param>
            <param name="rgAllGtBBoxes">Specifies the ground truth bboxes for the batch.</param>
            <param name="blobConfPred">Specifies the confidence prediction results.</param>
            <param name="blobConfGt">Specifies the confidence ground truth.</param>
        </member>
        <member name="M:MyCaffe.common.BBoxUtility`1.ComputeLocLoss(MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0},System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}},System.Int32,System.Int32,MyCaffe.param.ssd.MultiBoxLossParameter.LocLossType)">
            <summary>
            Compute the localization loss per matched prior.
            </summary>
            <param name="blobLocPred">Specifies the location prediction results.</param>
            <param name="blobLocGt">Specifies the encoded location ground truth.</param>
            <param name="rgAllMatchIndices">Specifies the mapping between predictions and the ground truth.</param>
            <param name="nNum">Specifies the number of images in the batch.</param>
            <param name="nNumPriors">Specifies the total number of priors.</param>
            <param name="lossType">Specifies the type of localization loss, Smooth_L1 or L2.</param>
            <returns>Returns the localization loss for all priors in the batch.</returns>
        </member>
        <member name="T:MyCaffe.common.Blob`1">
            <summary>
            The Blob is the main holder of data that moves through the Layers of the Net.
            </summary>
            <remarks>
            Each blob holds Data and optionally Diff where the data is passed through the Net
            Layers on each forward pass, and the Diff contains the errors passed backward
            throgh the Net Layers on the backward pass.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.common.Blob`1.MAX_BLOB_AXES">
            <summary>
            Defines the maximum number of Axes supported by the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Boolean,System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="bIncludeDiff">Optionally, specifies whether or not to include (and allocate) the Diff data.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            <b>DEPRECIATED</b>; use <code>Blob(...,rgShape)</code> instead.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="nNum">Specifies the number of inputs.</param>
            <param name="nChannels">Specifies the number of channels per input.</param>
            <param name="nHeight">Specifies the height of each input.</param>
            <param name="nWidth">Specifies the width of each input.</param>
            <param name="bIncludeDiff">Optionally, specifies whether or not to include (and allocate) the Diff data.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Collections.Generic.List{System.Int32},System.Boolean,System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="rgShape">Specifies the shape of each axis of the Blob.</param>
            <param name="bIncludeDiff">Optionally, specifies whether or not to include (and allocate) the Diff data.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Int32[],System.Boolean,System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="rgShape">Specifies the shape of each axis of the Blob.</param>
            <param name="bIncludeDiff">Optionally, specifies whether or not to include (and allocate) the Diff data.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="b">Create this blob to be like another Blob (e.g. same shape).</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.basecode.SimpleDatum,System.Boolean,System.Boolean,System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="d">Specifies the datum for which the Blob is shaped to match.</param>
            <param name="bCopyData">Optionally, specifies whether or not to actually copy the data.  When <i>false</i>, the shape is set, but no data is copied.</param>
            <param name="bIncludeDiff">Optionally, specifies whether or not to include (and allocate) the Diff data.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.BlobProto,System.Boolean)">
            <summary>
            The Blob constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="bp">Specifies the BlobProto used to load the Blob.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetConversionWorkSize(System.Boolean)">
            <summary>
            Returns the amount of memory (in bytes) required to convert from base to half and back.
            </summary>
            <param name="bUseHalfSize">Specifies whether or not we are converting to half size or not.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ConvertToHalf(System.Int64,System.UInt64,System.Boolean,System.Boolean)">
            <summary>
            Converts this blob from its base type to the half type.
            </summary>
            <param name="hWorkMem">Specifies the work memory.</param>
            <param name="lWorkSize">Specifies the work size.</param>
            <param name="bData">Specifies to convert the data.</param>
            <param name="bDiff">Specifies to convert the diff</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ConvertToBase(System.Int64,System.UInt64,System.Boolean,System.Boolean)">
            <summary>
            Converts this blob from the half type to the base type.
            </summary>
            <param name="hWorkMem">Specifies the work memory.</param>
            <param name="lWorkSize">Specifies the work size.</param>
            <param name="bData">Specifies to convert the data.</param>
            <param name="bDiff">Specifies to convert the diff</param>
        </member>
        <member name="P:MyCaffe.common.Blob`1.HalfSize">
            <summary>
            Returns whether or not this blob is using half sizes.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.freeze_learning">
            <summary>
            Specifies whether or not the diff is applied to the data during Update.  When freeze learning = <i>true</i>, the update is skipped.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.Cuda">
            <summary>
            Returns the CudaDnn object that manages the Blob's memory."/>
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.Log">
            <summary>
            Returns the Log associated with the blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Dispose(System.Boolean)">
            <summary>
            Releases all resources used by the Blob (including both GPU and Host).
            </summary>
            <param name="bDisposing">Set to <i>true</i> when disposing the object.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Dispose">
            <summary>
            Releases all resources used by the Blob (including both GPU and Host).
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Reshape(System.Int32,System.Int32,System.Int32,System.Int32,System.Nullable{System.Boolean})">
            <summary>
            <b>DEPRECIATED</b>; use <code>Reshape(rgShape)</code> instead.
            </summary>
            <param name="nNum"></param>
            <param name="nChannels"></param>
            <param name="nHeight"></param>
            <param name="nWidth"></param>
            <param name="bUseHalfSize">Optionally, specifies to use half sized memory.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Reshape(System.Collections.Generic.List{System.Int32},System.Nullable{System.Boolean})">
            <summary>
            Change the dimensions of the blob, allocating new memory if necessary.
            </summary>
            <remarks>
            This function can be called both to create an initial allocation
            of memory, and to adjust the dimensions of a top blob during Layer::Reshape
            or Layer::Forward.  When changing the size of blob, memory will only be
            reallocated if sufficient memory does not already exist, and excess memory
            will not be freed until Dispose is called.
            
            Note that reshaping an input blob and immediately calling Net::Backward is
            an error;  either Net::Forward or Net::Reshape need to be called to 
            propagate the new input shape to higher layers.
            </remarks>
            <param name="rgShape">Specifies the new shape.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half sized memory.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Reshape(System.Int32[],System.Nullable{System.Boolean})">
            <summary>
            Change the dimensions of the blob, allocating new memory if necessary.
            </summary>
            <remarks>
            This function can be called both to create an initial allocation
            of memory, and to adjust the dimensions of a top blob during Layer::Reshape
            or Layer::Forward.  When changing the size of blob, memory will only be
            reallocated if sufficient memory does not already exist, and excess memory
            will not be freed until Dispose is called.
            
            Note that reshaping an input blob and immediately calling Net::Backward is
            an error;  either Net::Forward or Net::Reshape need to be called to 
            propagate the new input shape to higher layers.
            </remarks>
            <param name="rgShape">Specifies the new shape.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half sized memory.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Reshape(MyCaffe.param.BlobShape,System.Nullable{System.Boolean})">
            <summary>
            Change the dimensions of the blob, allocating new memory if necessary.
            </summary>
            <remarks>
            This function can be called both to create an initial allocation
            of memory, and to adjust the dimensions of a top blob during Layer::Reshape
            or Layer::Forward.  When changing the size of blob, memory will only be
            reallocated if sufficient memory does not already exist, and excess memory
            will not be freed until Dispose is called.
            
            Note that reshaping an input blob and immediately calling Net::Backward is
            an error;  either Net::Forward or Net::Reshape need to be called to 
            propagate the new input shape to higher layers.
            </remarks>
            <param name="shape">Specifies the new shape of the Blob.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half sized memory.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ReshapeLike(MyCaffe.common.Blob{`0},System.Nullable{System.Boolean})">
            <summary>
            Reshape this Blob to have the same shape as another Blob.
            </summary>
            <param name="b">Specifies the other Blob.</param>
            <param name="bUseHalfSize">Optionally, specifies to use half sized memory.</param>
        </member>
        <member name="P:MyCaffe.common.Blob`1.shape_string">
            <summary>
            Returns a string describing the Blob's shape.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.DiffExists">
            <summary>
            Returns whether or not the Diff portion exists.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.shape">
            <summary>
            Returns an array where each element contains the shape of an axis of the Blob.
            </summary>
            <returns>The shape array is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.shape(System.Int32)">
            <summary>
            Returns the dimension of the nIdx'th axis (or the negative nIdx'th
            axis from teh end, if nIdx is negative.
            </summary>
            <param name="nIdx">The axis index, which may be negative as it will be
            'canonicalized' using CanonicalAxisIndex.</param>
            <returns></returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.num_axes">
            <summary>
            Returns the number of axes in the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.count">
            <summary>
            Returns the total number of items in the Blob.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.count(System.Int32,System.Int32)">
            <summary>
            Compute the volume of a slice; i.e., the product of dimensions
            among a range of axes.
            </summary>
            <param name="nStartIdx">The first axis to include in the slice.</param>
            <param name="nEndIdx">The first axis to exclude from the slice.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.count(System.Int32)">
            <summary>
            Compute the volume of a slice spanning from a particular first axis to the final
            axis.
            </summary>
            <param name="nStartIdx">The first axis to include in the slice.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.CanonicalAxisIndex(System.Int32)">
            <summary>
            Returns the 'canonical' version of a (usually) user-specified axis,
            allowing for negative indexing (e.g., -1 for the last axis).
            </summary>
            <param name="nIdx">The axis index. 
            </param>
            <returns>The zero based index is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.num">
            <summary>
            <b>DEPRECIATED</b>; legacy shape accessor num: use shape(0) instead.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.channels">
            <summary>
            <b>DEPRECIATED</b>; legacy shape accessor channels: use shape(1) instead.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.height">
            <summary>
            <b>DEPRECIATED</b>; legacy shape accessor height: use shape(2) instead.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.width">
            <summary>
            <b>DEPRECIATED</b>; legacy shape accessor width: use shape(3) instead.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.LegacyShape(System.Int32)">
            <summary>
            Returns the legacy shape at a given axis.
            </summary>
            <param name="nIdx">Specifies the axis.</param>
            <returns>The shape at a given axis is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.offset(System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Returns the flat offset given the number, channel, height and width.
            </summary>
            <param name="n">Specifies the num.</param>
            <param name="c">Specifies the channel.</param>
            <param name="h">Specifies the height.</param>
            <param name="w">Specifies the width.</param>
            <returns>The flat offset is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.offset(System.Collections.Generic.List{System.Int32})">
            <summary>
            Returns the flat offset given the array of axes values.
            </summary>
            <param name="rgIdx">Specifies the array of axes values.</param>
            <returns>The flat offset is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.CopyFrom(MyCaffe.common.Blob{`0},System.Int32,System.Int32,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Copy from a source Blob.
            </summary>
            <param name="src">The Blob to copy from.</param>
            <param name="nSrcOffset">The offset into the source data to copy.</param>
            <param name="nDstOffset">The offset into the destination data to copy.</param>
            <param name="nCount">The number of items to copy.</param>
            <param name="bCopyData">Copy the data.</param>
            <param name="bCopyDiff">Copy the diff.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.CopyFrom(MyCaffe.common.Blob{`0},System.Boolean,System.Boolean,System.Int64,System.Boolean)">
            <summary>
            Copy from a source Blob.
            </summary>
            <param name="src">The Blob to copy from.</param>
            <param name="bCopyDiff">If false, copy the data; if true, copy the diff.</param>
            <param name="bReshape">If false, require this Blob to be pre-shaped to the shape
            of other (and die otherwise); If true, Reshape this Blob to other's shape if
            necessary.</param>
            <param name="hDstHostBuffer">Optionally, specifies the host buffer of the destination.</param>
            <param name="bIgnoreShape">Optionally, specifies to ignore the shape and just make sure the count is the same before copying (default = false).</param>
            <returns>
            When used, the host buffer handle is returned.
            </returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.CopyFromAndTransposeHeightWidth(MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            Copy from a source Blob and transpose the height and width of the copy.
            </summary>
            <param name="blobSrc">The Blob to copy from.</param>
            <param name="bCopyDiff">If false, copy the data; if true, copy the diff.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.data_at(System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Returns the data at a given location in the Blob.
            </summary>
            <param name="n">Specifies the num.</param>
            <param name="c">Specifies the channel.</param>
            <param name="h">Specifies the height.</param>
            <param name="w">Specifies the width.</param>
            <returns>The data item at the location is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.diff_at(System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Returns the diff at a given location in the Blob.
            </summary>
            <param name="n">Specifies the num.</param>
            <param name="c">Specifies the channel.</param>
            <param name="h">Specifies the height.</param>
            <param name="w">Specifies the width.</param>
            <returns>The diff item at the location is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.data_at(System.Collections.Generic.List{System.Int32})">
            <summary>
            Returns the data at a given location in the Blob.
            </summary>
            <param name="rgIdx">Specifies an array of axes.</param>
            <returns>The data item at the location is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.diff_at(System.Collections.Generic.List{System.Int32})">
            <summary>
            Returns the diff at a given location in the Blob.
            </summary>
            <param name="rgIdx">Specifies an array of axes.</param>
            <returns>The diff item at the location is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.data">
            <summary>
            Returns the SyncedMemory that stores the data.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.diff">
            <summary>
            Returns the SyncedMemory that stores the diff.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.cpu_data">
            <summary>
            Returns the last host data retrieved from the GPU.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.mutable_cpu_data">
            <summary>
            Get data from the GPU and bring it over to the host, or
            Set data from the Host and send it over to the GPU.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.update_cpu_data">
            <summary>
            Update the CPU data by transferring the GPU data over to the Host.
            </summary>
            <returns>The Host data is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.gpu_data">
            <summary>
            Returns the data GPU handle used by the CudaDnn connection.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.mutable_gpu_data">
            <summary>
            Returns the data GPU handle used by the CudaDnn connection.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.cpu_diff">
            <summary>
            Returns the last host diff retrieved from the GPU.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.mutable_cpu_diff">
            <summary>
            Get diff from the GPU and bring it over to the host, or
            Set diff from the Host and send it over to the GPU.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.update_cpu_diff">
            <summary>
            Update the CPU diff by transferring the GPU diff over to the Host.
            </summary>
            <returns>The Host diff is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.gpu_diff">
            <summary>
            Returns the diff GPU handle used by the CudaDnn connection.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.mutable_gpu_diff">
            <summary>
            Returns the diff GPU handle used by the CudaDnn connection.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.gpu_shape">
            <summary>
            Returns the shape GPU handle used by the CudaDnn connection.  The shape data contains 
            the shape information of the Blob for use in GPU operations.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Update">
            <summary>
            The 'update' method is used for parameter blobs in a Net.
            </summary>
            <remarks>
            Update is called to apply the diff errors to the data.  When !bIncludeDiff or freeze_learning = <i>true</i>, no diff is applied.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.Blob`1.FromProto(MyCaffe.param.BlobProto,System.Boolean)">
            <summary>
            Create a new Blob from a given BlobProto.
            </summary>
            <param name="bp">Specifies the BlobProto to use.</param>
            <param name="bReshape">Specifies whether or not to reshape the Blob with the BlobProto shape.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ToProto(System.Boolean)">
            <summary>
            Writes the Blob to a new BlobProto.
            </summary>
            <param name="bWriteDiff">When <i>true</i>, the diff is written to the BlobProto, otherwise the data is written.</param>
            <returns>The new BlobProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.asum_data">
            <summary>
            Compute the sum of absolute values (L1 norm) of the data.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.asum_diff">
            <summary>
            Compute the sum of absolute values (L1 norm) of the diff.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.sumsq_data">
            <summary>
            Calcualte the sum of squares (L2 norm squared) of the data.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.sumsq_diff">
            <summary>
            Calcualte the sum of squares (L2 norm squared) of the diff.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.scale_data(System.Double)">
            <summary>
            Scale the data by a scaling factor.
            </summary>
            <param name="df">Specifies the scaling factor.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.scale_diff(System.Double)">
            <summary>
            Scale the diff by a scaling factor.
            </summary>
            <param name="df">Specifies the scaling factor.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.scale_data(`0)">
            <summary>
            Scale the blob data by a constant factor.
            </summary>
            <param name="fScaleFactor">Specifies the scaling factor.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.scale_diff(`0)">
            <summary>
            Scale the blob diff by a constant factor.
            </summary>
            <param name="fScaleFactor">Specifies the scaling factor.</param>
        </member>
        <member name="P:MyCaffe.common.Blob`1.reshape_when_sharing">
            <summary>
            When true, this Blob is reshaped to the source when sharing the source data (default = false).
            </summary>
            <remarks>
            This setting is used by the Net when sharing trained weights.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ShareData(MyCaffe.common.Blob{`0})">
            <summary>
            Set the data to point to the data of the other blob -- useful in Layers which
            simply perform a copy in their forward pass.
            </summary>
            <param name="b"></param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ShareDiff(MyCaffe.common.Blob{`0})">
            <summary>
            Set the diff to point to the diff of the other blob -- useful in Layers which
            simply perform a copy in their forward pass.
            </summary>
            <param name="b"></param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Share(MyCaffe.common.Blob{`0})">
            <summary>
            Share another Blob with this one, by setting the data and diff to the same data and diff of the other Blob.
            </summary>
            <param name="b">Specifies the other Blob to share.</param>
        </member>
        <member name="P:MyCaffe.common.Blob`1.snapshot_requested">
            <summary>
            Get/set the snapshot request.
            </summary>
            <remarks>
            This setting is used by learnable parameters that have requested a snapshot.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetData(System.Int32)">
            <summary>
            Returns the data at a given flat index within the Blob.
            </summary>
            <param name="nIdx">Specifies the flat index in the range [0,count()-1].</param>
            <returns>The data item at the index is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetDiff(System.Int32)">
            <summary>
            Returns the diff at a given flat index within the Blob.
            </summary>
            <param name="nIdx">Specifies the flat index in the range [0,count()-1].</param>
            <returns>The diff item at the index is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetData(`0[],System.Int32,System.Boolean)">
            <summary>
            Sets a number of items within the Blob's data.
            </summary>
            <param name="rgData">Specifies the data to set.</param>
            <param name="nCount">Optionally, specifies a subset count of items to set.</param>
            <param name="bSetCount">Optionally, specifies whether or not to set the count.  The count is always set when re-allocating the buffer.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetData(System.Double,System.Int32)">
            <summary>
            Either sets all of the data items in the Blob to a given value, or alternatively only sets a single
            indexed item to a given value.
            </summary>
            <param name="dfVal">Specifies the value to set.</param>
            <param name="nIdx">Optionally, specifies the index of the item to set.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetData(System.Double,System.Int32,System.Int32)">
            <summary>
            Set a data range with a given value.
            </summary>
            <param name="dfVal">Specifies the value to set.</param>
            <param name="nStartIdx">Specifies the start index.</param>
            <param name="nCount">Specifies the number of items to set.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetDiff(System.Double,System.Int32)">
            <summary>
            Either sets all of the diff items in the Blob to a given value, or alternatively only sets a single
            indexed item to a given value.
            </summary>
            <param name="dfVal">Specifies the value to set.</param>
            <param name="nIdx">Optionally, specifies the index of the item to set.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetDiff(`0[],System.Int32,System.Boolean)">
            <summary>
            Sets a number of items within the Blob's diff.
            </summary>
            <param name="rgDiff">Specifies the diff to set.</param>
            <param name="nCount">Optionally, specifies a subset count of items to set.</param>
            <param name="bSetCount">Optionally, specifies whether or not to set the count.  The count is always set when re-allocating the buffer.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetData(MyCaffe.basecode.SimpleDatum,System.Boolean,System.Boolean)">
            <summary>
            Sets the Blob values to the data contained within a SimpleDatum.
            </summary>
            <param name="d">Specifies the SimpleDatum.</param>
            <param name="bReshape">Specifies whether or not to reshape the Blob to match the SimpleDatum.</param>
            <param name="bCopyData">Optionally, specifies whether or not to transfer the data.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.SetCPUData(`0[])">
            <summary>
            Sets just the CPU data to the data specified.
            </summary>
            <param name="rg">Specifies the CPU data to set.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.AsyncGpuPush(System.Int64)">
            <summary>
            Asynchronously pushes the CPU data, previously set with SetCPUData, to the GPU.
            </summary>
            <param name="hStream">Specifies a handle to the Cuda Stream to use for synchronization.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ShapeEquals(MyCaffe.param.BlobProto)">
            <summary>
            Compares the shape of this blob to the shape within a BlobProto.
            </summary>
            <param name="bp">Specifies the BlobProto to compare.</param>
            <returns>If the shapes are the same this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.CompareShape(System.Collections.Generic.List{System.Int32})">
            <summary>
            Compares the shape of this blob to another shape.
            </summary>
            <param name="rgShape">Specifies the shape to compare with.</param>
            <returns>If the shapes are the same, return <i>true</i>, othewise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ToSizeString">
            <summary>
            Returns a string describing the 4D shape of the Blob.
            </summary>
            <returns>A shape string is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ToDatum">
            <summary>
            Returns a new Datum that contains the shape and data of the Blob.
            </summary>
            <returns>A Datum is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.Name">
            <summary>
            Get/set the name of the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Clone">
            <summary>
            Copies the Blob, including its data and diff.
            </summary>
            <returns>A new copy of the Blob is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.MathAdd(MyCaffe.common.Blob{`0},`0)">
            <summary>
            Clones the input Blob, scales the clone and then adds the data from this Blob to it.
            </summary>
            <remarks>
            Calculation: Y = Clone(blobA) * dfScale + this
            </remarks>
            <param name="blobA">Specifies the Blob to clone.</param>
            <param name="fScale">Specifies the scaling factor to apply to the clone.</param>
            <returns>The Blob copy is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.MathSub(MyCaffe.common.Blob{`0})">
            <summary>
            Clones the input Blob and subtracts the data from this blob from it.
            </summary>
            <remarks>
            Calculation: Y = Clone(blobA) + this
            </remarks>
            <param name="blobA">Specifies the Blob to clone.</param>
            <returns>The Blob copy is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.MathDiv(`0)">
            <summary>
            Clones the input Blob and divides a scalar from all of the clones data items.
            </summary>
            <remarks>
            Calculation: Y = Clone(this) * fScale
            </remarks>
            <param name="fScale">Specifies scaling factor.</param>
            <returns>The Blob copy is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Save(System.IO.BinaryWriter,System.Boolean,System.Boolean)">
            <summary>
            Saves this Blob to a binary stream.
            </summary>
            <param name="bw">Specifies the binary writer.</param>
            <param name="bData">Specifies whether or not to save the data.</param>
            <param name="bDiff">Specifies whether or not to save the diff.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Load(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.IO.BinaryReader,System.Boolean,System.Boolean)">
            <summary>
            Lods a new Blob from a binary reader.
            </summary>
            <param name="cuda">Specifies the instance of the CudaDnn connection to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="br">Specifies the binary reader.</param>
            <param name="bData">Specifies whether or not to read in the data.</param>
            <param name="bDiff">Specifies whether or not to read in the diff.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ToString">
            <summary>
            Returns a string representation of the Blob.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.ToString(System.Int32,System.Boolean)">
            <summary>
            Get the string representation containing up to the first 'nMax' items.
            </summary>
            <param name="nMax">Specifies the maximum number of data items to return.</param>
            <param name="bDiff">Specifies to returive the diff when <i>true</i>.</param>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.min_data">
            <summary>
            Returns the minimum value in the data of the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetMinData(System.Int64@)">
            <summary>
            Returns the minimum data and the position where the minimum is located in the data.
            </summary>
            <param name="lPos">Returns the location of the minimum.</param>
            <returns>Returns the minimum value.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.max_data">
            <summary>
            Returns the maximum value in the data of the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetMaxData(System.Int64@)">
            <summary>
            Returns the maximum data and the position where the maximum is located in the data.
            </summary>
            <param name="lPos">Returns the location of the maximum.</param>
            <returns>Returns the maximum value.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.min_diff">
            <summary>
            Returns the minimum value in the diff of the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetMinDiff(System.Int64@)">
            <summary>
            Returns the minimum diff and the position where the minimum is located in the diff.
            </summary>
            <param name="lPos">Returns the location of the minimum.</param>
            <returns>Returns the minimum value.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.max_diff">
            <summary>
            Returns the maximum value in the diff of the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.GetMaxDiff(System.Int64@)">
            <summary>
            Returns the maximum diff and the position where the maximum is located in the diff.
            </summary>
            <param name="lPos">Returns the location of the maximum.</param>
            <returns>Returns the maximum value.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.minmax_data(MyCaffe.common.Blob{`0},System.Boolean,System.Boolean)">
            <summary>
            Returns the minimum and maximum values in the data of the Blob.
            </summary>
            <param name="work">Specifies a workspace used to optimize the query.</param>
            <param name="bDetectNans">Optionally, specifies whether or not to detect Nan's and Infinity values.</param>
            <param name="bUseChunks">Optionally, specifies whether or not to use the min/max on all of the memory (default) or run in chunks (true).</param>
            <returns>A tuple containing the 'min', 'max' and optionally 'number of nans' and 'number of infinity' is returned for the data.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.minmax_diff(MyCaffe.common.Blob{`0},System.Boolean,System.Boolean)">
            <summary>
            Returns the minimum and maximum values in the diff of the Blob.
            </summary>
            <param name="work">Specifies a workspace used to optimize the query.</param>
            <param name="bDetectNans">Optionally, specifies whether or not to detect Nan's and Infinity values.</param>
            <param name="bUseChunks">Optionally, specifies whether or not to use the min/max on all of the memory (default) or run in chunks (true).</param>
            <returns>A tuple containing the 'min', 'max' and optionally 'number of nans' and 'number of infinity' is returned for the data.</returns>
        </member>
        <member name="P:MyCaffe.common.Blob`1.type">
            <summary>
            Returns the BLOB_TYPE of the Blob.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Blob`1.Tag">
            <summary>
            Returns a user defined object associated with the Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Blob`1.add_scalar(System.Double)">
            <summary>
            Adds a scalar value to the Blob.
            </summary>
            <param name="dfVal">Specifies the scalar value.</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.Resize(System.Collections.Generic.List{System.Int32})">
            <summary>
            The Resize method resizes the 3rd and 4th axes of the blob.
            </summary>
            <remarks>
            Currently, the Resize method only works on 4 axis blobs.  Resize is different from Reshape in that Resize
            actually averages the data when resizing the blob.
            </remarks>
            <param name="rgShape">Specifies the new shape to resize to.</param>
            <returns>A newly re-sized Blob is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.NormalizeData(System.Nullable{System.Double},System.Nullable{System.Double})">
            <summary>
            Normalize the blob data by subtracting the mean and dividing by the standard deviation.
            </summary>
            <param name="dfMean">Optionally, specifies a mean value to use (default = <i>null</i>).</param>
            <param name="dfStd">Optionally, specifies a std value to use (default = <i>null</i>).</param>
        </member>
        <member name="M:MyCaffe.common.Blob`1.mean(System.Single[],System.Boolean)">
            <summary>
            Calculate the mean of the blob data.
            </summary>
            <param name="rgDf">Optionally, specifies the CPU data to use (default = <i>null</i>).</param>
            <param name="bDiff">Optionally, specifies to use the diff instead of the data.</param>
            <returns>The mean is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Blob`1.std(System.Nullable{System.Double},System.Single[])">
            <summary>
            Calculate the standard deviation of the blob data.
            </summary>
            <param name="dfMean">Optionally, specifies the mean to use (default = <i>null</i>).</param>
            <param name="rgDf">Optionally, specifies the CPU data to calculate the std on (default = <i>null</i>).</param>
            <returns>The standard deviation of the bob data is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.BlobCollection`1">
            <summary>
            The BlobCollection contains a list of Blobs.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.#ctor">
            <summary>
            The BlobCollection constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobCollection`1.Count">
            <summary>
            Returns the number of items in the collection.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobCollection`1.Item(System.Int32)">
            <summary>
            Get/set an item within the collection at a given index.
            </summary>
            <param name="nIdx">Specifies the index of the item to access.</param>
            <returns>The item at the index is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.BlobCollection`1.Item(System.String)">
            <summary>
            Get the item with the specified name, or throw an exception if not found.
            </summary>
            <param name="strName">Specifies the name of the blob to find.</param>
            <returns>The blob with the matching name is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.FindBlob(System.String)">
            <summary>
            Finds a given blob in the collection based on its name.
            </summary>
            <param name="strName">Specifies the name of the blob to find.</param>
            <returns>If found, the blob is returned, othweriwse <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Insert(System.Int32,MyCaffe.common.Blob{`0})">
            <summary>
            Insert a blob at the given index.
            </summary>
            <param name="nIdx">Specifies the index where the blob should be added.</param>
            <param name="b">Specifies the blob to add.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Add(MyCaffe.common.Blob{`0})">
            <summary>
            Add a new Blob to the collection.
            </summary>
            <param name="b">Specifies the Blob to add.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Remove(MyCaffe.common.Blob{`0})">
            <summary>
            If it exists, remove a Blob from the collection.
            </summary>
            <param name="b">Specifies the Blob.</param>
            <returns>If the Blob is found and removed, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.RemoveAt(System.Int32)">
            <summary>
            Remove a Blob at a given index in the collection.
            </summary>
            <param name="nIdx">Specifies the index.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Clear(System.Boolean)">
            <summary>
            Remove all items from the collection.
            </summary>
            <param name="bDispose">Optionally, call Dispose on each item removed.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Contains(MyCaffe.common.Blob{`0})">
            <summary>
            Returns whether or not the collection contains a given blob.
            </summary>
            <param name="blob">Specifies the blob to look for.</param>
            <returns>If the blob is in the collection, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.FindRelatedBlobs(MyCaffe.common.Blob{`0})">
            <summary>
            Find all Blobs in the collection that contain (in part or in whole) the name of a given Blob.
            </summary>
            <param name="b">Specifies the Blob to look for.</param>
            <returns>A new collection of all Blobs that match are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.GetEnumerator">
            <summary>
            Get the enumerator for the collection.
            </summary>
            <returns>The enumerator is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.System#Collections#IEnumerable#GetEnumerator">
            <summary>
            Get the enumerator for the collection.
            </summary>
            <returns>The enumerator is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Clone">
            <summary>
            Copy the collection and return it as a new collection.
            </summary>
            <returns>The new collection is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.ReshapeLike(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshapes all blobs in the collection to the sizes of the source.
            </summary>
            <param name="src">Specifies the source collection to reshape to.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.CopyFrom(MyCaffe.common.BlobCollection{`0},System.Boolean)">
            <summary>
            Copy the data or diff from another BlobCollection into this one.
            </summary>
            <param name="bSrc">Specifies the src BlobCollection to copy.</param>
            <param name="bCopyDiff">Optionally, specifies to copy the diff instead of the data (default = <i>false</i>).</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Accumulate(MyCaffe.common.CudaDnn{`0},MyCaffe.common.BlobCollection{`0},System.Boolean)">
            <summary>
            Accumulate the diffs from one BlobCollection into another.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to add the blobs into this collection.</param>
            <param name="src">Specifies the source BlobCollection to add into this one.</param>
            <param name="bAccumulateDiff">Specifies to accumulate diffs when <i>true</i>, and the data otherwise.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.SetDiff(System.Double)">
            <summary>
            Set all blob diff to the value specified.
            </summary>
            <param name="df">Specifies the value to set all blob diff to.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.SetData(System.Double)">
            <summary>
            Set all blob data to the value specified.
            </summary>
            <param name="df">Specifies the value to set all blob data to.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.MathSub(MyCaffe.common.BlobCollection{`0},System.Boolean)">
            <summary>
            Create a new collection of cloned Blobs created by calling MathSub to subtract the Blobs in this collection from another collection.
            </summary>
            <remarks>
            Calculation: Y = Clone(colA) - this
            </remarks>
            <param name="col">Specifies the collection that this collection will be subtracted from.</param>
            <param name="bSkipFirstItem">Specifies whether or not to skip the first item.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.MathAdd(MyCaffe.common.BlobCollection{`0},System.Double,System.Boolean)">
            <summary>
            Create a new collection of cloned Blobs created by calling MathAdd to add the Blobs in this collection to another collection.
            </summary>
            <remarks>
            Calculation: Y = Clone(colA) * dfScale + this
            </remarks>
            <param name="colA">Specifies the collection that will be cloned.</param>
            <param name="dfScale">Specifies the scale that will be applied to the clone of this collection</param>
            <param name="bSkipFirstItem">Specifies whether or not to skip the first item.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.MathDiv(System.Double,System.Boolean)">
            <summary>
            Create a new collection of cloned Blobs created by calling MathDif to divide the Blobs in this collection with a scalar.
            </summary>
            <param name="dfVal">Specifies the divisor.</param>
            <param name="bSkipFirstItem">Specifies whether or not to skip the first item.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Save(System.IO.BinaryWriter,System.Boolean,System.Boolean)">
            <summary>
            Save the collection to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer.</param>
            <param name="bData">Specifies whether or not to save the data.</param>
            <param name="bDiff">Specifies whether or not to save the diff.</param>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Load(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.IO.BinaryReader,System.Boolean,System.Boolean)">
            <summary>
            Loads a new collection from a binary reader.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use for the Cuda connection.</param>
            <param name="log">Specifies the Log to use for output.</param>
            <param name="br">Specifies the binary reader.</param>
            <param name="bData">Specifies whether or not to read the data.</param>
            <param name="bDiff">Specifies whether or not to read the diff.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Share(MyCaffe.common.Blob{`0},System.Collections.Generic.List{System.Int32},System.Boolean)">
            <summary>
            Share the first Blob found with the same name as the given Blob.
            </summary>
            <param name="b">Specifies the Blob that will share the found Blob.</param>
            <param name="rgMinShape">Specifies the minimum shape required to share.</param>
            <param name="bThrowExceptions">Specifies whether or not th throw an Exception if the sizes do not match.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.SnapshotRequested(System.Boolean)">
            <summary>
            Returns whether or not any blobs have a snapshot request set to true.
            </summary>
            <returns>If any blob in the colleciton has their snapshot request set to true, then <i>true</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobCollection`1.Dispose">
            <summary>
            Release all resource used by the collection and its Blobs.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BlockingQueue`1">
            <summary>
            The BlockingQueue is used for synchronized Queue operations.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.#ctor(MyCaffe.basecode.CancelEvent)">
            <summary>
            The BlockingQueue constructor.
            </summary>
            <param name="evtCancel">Specifies a CancelEvent used to terminate wait sates within the Queue.</param>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Reset">
            <summary>
            Reset the abort event.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Abort">
            <summary>
            Cancel the blocking queue operations.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlockingQueue`1.Count">
            <summary>
            Return the number of items in the queue.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.GetAt(System.Int32,`0@)">
            <summary>
            Returns the item at a given index within the queue.
            </summary>
            <param name="nIdx">Specifies the index.</param>
            <param name="t">Specifies the data value at the index.</param>
            <returns>If the CancelEvent is null or is set, <i>false</i> is returned, otherwise if the data is successfully retrieved <i>true</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Clear">
            <summary>
            Remove all items from the queue.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Push(`0)">
            <summary>
            Add an item to the back of the queue.
            </summary>
            <param name="t">Specifies the item to add.</param>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Pop(`0@)">
            <summary>
            Remove an item from the front of the queue.
            </summary>
            <remarks>
            This function will wait until either data is added to the queue or the CancelEvent or AbortEvent are set.
            </remarks>
            <param name="t">Specifies the item removed.</param>
            <returns>If a CancelEvent occurs, <i>false</i> is returned, otherwise if the data is successfully removed from the queue <i>true</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Peek(`0@)">
            <summary>
            Retrieve an item from the front of the queue, but do not remove it.
            </summary>
            <remarks>
            This function will wait until either data is added to the queue or the CancelEvent or AbortEvent are set.
            </remarks>
            <param name="t">Specifies the item removed.</param>
            <returns>If a CancelEvent occurs, <i>false</i> is returned, otherwise if the data is successfully removed from the queue <i>true</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Dispose(System.Boolean)">
            <summary>
            Release all resources used by the queue.
            </summary>
            <param name="bDisposing">Set to <i>true</i> when called from Dispose().</param>
        </member>
        <member name="M:MyCaffe.common.BlockingQueue`1.Dispose">
            <summary>
            Release all resources used by the queue.
            </summary>
        </member>
        <member name="T:MyCaffe.common.DistanceMethod">
            <summary>
            Specifies the distance method used when calculating batch distances.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DistanceMethod.HAMMING">
            <summary>
            Specifies to calculate the hamming distance.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DistanceMethod.EUCLIDEAN">
            <summary>
            Specifies to calculate the euclidean distance.
            </summary>
        </member>
        <member name="T:MyCaffe.common.PoolingMethod">
            <summary>
            Specifies the pooling method used by the cuDnn function SetPoolingDesc.
            </summary>
            <remarks>
            @see CudaDnn::SetPoolingDesc
            </remarks>
        </member>
        <member name="F:MyCaffe.common.PoolingMethod.MAX">
            <summary>
            Specifies to use <code>CUDNN_POOLING_MAX</code> in CUDA C++ code.
            </summary>
        </member>
        <member name="F:MyCaffe.common.PoolingMethod.AVE">
            <summary>
            Specifies to use <code>CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</code> in CUDA C++ code.
            </summary>
        </member>
        <member name="T:MyCaffe.common.DataType">
            <summary>
            Specifies the base datatype corresponding the the template type 'T'.  Currently, only <code>double</code> and <code>float</code> types are supported. 
            </summary>
        </member>
        <member name="F:MyCaffe.common.DataType.DOUBLE">
            <summary>
            Specifies the double type.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DataType.FLOAT">
            <summary>
            Specifies the single type.
            </summary>
        </member>
        <member name="T:MyCaffe.common.DEVINIT">
            <summary>
            Specifies the initialization flags used when initializing CUDA.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVINIT.NONE">
            <summary>
            No flag specified.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVINIT.CUBLAS">
            <summary>
            Initialize cuBlas.  This should be initialized for cuBlas is used for many of the math operations.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVINIT.CURAND">
            <summary>
            Initialize cuRand.  This should be initialized for cuRand is used for most of the random operations.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVINIT.SETSEED">
            <summary>
            Set the cuRand random number generator seed - typically only used when testing to ensure that 
            random numbers are generated in a predictable ordering.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BATCHNORM_MODE">
            <summary>
            Specifies the cuDnn batch norm mode to use.
            </summary>
            <remarks>
            @see [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) documenation for more details.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.BATCHNORM_MODE.PER_ACTIVATION">
            <summary>
            Specifies to use the per-activation batch normalization mode.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BATCHNORM_MODE.SPATIAL">
            <summary>
            Specifies to use the spatial batch normalization mode.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BATCHNORM_MODE.SPATIAL_PERSISTENT">
            <summary>
            Specifies to use the spatial persistent batch normalization mode.
            </summary>
        </member>
        <member name="T:MyCaffe.common.CONV_FWD_ALGO">
            <summary>
            Specifies the cuDnn convolution forward algorithm to use.
            </summary>
            <remarks>
            @see [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) documenation for more details.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.NONE">
            <summary>
            Specifies to not use a forward algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.IMPLICIT_GEMM">
            <summary>
            Specifies to use the implicit gemm algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.IMPLICIT_PRECOMP_GEMM">
            <summary>
            Specifies to use the implicit pre-computation gemm algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_GEMM">
            <summary>
            Specifies to use the gemm algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_DIRECT">
            <summary>
            Specifies to use the direct algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_FFT">
            <summary>
            Specifies to use the fft algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_FFT_TILING">
            <summary>
            Specifies to use the fft tiling algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_WINOGRAD">
            <summary>
            Specifies to use the winograd algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_FWD_ALGO.ALGO_WINOGRAD_NONFUSED">
            <summary>
            Specifies to use the non-fused winograd algorithm.
            </summary>
        </member>
        <member name="T:MyCaffe.common.CONV_BWD_FILTER_ALGO">
            <summary>
            Specifies the cuDnn convolution backward filter algorithm to use.
            </summary>
            <remarks>
            @see [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) documenation for more details.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_FILTER_ALGO.ALGO_0">
            <summary>
            Specifies to use algorithm 0 - which is non-deterministic.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_FILTER_ALGO.ALGO_1">
            <summary>
            Specifies to use algorithm 1.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_FILTER_ALGO.ALGO_FFT">
            <summary>
            Specifies to use the fft algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_FILTER_ALGO.ALGO_3">
            <summary>
            Specifies to use algorithm 0 with a workspace - which is non-deterministic.
            </summary>
        </member>
        <member name="T:MyCaffe.common.CONV_BWD_DATA_ALGO">
            <summary>
            Specifies the cuDnn convolution backward data algorithm to use.
            </summary>
            <remarks>
            @see [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) documenation for more details.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_DATA_ALGO.ALGO_0">
            <summary>
            Specifies to use algorithm 0 - which is non-deterministic.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_DATA_ALGO.ALGO_1">
            <summary>
            Specifies to use algorithm 1.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CONV_BWD_DATA_ALGO.ALGO_FFT">
            <summary>
            Specifies to use the fft algorithm.
            </summary>
        </member>
        <member name="T:MyCaffe.common.POOLING_METHOD">
            <summary>
            Specifies the pooling method to use when using the Caffe pooling (instead of the pooling from NVIDIA's cuDnn).
            </summary>
            <remarks>
            @see CudaDnn::pooling_fwd
            </remarks>
        </member>
        <member name="F:MyCaffe.common.POOLING_METHOD.MAX">
            <summary>
            Select the maximum value from the kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.common.POOLING_METHOD.AVE">
            <summary>
            Select the average of the values in the kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.common.POOLING_METHOD.STO_TRAIN">
            <summary>
            Select the stochastic value in the kernel - used during a training pass.
            </summary>
        </member>
        <member name="F:MyCaffe.common.POOLING_METHOD.STO_TEST">
            <summary>
            Select the stochastic value in the kernel - used during a testing pass.
            </summary>
        </member>
        <member name="T:MyCaffe.common.RNN_MODE">
            <summary>
            Specifies the RNN mode to use with the Recurrent Layer when using the cuDNN engine.
            </summary>
        </member>
        <member name="F:MyCaffe.common.RNN_MODE.RNN_RELU">
            <summary>
            Specifies to use a single RelU gate Recurrent Learning unit.
            </summary>
        </member>
        <member name="F:MyCaffe.common.RNN_MODE.RNN_TANH">
            <summary>
            Specifies to use a single TanH gate Recurrent Learning unit.
            </summary>
        </member>
        <member name="F:MyCaffe.common.RNN_MODE.LSTM">
            <summary>
            Specifies to use a 4 gate LSTM Recurrent Learning unit.
            </summary>
        </member>
        <member name="T:MyCaffe.common.RNN_DATALAYOUT">
            <summary>
            Specifies the RNN data layout of the data input.
            </summary>
        </member>
        <member name="F:MyCaffe.common.RNN_DATALAYOUT.RNN_SEQ_MAJOR">
            <summary>
            Specifies ordering with sequence major ordering.
            </summary>
        </member>
        <member name="F:MyCaffe.common.RNN_DATALAYOUT.RNN_BATCH_MAJOR">
            <summary>
            Specifies ordering with batch major ordering.
            </summary>
        </member>
        <member name="T:MyCaffe.common.DEVPROP">
            <summary>
            Specifies certain device properties to query from Cuda.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVPROP.DEVICECOUNT">
            <summary>
            Query the number of devices (gpu's) installed.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVPROP.NAME">
            <summary>
            Query the name of a given GPU.
            </summary>
        </member>
        <member name="F:MyCaffe.common.DEVPROP.MULTIGPUBOARDGROUPID">
            <summary>
            Query a GPU board group ID.
            </summary>
        </member>
        <member name="T:MyCaffe.common.MEMTEST_TYPE">
            <summary>
            Specifies the memory test to perform.
            </summary>
            <remarks>
            @see CudaDnn::RunMemoryTest
            </remarks>
        </member>
        <member name="F:MyCaffe.common.MEMTEST_TYPE.MOV_INV_8">
            <summary>
            Specifies the mov-inv-8 test.
            </summary>
        </member>
        <member name="T:MyCaffe.common.NCCL_REDUCTION_OP">
            <summary>
            Specifies the reduction operation to use with 'Nickel' NCCL.
            </summary>
            <remarks>
            @see CudaDnn::NcclAllReduce
            </remarks>
        </member>
        <member name="F:MyCaffe.common.NCCL_REDUCTION_OP.SUM">
            <summary>
            Sum the values.
            </summary>
        </member>
        <member name="F:MyCaffe.common.NCCL_REDUCTION_OP.PROD">
            <summary>
            Multiply the values.
            </summary>
        </member>
        <member name="F:MyCaffe.common.NCCL_REDUCTION_OP.MAX">
            <summary>
            Return the maximum value.
            </summary>
        </member>
        <member name="F:MyCaffe.common.NCCL_REDUCTION_OP.MIN">
            <summary>
            Return the minimum value.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SSD_MINING_TYPE">
            <summary>
            Defines the mining type used during SSD cuda training.
            </summary>
            <remarks>
            This enum matches the values of the MultiBoxLossParameter.MiningType with the values supported
            in the low level CudaDnnDll.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.SSD_MINING_TYPE.NONE">
            <summary>
            Use all negatives.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_MINING_TYPE.MAX_NEGATIVE">
            <summary>
            Select negatives based on the score.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_MINING_TYPE.HARD_EXAMPLE">
            <summary>
            Select hard examples based on Shrivastava et. al. method.
            </summary>
            <remarks>
            @see [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540) by Abhinav Shrivastava, Abhinav Gupta, Ross Girshick, 2016.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.SSD_MATCH_TYPE">
            <summary>
            Defines the matching method used during SSD cuda training.
            </summary>
            <remarks>
            This enum matches the values of the MultiBoxLossParameter.MatchType with the values supported
            in the low level CudaDnnDll.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.SSD_MATCH_TYPE.BIPARTITE">
            <summary>
            Specifies to use Bi-Partite.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_MATCH_TYPE.PER_PREDICTION">
            <summary>
            Specifies to use per-prediction matching.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SSD_CODE_TYPE">
            <summary>
            Defines the encode/decode type used during SSD cuda training.
            </summary>
            <remarks>
            This enum matches the values of the PriorBoxParameter.CodeType with the values supported
            in the low level CudaDnnDll.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.SSD_CODE_TYPE.CORNER">
            <summary>
            Encode the corner.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_CODE_TYPE.CENTER_SIZE">
            <summary>
            Encode the center size.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_CODE_TYPE.CORNER_SIZE">
            <summary>
            Encode the corner size.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SSD_CONF_LOSS_TYPE">
            <summary>
            Defines the confidence loss types used during SSD cuda training.
            </summary>
            <remarks>
            This enum matches the values of the MultiboxLossParameter.ConfLossType with the values supported
            in the low level CudaDnnDll.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.SSD_CONF_LOSS_TYPE.SOFTMAX">
            <summary>
            Specifies to use softmax.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_CONF_LOSS_TYPE.LOGISTIC">
            <summary>
            Specifies to use logistic.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SSD_LOC_LOSS_TYPE">
            <summary>
            Defines the location loss types used during SSD cuda training.
            </summary>
            <remarks>
            This enum matches the values of the MultiboxLossParameter.LocLossType with the values supported
            in the low level CudaDnnDll.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.SSD_LOC_LOSS_TYPE.L2">
            <summary>
            Specifies to use L2 loss.
            </summary>
        </member>
        <member name="F:MyCaffe.common.SSD_LOC_LOSS_TYPE.SMOOTH_L1">
            <summary>
            Specifies to use smooth L1 loss.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ORIENTATION">
            <summary>
            Specifies the orientation of a matrix.
            </summary>
            <remarks>
            @see CudaDnn::matrix_add_vector
            </remarks>
        </member>
        <member name="F:MyCaffe.common.ORIENTATION.COL">
            <summary>
            Specifies to add the vector to each column.
            </summary>
        </member>
        <member name="F:MyCaffe.common.ORIENTATION.ROW">
            <summary>
            Specifies to add the vector to each row.
            </summary>
        </member>
        <member name="T:MyCaffe.common.TRANSPOSE_OPERATION">
            <summary>
            Specifies the type of operation to perform along with a matrix transposition.
            </summary>
            <remarks>
            @see CudaDnn::matrix_transpose_operation
            </remarks>
        </member>
        <member name="F:MyCaffe.common.TRANSPOSE_OPERATION.ADD">
            <summary>
            Add the matrix values after transposing.
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRANSPOSE_OPERATION.MUL">
            <summary>
            Multiply the matrix values after transposing.
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRANSPOSE_OPERATION.DIV">
            <summary>
            Divide the matrix values after transposing.
            </summary>
        </member>
        <member name="T:MyCaffe.common.AGGREGATIONS">
            <summary>
            Specifies different aggregation operations.
            </summary>
        </member>
        <member name="F:MyCaffe.common.AGGREGATIONS.SUM">
            <summary>
            Sum the values.
            </summary>
        </member>
        <member name="F:MyCaffe.common.AGGREGATIONS.MAX">
            <summary>
            Return the maximum value.
            </summary>
        </member>
        <member name="F:MyCaffe.common.AGGREGATIONS.MIN">
            <summary>
            Return the minimum value.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ICudaDevice">
            <summary>
            Specifies the general cuda device interface.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.ICudaMemory">
            <summary>
            Specifies the cuda memory operations interface.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.ICudaCuDnn">
            <summary>
            Specifies the interface to common cuDnn functionality.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.ICudaMath">
            <summary>
            Specifies the interface to common math functions.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.ICudaRandom">
            <summary>
            Specifies the interface to common random number generation functions.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.ICudaDnn">
            <summary>
            Specifies the combination interface that encompasses all other interfaces.
            </summary>
            <remarks>
            This interface is primarily used for testing.
            </remarks>
        </member>
        <member name="T:MyCaffe.common.CudaDnn`1">
            <summary>
            The CudaDnn object is the main interface to the Low-Level Cuda C++ DLL.
            </summary>
            <remarks>
            This is the transition location where C# meets C++.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="T:MyCaffe.common.CudaDnn`1.CUDAQRY">
            <summary>
            Specifies the type of string information to quer from the Cuda C++ layer.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CudaDnn`1.CUDAQRY.DEVICE_NAME">
            <summary>
            Query the device (GPU) name.
            </summary>
        </member>
        <member name="F:MyCaffe.common.CudaDnn`1.CUDAQRY.DEVICE_P2P_INFO">
            <summary>
            Query the device (GPU) Peer-to-Peer information.  Note, P2P mode is only available when
            running a device in TCC mode.  For more information see the [NVIDIA SMI Documentation](http://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)
            </summary>
        </member>
        <member name="F:MyCaffe.common.CudaDnn`1.CUDAQRY.DEVICE_INFO">
            <summary>
            Query the device (GPU) general information such as memory and processor usage.
            </summary>
        </member>
        <member name="T:MyCaffe.common.CudaDnn`1.CUDAFN">
            <summary>
            Specifies the function indexes supported by the Low-Level Cuda Dnn DLL.
            </summary>
            <remarks><b>IMPORTANT:</b> These index values must match the index values 
            specified within the Low-Level Cuda Dnn DLL.</remarks>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.#ctor(System.Int32,MyCaffe.common.DEVINIT,System.Nullable{System.Int64},System.String,System.Boolean,System.Boolean)">
            <summary>
            The CudaDnn constructor.
            </summary>
            <param name="nDeviceID">Specifies the zero-based device (GPU) id.  Note, if there are 5 GPU's in the system, the device ID's will be numbered 0, 1, 2, 3, 4.</param>
            <param name="flags">Specifies the flags under which to initialize the Low-Level Cuda system.</param>
            <param name="lSeed">Optionally specifies the random number generator seed.  Typically this is only used during testing.</param>
            <param name="strPath">Specifies the file path of the Low-Level Cuda DNN Dll file. When NULL or empty, the Low-Level <code>CudaDNNDll.dll</code> file in the directory of 
            the currently executing process (that is using the CudaDnn object) is used.</param>
            <param name="bResetFirst">Specifies to reset the device before initialzing.  <b>IMPORTANT:</b> It is only recommended to set this to <code>true</code> when testing.</param>
            <param name="bEnableMemoryTrace">Optionally, specifies to enable the memory tracing (only supported in debug mode and dramatically slows down processing).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.#ctor(MyCaffe.common.CudaDnn{`0},System.Boolean)">
            <summary>
            Alternate CudaDnn constructor.
            </summary>
            <param name="cuda">Specifies an already created CudaDn instance.  The internal Cuda Control of this instance is used by the new instance.</param>
            <param name="bEnableGhostMemory">Specifies to enable the ghost memory used to estimate GPU memory usage without allocating any GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.Dispose(System.Boolean)">
            <summary>
            Disposes this instance freeing up all of its host and GPU memory.
            </summary>
            <param name="bDisposing">When true, specifies that the call is from a Dispose call.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.Dispose">
            <summary>
            Disposes this instance freeing up all of its host and GPU memory.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetCudaDnnDllPath">
            <summary>
            Returns the path to the CudaDnnDll module to use for low level CUDA processing.
            </summary>
            <returns>The CudaDnnDll path is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DisableGhostMemory">
            <summary>
            Disables the ghost memory, if enabled.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ResetGhostMemory">
            <summary>
            Resets the ghost memory by enabling it if this instance was configured to use ghost memory.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.TotalMemoryUsed">
            <summary>
            Returns the total amount of GPU memory used by this instance.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.TotalMemoryUsedAsText">
            <summary>
            Returns the total amount of memory used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.KernelHandle">
            <summary>
            Returns the Low-Level kernel handle used for this instance.  Each Low-Level kernel maintains its own
            set of look-up tables for memory, streams, cuDnn constructs, etc.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.KernelCopy(System.Int32,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Copy memory from the look-up tables in one kernel to another.
            </summary>
            <param name="nCount">Specifies the number of items to copy.</param>
            <param name="hSrc">Specifies the handle to the source memory.</param>
            <param name="nSrcOffset">Specifies the offset (in items, not bytes) from which to start the copy in the source memory.</param>
            <param name="hDstKernel">Specifies the destination kernel holding the look-up table and memory where the data is to be copied.</param>
            <param name="hDst">Specifies the handle to the destination memory where the data is to be copied.</param>
            <param name="nDstOffset">Specifies the offset (in items, not bytes) where the copy to to be placed within the destination data.</param>
            <param name="hHostBuffer">Specifies the handle to the host buffer to be used when transfering the data from one kernel to another.</param>
            <param name="hHostKernel">Optionally, specifies the handle to the kernel holding the look-up table for the host buffer.</param>
            <param name="hStream">Optionally, specifies the handle to the CUDA stream to use for the transfer.</param>
            <param name="hSrcKernel">Optionally, specifies the handle to the source kernel.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.KernelAdd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Add memory from one kernel to memory residing on another kernel.
            </summary>
            <param name="nCount">Specifies the number of items within both A and B.</param>
            <param name="hA">Specifies the handle to the memory A.</param>
            <param name="hDstKernel">Specifies the kernel where the memory B and the desitnation memory C reside.</param>
            <param name="hB">Specifies the handle to the memory B (for which A will be added).</param>
            <param name="hC">Specifies the destination data where A+B will be placed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.KernelCopyNccl(System.Int64,System.Int64)">
            <summary>
            Copies an Nccl handle from one kernel to the current kernel of the current CudaDnn instance.
            </summary>
            <remarks>
            Nccl handles are created on the main Kernel, but when used must transferred to the destination kernel (running on
            a different thread) where the secondary Nccl handle is used.
            </remarks>
            <param name="hSrcKernel">Specifies the source kernel (typically where the Nccl handle was created).</param>
            <param name="hSrcNccl">Specifies the source Nccl handle to be copied.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetDefaultCudaPath(System.String)">
            <summary>
            Used to <i>optionally</i> set the default path to the Low-Level Cuda Dnn DLL file.
            </summary>
            <param name="strPath">Specifies the file path to the Low-Level Cuda Dnn DLL file to use.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.basetype_size(System.Boolean)">
            <summary>
            Returns the base type size in bytes.
            </summary>
            <param name="bUseHalfSize">Specifies whether or not to use half size or the base size.</param>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.Path">
            <summary>
            Specifies the file path used to load the Low-Level Cuda DNN Dll file.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.DefaultPath">
            <summary>
            Specifies the default path used t load the Low-Level Cuda DNN Dll file.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetDeviceID(System.Int32,MyCaffe.common.DEVINIT,System.Nullable{System.Int64})">
            <summary>
            Set the device ID used by the current instance of CudaDnn.
            </summary>
            <param name="nDeviceID">Specifies the zero-based device (GPU) id.  When -1, the device ID is set to the device ID used to create the instance of CudaDnn.</param>
            <param name="flags">Optionally, specifies the initialization flags.</param>
            <param name="lSeed">Optionally, specifies the random number generator seed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetRandomSeed(System.Int64)">
            <summary>
            Set the random number generator seed.
            </summary>
            <param name="lSeed">Specifies the seed to set.</param>
        </member>
        <member name="P:MyCaffe.common.CudaDnn`1.OriginalDeviceID">
            <summary>
            Returns the original device ID used to create the instance of CudaDnn.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceID">
            <summary>
            Returns the current device id set within Cuda.
            </summary>
            <returns>The device id.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceName(System.Int32)">
            <summary>
            Query the name of a device.
            </summary>
            <param name="nDeviceID">Specifies the device id.</param>
            <returns>The name of the GPU at the device id is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceP2PInfo(System.Int32)">
            <summary>
            Query the peer-to-peer information of a device.
            </summary>
            <param name="nDeviceID">Specifies the device id.</param>
            <returns>The peer-to-per information of the GPU at the device id is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceInfo(System.Int32,System.Boolean)">
            <summary>
            Query the device information of a device.
            </summary>
            <param name="nDeviceID">Specifies the device id.</param>
            <param name="bVerbose">When true, more detailed information is returned.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ResetDevice">
            <summary>
            Reset the current device.
            </summary>
            <remarks><b>IMPORTANT:</b> This function will delete all memory and state information on the current device, which may
            cause other CudaDnn instances using the same device, to fail.  For that reason, it is recommended to only call
            this function when testing.</remarks>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SynchronizeDevice">
            <summary>
            Synchronize the operations on the current device.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetMultiGpuBoardGroupID(System.Int32)">
            <summary>
            Query the mutli-gpu board group id for a device.
            </summary>
            <param name="nDeviceID">Specifies the device id.</param>
            <returns>The mutli-gpu board group id is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceCount">
            <summary>
            Query the number of devices (gpu's) installed.
            </summary>
            <returns>The number of GPU's is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CheckMemoryAttributes(System.Int64,System.Int32,System.Int64,System.Int32)">
            <summary>
            Check the memory attributes of two memory blocks on different devices to see if they are compatible
            for peer-to-peer memory transfers.
            </summary>
            <param name="hSrc">Specifies the handle to the source memory.</param>
            <param name="nSrcDeviceID">Specifies the device id where the source memory resides.</param>
            <param name="hDst">Specifies the handle to the destination memory.</param>
            <param name="nDstDeviceID">Specifies the device id where the destination memory resides.</param>
            <returns>This function returns <code>true</code> when both devices support peer-to-peer communcation, <code>false</code> otherwise.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDeviceMemory(System.Double@,System.Double@,System.Boolean@,System.Int32)">
            <summary>
            Queries the amount of total, free and used memory on a given GPU.
            </summary>
            <param name="dfFree">Specifies the amount of free memory in GB.</param>
            <param name="dfUsed">Specifies the amount of used memory in GB.</param>
            <param name="bCudaCallUsed">Specifies whether or not the used memory is an estimate calculated using the Low-Level Cuda DNN Dll handle table.</param>
            <param name="nDeviceID">Specifies the specific device id to query, or if -1, uses calculates an estimate of the memory used using the current low-level Cuda DNN Dll handle table.</param>
            <returns>The device's total amount of memory in GB is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetRequiredCompute(System.Int32@,System.Int32@)">
            <summary>
            The GetRequiredCompute function returns the Major and Minor compute values required by the current CudaDNN DLL used.
            </summary>
            <param name="nMinMajor">Specifies the minimum required major compute value.</param>
            <param name="nMinMinor">Specifies the minimum required minor compute value.</param>
            <remarks>
            Together the Major.Minor compute values define the minimum required compute for the CudaDNN DLL used.
            </remarks>
            <returns>
            The path to the CudaDNN dll in use is returned.
            </returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DeviceCanAccessPeer(System.Int32,System.Int32)">
            <summary>
            Query whether or not two devices can access each other via peer-to-peer memory copies.
            </summary>
            <param name="nSrcDeviceID">Specifies the device id of the source.</param>
            <param name="nPeerDeviceID">Specifies the device id of the peer to the source device.</param>
            <returns><code>true</code> is returned if the source device can access the peer device via peer-to-peer communcation, <code>false</code> otherwise.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DeviceEnablePeerAccess(System.Int32)">
            <summary>
            Enables peer-to-peer access between the current device used by the CudaDnn instance and a peer device.
            </summary>
            <param name="nPeerDeviceID">Specifies the device id of the peer device.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DeviceDisablePeerAccess(System.Int32)">
            <summary>
            Disables peer-to-peer access between the current device used by the CudaDnn instance and a peer device.
            </summary>
            <param name="nPeerDeviceID">Specifies the device id of the peer device.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(System.Collections.Generic.List{System.Double})">
            <summary>
            Allocate a block of GPU memory and copy a list of doubles to it.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="rg">Specifies a list of doubles to copy to the GPU.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(System.Collections.Generic.List{System.Single})">
            <summary>
            Allocate a block of GPU memory and copy a list of floats to it.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="rg">Specifies a list of floats to copy to the GPU.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(System.Double[],System.Int64)">
            <summary>
            Allocate a block of GPU memory and copy an array of doubles to it, optionally using a stream for the copy.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="rgSrc">Specifies an array of doubles to copy to the GPU.</param>
            <param name="hStream">Optionally specifies a stream to use for the copy.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(System.Single[],System.Int64)">
            <summary>
            Allocate a block of GPU memory and copy an array of float to it, optionally using a stream for the copy.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="rgSrc">Specifies an array of float to copy to the GPU.</param>
            <param name="hStream">Optionally specifies a stream to use for the copy.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(`0[],System.Int64,System.Boolean)">
            <summary>
            Allocate a block of GPU memory and copy an array of type 'T' to it, optionally using a stream for the copy.
            </summary>
            <param name="rgSrc">Specifies an array of 'T' to copy to the GPU.</param>
            <param name="hStream">Optionally, specifies a stream to use for the copy.</param>
            <param name="bHalfSize">Optionally, specifies to use half size float memory - only available with the 'float' base type.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocMemory(System.Int64,System.Boolean)">
            <summary>
            Allocate a block of GPU memory with a specified capacity.
            </summary>
            <param name="lCapacity">Specifies the capacity to allocate (in items, not bytes).</param>
            <param name="bHalfSize">Optionally, specifies to use half size float memory - only available with the 'float' base type.</param>
            <returns>The handle to the GPU memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeMemory(System.Int64)">
            <summary>
            Free previously allocated GPU memory.
            </summary>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CopyDeviceToHost(System.Int64,System.Int64,System.Int64)">
            <summary>
            Copy from GPU memory to Host memory.
            </summary>
            <param name="lCount">Specifies the number of items (of base type each) to copy.</param>
            <param name="hGpuSrc">Specifies the GPU memory containing the source data.</param>
            <param name="hHostDst">Specifies the Host memory containing the host destination.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CopyHostToDevice(System.Int64,System.Int64,System.Int64)">
            <summary>
            Copy from Host memory to GPU memory.
            </summary>
            <param name="lCount">Specifies the number of items (of base type each) to copy.</param>
            <param name="hHostSrc">Specifies the Host memory containing the host source data.</param>
            <param name="hGpuDst">Specifies the GPU memory containing the destination.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocHostBuffer(System.Int64)">
            <summary>
            Allocate a block of host memory with a specified capacity.
            </summary>
            <param name="lCapacity">Specifies the capacity to allocate (in items, not bytes).</param>
            <returns>The handle to the host memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeHostBuffer(System.Int64)">
            <summary>
            Free previously allocated host memory.
            </summary>
            <param name="hMem">Specifies the handle to the host memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetHostBufferCapacity(System.Int64)">
            <summary>
            Returns the host memory capacity.
            </summary>
            <param name="hMem">Specfies the host memory.</param>
            <returns>The current host memory capacity is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetHostMemoryDouble(System.Int64)">
            <summary>
            Retrieves the host memory as an array of doubles.
            </summary>
            <remarks>This function converts the output array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the host memory.</param>
            <returns>An array of doubles is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetHostMemoryFloat(System.Int64)">
            <summary>
            Retrieves the host memory as an array of floats.
            </summary>
            <remarks>This function converts the output array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the host memory.</param>
            <returns>An array of floats is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetHostMemory(System.Int64)">
            <summary>
            Retrieves the host memory as an array of type 'T'
            </summary>
            <param name="hMem">Specifies the handle to the host memory.</param>
            <returns>An array of type 'T' is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetMemoryDouble(System.Int64,System.Int64)">
            <summary>
            Retrieves the GPU memory as an array of doubles.
            </summary>
            <remarks>This function converts the output array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="lCount">Optionally, specifies a count of items to retrieve.</param>
            <returns>An array of double is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetMemoryFloat(System.Int64,System.Int64)">
            <summary>
            Retrieves the GPU memory as an array of float.
            </summary>
            <remarks>This function converts the output array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="lCount">Optionally, specifies a count of items to retrieve.</param>
            <returns>An array of float is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetMemory(System.Int64,System.Int64)">
            <summary>
            Retrieves the GPU memory as an array of type 'T'
            </summary>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="lCount">Optionally, specifies a count of items to retrieve.</param>
            <returns>An array of type 'T' is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemory(System.Int64,System.Collections.Generic.List{System.Double})">
            <summary>
            Copies a list of doubles into a block of already allocated GPU memory.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rg">Specifies the list of doubles to copy.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemory(System.Int64,System.Collections.Generic.List{System.Single})">
            <summary>
            Copies a list of float into a block of already allocated GPU memory.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rg">Specifies the list of float to copy.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemory(System.Int64,System.Double[],System.Int64)">
            <summary>
            Copies an array of double into a block of already allocated GPU memory.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of double to copy.</param>
            <param name="hStream">Optionally specifies the stream to use for the copy operation.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemory(System.Int64,System.Single[],System.Int64)">
            <summary>
            Copies an array of float into a block of already allocated GPU memory.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of float to copy.</param>
            <param name="hStream">Optionally specifies the stream to use for the copy operation.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemory(System.Int64,`0[],System.Int64,System.Int32)">
            <summary>
            Copies an array of type 'T' into a block of already allocated GPU memory.
            </summary>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of type 'T' to copy.</param>
            <param name="hStream">Optionally specifies the stream to use for the copy operation.</param>
            <param name="nCount">Optionally, specifies a count of items to retrieve.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemoryAt(System.Int64,System.Double[],System.Int32)">
            <summary>
            Copies an array of double into a block of already allocated GPU memory starting at a specific offset.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of double to copy.</param>
            <param name="nOffset">Specifies offset within the GPU memory from where the copy is to start.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemoryAt(System.Int64,System.Single[],System.Int32)">
            <summary>
            Copies an array of float into a block of already allocated GPU memory starting at a specific offset.
            </summary>
            <remarks>This function converts the input array into the base type 'T' for which the instance of CudaDnn was defined.</remarks>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of float to copy.</param>
            <param name="nOffset">Specifies offset within the GPU memory from where the copy is to start.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetMemoryAt(System.Int64,`0[],System.Int32)">
            <summary>
            Copies an array of type 'T' into a block of already allocated GPU memory starting at a specific offset.
            </summary>
            <param name="hMem">Specifies the handle to the GPU memory.</param>
            <param name="rgSrc">Specifies the array of type 'T' to copy.</param>
            <param name="nOffset">Specifies offset within the GPU memory from where the copy is to start.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetHostMemory(System.Int64,`0[])">
            <summary>
            Copies an array of type 'T' into a block of already allocated host memory.
            </summary>
            <param name="hMem">Specifies the handle to the host memory.</param>
            <param name="rgSrc">Specifies the array of type 'T' to copy.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateMemoryPointer(System.Int64,System.Int64,System.Int64)">
            <summary>
            Creates a memory pointer into an already existing block of GPU memory.
            </summary>
            <param name="hData">Specifies a handle to the GPU memory.</param>
            <param name="lOffset">Specifies the offset into the GPU memory (in items, not bytes), where the pointer is to start.</param>
            <param name="lCount">Specifies the number of items (not bytes) in the 'virtual' memory block pointed to by the memory pointer.</param>
            <returns>A handle to the memory pointer is returned.  Handles to memory poitners can be used like any other handle to GPU memory.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeMemoryPointer(System.Int64)">
            <summary>
            Frees a memory pointer.
            </summary>
            <param name="hData">Specifies the handle to the memory pointer.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateMemoryTest(System.UInt64@,System.Double@,System.UInt64@,System.UInt64@,System.Double)">
            <summary>
            Creates a new memory test on the current GPU.
            </summary>
            <param name="ulTotalNumBlocks">Returns the total number of blocks available to test.</param>
            <param name="dfMemAllocatedInGB">Returns the total amount of allocated memory, specified in GB.</param>
            <param name="ulMemStartAddr">Returns the start address of the memory test.</param>
            <param name="ulBlockSize">Returns the block size of the memory to be tested.</param>
            <param name="dfPctToAllocate">Specifies the percentage of avaiable memory to test, where 1.0 = 100%.</param>
            <returns>A handle to the memory test is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeMemoryTest(System.Int64)">
            <summary>
            Free a memory test, freeing up all GPU memory used.
            </summary>
            <param name="h">Specifies the handle to the memory test.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RunMemoryTest(System.Int64,MyCaffe.common.MEMTEST_TYPE,System.UInt64,System.UInt64,System.Boolean,System.Boolean,System.Boolean,System.Boolean)">
            <summary>
            The RunMemoryTest method runs the memory test from the block start offset through the block count on the
            memory previously allocated using CreateMemoryTest.
            </summary>
            <param name="h">Specifies the handle to the memory test data.</param>
            <param name="type">Specifies the type of memory test to run.</param>
            <param name="ulBlockStartOffset">Specifies the block start offset (offset into the total blocks returned
            by CreateMemoryTest).</param>
            <param name="ulBlockCount">Specifies the number of blocks to test.</param>
            <param name="bVerbose">When disabled, the memory test is just run once and the number of errors is returned.
            When eanbled, the memory test is run twice and the erroring adresses are returned along with the error count.</param>
            <returns>The format of the array returned is as follows:
            rg[0] - specifies the starting memory address used for this memory test run.
            rg[1] - specifies the number of addresses over which the test was run (specified in 1 byte increments).
            rg[2] - specifies the number of errors found.
            rg[3, ...] - specifies the erroring addresses (specified in 1-bit increments)
            <param name="bWrite">Specifies to perform a write test.</param>
            <param name="bReadWrite">Specifies to perform a read/write test.</param>
            <param name="bRead">Specifies to peroform a read test.</param>
            </returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateImageOp(System.Int32,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Int64)">
            <summary>
            Create a new ImageOp used to perform image operations on the GPU.
            </summary>
            <param name="nNum">Specifies the number of items (usually the blob.num).</param>
            <param name="dfBrightnessProb">Specifies the brightness probability [0,1].</param>
            <param name="dfBrightnessDelta">Specifies the brightness delta.</param>
            <param name="dfContrastProb">Specifies the contrast probability [0,1]</param>
            <param name="dfContrastLower">Specifies the contrast lower bound value.</param>
            <param name="dfContrastUpper">Specifies the contrast upper bound value.</param>
            <param name="dfSaturationProb">Specifies the saturation probability [0,1]</param>
            <param name="dfSaturationLower">Specifies the saturation lower bound value.</param>
            <param name="dfSaturationUpper">Specifies the saturation upper bound value.</param>
            <param name="lRandomSeed">Optionally, specifies the random seed or 0 to ignore (default = 0).</param>
            <returns>A handle to the ImageOp is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeImageOp(System.Int64)">
            <summary>
            Free an image op, freeing up all GPU memory used.
            </summary>
            <param name="h">Specifies the handle to the image op.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DistortImage(System.Int64,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Distort an image using the ImageOp handle provided.
            </summary>
            <param name="h">Specifies a handle to the ImageOp that defines how the image will be distorted.</param>
            <param name="nCount">Specifies the total number of data elements within 'hX' and 'hY'.</param>
            <param name="nNum">Specifies the number of items to be distorted (typically blob.num) in 'hX' and 'hY'.</param>
            <param name="nDim">Specifies the dimension of each item.</param>
            <param name="hX">Specifies a handle to the GPU memory containing the source data to be distorted.</param>
            <param name="hY">Specifies a handle to the GPU memory containing the destination of the distortion.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateStream(System.Boolean,System.Int32)">
            <summary>
            Create a new stream on the current GPU.
            </summary>
            <param name="bNonBlocking">When <code>false</code> (the default) the created stream is a 'blocking' stream, otherwise it is an asynchronous, non-blocking stream.</param>
            <param name="nIndex">Specifies an index for the stream where indexed streams are shared when the index = 0 or greater.</param>
            <returns>The handle to the stream is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeStream(System.Int64)">
            <summary>
            Free a stream.
            </summary>
            <param name="h">Specifies the handle to the stream.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SynchronizeStream(System.Int64)">
            <summary>
            Synchronize a stream on the current GPU, waiting for its operations to complete.
            </summary>
            <param name="h">Specifies the handle to the stream.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SynchronizeThread">
            <summary>
            Synchronize all kernel threads on the current GPU.
            </summary>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateCuDNN(System.Int64)">
            <summary>
            Create a new instance of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <param name="hStream">Specifies a stream used by cuDnn.</param>
            <returns>The handle to cuDnn is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeCuDNN(System.Int64)">
            <summary>
            Free an instance of cuDnn.
            </summary>
            <param name="h">Specifies the handle to cuDnn.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateNCCL(System.Int32,System.Int32,System.Int32,System.Guid)">
            <summary>
            Create an instance of [NVIDIA's NCCL 'Nickel'](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/)
            </summary>
            <param name="nDeviceId">Specifies the device where this instance of NCCL is going to run.</param>
            <param name="nCount">Specifies the total number of NCCL instances used.</param>
            <param name="nRank">Specifies the zero-based rank of this instance of NCCL.</param>
            <param name="guid">Specifies the unique Guid for this isntance of NCCL.</param>
            <returns>The handle to a new instance of NCCL is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeNCCL(System.Int64)">
            <summary>
            Free an instance of NCCL.
            </summary>
            <param name="hNccl">Specifies the handle to NCCL.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.NcclInitializeSingleProcess(System.Int64[])">
            <summary>
            Initializes a set of NCCL instances for use in a single process.
            </summary>
            <remarks>
            See [Fast Multi-GPU collectives with NCCL](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/).
            </remarks>
            <param name="rghNccl">Specifies the array of NCCL handles that will be working together.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.NcclInitializeMultiProcess(System.Int64)">
            <summary>
            Initializes a set of NCCL instances for use in different processes.
            </summary>
            <remarks>
            See [Fast Multi-GPU collectives with NCCL](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/).
            </remarks>
            <param name="hNccl">Specifies the handle of NCCL to initialize.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.NcclBroadcast(System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Broadcasts a block of GPU data to all NCCL instances.
            </summary>
            <remarks>
            See [Fast Multi-GPU collectives with NCCL](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/).
            </remarks>
            <param name="hNccl">Specifies a handle to an NCCL instance.</param>
            <param name="hStream">Specifies a handle to the stream to use for synchronization.</param>
            <param name="hX">Specifies a handle to the GPU data to be broadcasted (or recieved).</param>
            <param name="nCount">Specifies the number of items (not bytes) in the data.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.NcclAllReduce(System.Int64,System.Int64,System.Int64,System.Int32,MyCaffe.common.NCCL_REDUCTION_OP,System.Double)">
            <summary>
            Performs a reduction on all NCCL instances as specified by the reduction operation.
            </summary>
            <remarks>
            See [Fast Multi-GPU collectives with NCCL](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/).
            </remarks>
            <param name="hNccl">Specifies a handle to an NCCL instance.</param>
            <param name="hStream">Specifies a handle to the stream to use for synchronization.</param>
            <param name="hX">Specifies a handle to the GPU data to reduce with the other instances of NCCL.</param>
            <param name="nCount">Specifies the number of items (not bytes) in the data.</param>
            <param name="op">Specifies the reduction operation to perform.</param>
            <param name="dfScale">Optionally, specifies a scaling to be applied to the final reduction.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateExtension(System.String)">
            <summary>
            Create an instance of an Extension DLL.
            </summary>
            <param name="strExtensionDllPath">Specifies the file path to the extension DLL.</param>
            <returns>The handle to a new instance of Extension is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeExtension(System.Int64)">
            <summary>
            Free an instance of an Extension.
            </summary>
            <param name="hExtension">Specifies the handle to the Extension.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RunExtension(System.Int64,System.Int64,`0[])">
            <summary>
            Run a function on the extension specified.
            </summary>
            <param name="hExtension">Specifies the handle to the extension created with CreateExtension.</param>
            <param name="lfnIdx">Specifies the extension function to run.</param>
            <param name="rgParam">Specifies the parameters to pass to the extension.</param>
            <returns>The values returned by the extension are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateTensorDesc">
            <summary>
            Create a new instance of a tensor descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The tensor descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeTensorDesc(System.Int64)">
            <summary>
            Free a tensor descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the tensor descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetTensorNdDesc(System.Int64,System.Int32[],System.Int32[],System.Boolean)">
            <summary>
            Sets the values of a tensor descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the tensor descriptor.</param>
            <param name="rgDim">Specifies the dimensions of the data.</param>
            <param name="rgStride">Specifies the stride of the data.</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetTensorDesc(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Sets the values of a tensor descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the tensor descriptor.</param>
            <param name="n">Specifies the number of items.</param>
            <param name="c">Specifies the number of channels in each item.</param>
            <param name="h">Specifies the height of each item.</param>
            <param name="w">Specifies the width of each item.</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetTensorDesc(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Sets the values of a tensor descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the tensor descriptor.</param>
            <param name="n">Specifies the number of items.</param>
            <param name="c">Specifies the number of channels in each item.</param>
            <param name="h">Specifies the height of each item.</param>
            <param name="w">Specifies the width of each item.</param>
            <param name="nStride">Specifies the stride between two images.</param>
            <param name="cStride">Specifies the stride between two channels.</param>
            <param name="hStride">Specifies the stride between two rows.</param>
            <param name="wStride">Specifies the stride between two columns.</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AddTensor(System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32)">
            <summary>
            Add two tensors together.
            </summary>
            <param name="hCuDnn">Specifies a handle to the cuDnn instance.</param>
            <param name="hSrcDesc">Specifies a handle to the source tensor descriptor.</param>
            <param name="hSrc">Specifies a handle to the source GPU memory.</param>
            <param name="nSrcOffset">Specifies an offset within the GPU memory.</param>
            <param name="hDstDesc">Specifies a handle to the destination tensor descriptor.</param>
            <param name="hDst">Specifies a handle to the desination GPU memory.</param>
            <param name="nDstOffset">Specifies an offset within the GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AddTensor(System.Int64,`0,System.Int64,System.Int64,System.Int32,`0,System.Int64,System.Int64,System.Int32)">
            <summary>
            Add two tensors together.
            </summary>
            <param name="hCuDnn">Specifies a handle to the cuDnn instance.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the source GPU memory before the add.</param>
            <param name="hSrcDesc">Specifies a handle to the source tensor descriptor.</param>
            <param name="hSrc">Specifies a handle to the source GPU memory.</param>
            <param name="nSrcOffset">Specifies an offset within the GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the destination GPU memory before the add.</param>
            <param name="hDstDesc">Specifies a handle to the destination tensor descriptor.</param>
            <param name="hDst">Specifies a handle to the desination GPU memory.</param>
            <param name="nDstOffset">Specifies an offset within the GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateFilterDesc">
            <summary>
            Create a new instance of a filter descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The filter descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeFilterDesc(System.Int64)">
            <summary>
            Free a filter descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the filter descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetFilterNdDesc(System.Int64,System.Int32[],System.Boolean)">
            <summary>
            Sets the values of a filter descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the filter descriptor.</param>
            <param name="rgDim">Specifies the dimensions of the data.</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetFilterDesc(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Sets the values of a filter descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the filter descriptor.</param>
            <param name="n">Specifies the number of items.</param>
            <param name="c">Specifies the number of channels in each item.</param>
            <param name="h">Specifies the height of each item.</param>
            <param name="w">Specifies the width of each item.</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateConvolutionDesc">
            <summary>
            Create a new instance of a convolution descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The convolution descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeConvolutionDesc(System.Int64)">
            <summary>
            Free a convolution descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the convolution descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetConvolutionDesc(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Set the values of a convolution descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the convolution descriptor.</param>
            <param name="hPad">Specifies the pad applied to the height.</param>
            <param name="wPad">Specifies the pad applied to the width.</param>
            <param name="hStride">Specifies the stride of the height.</param>
            <param name="wStride">Specifies the stride of the width.</param>
            <param name="hDilation">Specifies the dilation of the height (default = 1).</param>
            <param name="wDilation">Specifies the dilation of the width (default = 1).</param>
            <param name="bUseTensorCores">Optionally, specifies whether or not to use the Tensor Cores (if available).</param>
            <param name="bHalf">Optionally, specifies whether or not to use the FP16 half data type.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetConvolutionInfo(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.UInt64,System.Boolean,MyCaffe.common.CONV_FWD_ALGO@,System.UInt64@,MyCaffe.common.CONV_BWD_FILTER_ALGO@,System.UInt64@,MyCaffe.common.CONV_BWD_DATA_ALGO@,System.UInt64@,MyCaffe.common.CONV_FWD_ALGO)">
            <summary>
            Queryies the algorithms and workspace sizes used for a given convolution descriptor.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="lWorkspaceSizeLimitInBytes">Specifies the workspace limits (in bytes).</param>
            <param name="bUseTensorCores">Specifies whether or not to use tensor cores (this parameter must match the setting of the 'bUseTensorCores' specified in the 'SetConvolutionDesc' method.</param>
            <param name="algoFwd">Returns the algorithm used for the convolution foward.</param>
            <param name="lWsSizeFwd">Returns the workspace size (in bytes) for the convolution foward.</param>
            <param name="algoBwdFilter">Returns the algorithm used for the backward filter.</param>
            <param name="lWsSizeBwdFilter">Returns the workspace size (int bytes) for the backward filter.</param>
            <param name="algoBwdData">Returns the algorithm for the backward data.</param>
            <param name="lWsSizeBwdData">Returns the workspace (in bytes) for the backward data.</param>
            <param name="preferredFwdAlgo">Optionally, specifies a preferred forward algo to attempt to use for forward convolution.  The new algo is only used if the current device supports it.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionForward(System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_FWD_ALGO,System.Int64,System.Int32,System.UInt64,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeight">Specifies a handle to the weight data in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoFwd">Specifies the algorithm to use for the foward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionForward(System.Int64,`0,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_FWD_ALGO,System.Int64,System.Int32,System.UInt64,`0,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeight">Specifies a handle to the weight data in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoFwd">Specifies the algorithm to use for the foward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardBias(System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the bias.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="hBiasDesc">Specifies a handle to the bias tensor descriptor.</param>
            <param name="hBiasDiff">Specifies a handle to the bias diff in GPU memory.</param>
            <param name="nBiasOffset">Specifies an offset into the diff memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardBias(System.Int64,`0,System.Int64,System.Int64,System.Int32,`0,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the bias.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBiasDesc">Specifies a handle to the bias tensor descriptor.</param>
            <param name="hBiasDiff">Specifies a handle to the bias diff in GPU memory.</param>
            <param name="nBiasOffset">Specifies an offset into the diff memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardFilter(System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_BWD_FILTER_ALGO,System.Int64,System.Int32,System.UInt64,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the filter.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoBwd">Specifies the algorithm to use when performing the backward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeightDiff">Specifies a handle to the weight diff in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardFilter(System.Int64,`0,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_BWD_FILTER_ALGO,System.Int64,System.Int32,System.UInt64,`0,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the filter.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoBwd">Specifies the algorithm to use when performing the backward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeightDiff">Specifies a handle to the weight diff in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardData(System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_BWD_DATA_ALGO,System.Int64,System.Int32,System.UInt64,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the data.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeight">Specifies a handle to the weight data in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoBwd">Specifies the algorithm to use when performing the backward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ConvolutionBackwardData(System.Int64,`0,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,MyCaffe.common.CONV_BWD_DATA_ALGO,System.Int64,System.Int32,System.UInt64,`0,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Perform a convolution backward pass on the data.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hFilterDesc">Specifies a handle to the filter descriptor.</param>
            <param name="hWeight">Specifies a handle to the weight data in GPU memory.</param>
            <param name="nWeightOffset">Specifies an offset into the weight memory (in items, not bytes).</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top memory (in items, not bytes).</param>
            <param name="hConvDesc">Specifies a handle to the convolution descriptor.</param>
            <param name="algoBwd">Specifies the algorithm to use when performing the backward operation.</param>
            <param name="hWorkspace">Specifies a handle to the GPU memory to use for the workspace.</param>
            <param name="nWorkspaceOffset">Specifies an offset into the workspace memory.</param>
            <param name="lWorkspaceSize">Specifies the size of the workspace memory (in bytes).</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="nBottomOffset">Specifies an offset into the bottom memory (in items, not bytes).</param>
            <param name="bSyncStream">Optionally, specifies whether or not to syncrhonize the stream. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreatePoolingDesc">
            <summary>
            Create a new instance of a pooling descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The pooling descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreePoolingDesc(System.Int64)">
            <summary>
            Free a pooling descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the pooling descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetPoolingDesc(System.Int64,MyCaffe.common.PoolingMethod,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Set the values of a pooling descriptor.
            </summary>
            <param name="hHandle">Specifies the handle to the convolution descriptor.</param>
            <param name="method">Specifies the pooling method to use.</param>
            <param name="h">Specifies the pooling area height.</param>
            <param name="w">Specifies the pooling area width.</param>
            <param name="hPad">Specifies the height padding.</param>
            <param name="wPad">Specifies the width padding.</param>
            <param name="hStride">Specifies the height stride.</param>
            <param name="wStride">Specifies the width stride.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.PoolingForward(System.Int64,System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a pooling forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hPoolingDesc">Specifies a handle to the pooling descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.PoolingBackward(System.Int64,System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a pooling backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hPoolingDesc">Specifies a handle to the pooling descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DeriveBatchNormDesc(System.Int64,System.Int64,System.Int64,System.Int64,MyCaffe.common.BATCHNORM_MODE)">
            <summary>
            Derive the batch norm descriptors for both the forward and backward passes.
            </summary>
            <param name="hFwdScaleBiasMeanVarDesc">Specifies a handle to the scale bias mean var tensor descriptor for the forward pass.</param>
            <param name="hFwdBottomDesc">Specifies a handle to the forward bottom tensor descriptor.</param>
            <param name="hBwdScaleBiasMeanVarDesc">Specifies a handle to the scale bias mean var tensor descriptor for the backward pass.</param>
            <param name="hBwdBottomDesc">Specifies a handle to the backward bottom tensor descriptor.</param>
            <param name="mode"></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.BatchNormForward(System.Int64,MyCaffe.common.BATCHNORM_MODE,`0,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Double,System.Int64,System.Int64,System.Double,System.Int64,System.Int64,System.Boolean)">
            <summary>
            Run the batch norm forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="mode">Specifies the batch normalization mode.</param>
            <param name="fAlpha">Specifies the alpha value.</param>
            <param name="fBeta">Specifies the beta value.</param>
            <param name="hFwdBottomDesc">Specifies a handle to the forward bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data tensor.</param>
            <param name="hFwdTopDesc">Specifies a handle to the forward top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top tensor.</param>
            <param name="hFwdScaleBiasMeanVarDesc">Specifies a handle to the forward scale bias mean variance descriptor.</param>
            <param name="hScaleData">Specifies a handle to the scale tensor.</param>
            <param name="hBiasData">Specifies a handle to the bias tensor.</param>
            <param name="dfFactor">Specifies a scaling factor.</param>
            <param name="hGlobalMean">Specifies a handle to the global mean tensor.</param>
            <param name="hGlobalVar">Specifies a handle to the global variance tensor.</param>
            <param name="dfEps">Specifies the epsilon value to avoid dividing by zero.</param>
            <param name="hSaveMean">Specifies a handle to the saved mean tensor.</param>
            <param name="hSaveInvVar">Specifies a handle to the saved variance tensor.</param>
            <param name="bTraining">Specifies that this is a training pass when <i>true</i>, and a testing pass when <i>false</i>.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.BatchNormBackward(System.Int64,MyCaffe.common.BATCHNORM_MODE,`0,`0,`0,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Double,System.Int64,System.Int64)">
            <summary>
            Run the batch norm backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="mode">Specifies the batch normalization mode.</param>
            <param name="fAlphaDiff">Specifies the alpha value applied to the diff.</param>
            <param name="fBetaDiff">Specifies the beta value applied to the diff.</param>
            <param name="fAlphaParamDiff">Specifies the alpha value applied to the param diff.</param>
            <param name="fBetaParamDiff">Specifies the beta value applied to the param diff.</param>
            <param name="hBwdBottomDesc">Specifies a handle to the backward bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data tensor.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff tensor.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff tensor.</param>
            <param name="hBwdScaleBiasMeanVarDesc">Specifies a handle to the backward scale bias mean var descriptor.</param>
            <param name="hScaleData">Specifies a handle to the scale data tensor.</param>
            <param name="hScaleDiff">Specifies a handle to the scale diff tensor.</param>
            <param name="hBiasDiff">Specifies a handle to the bias diff tensor.</param>
            <param name="dfEps">Specifies the epsilon value.</param>
            <param name="hSaveMean">Specifies a handle to the saved mean tensor.</param>
            <param name="hSaveInvVar">Specifies a handle to the saved variance tensor.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateDropoutDesc">
            <summary>
            Create a new instance of a dropout descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The dropout descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeDropoutDesc(System.Int64)">
            <summary>
            Free a dropout descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the dropout descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetDropoutDesc(System.Int64,System.Int64,System.Double,System.Int64,System.Int64)">
            <summary>
            Set the dropout descriptor values.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hDropoutDesc">Specifies a handle to the dropout descriptor.</param>
            <param name="dfDropout">Specifies the droput probability (0.5 = 50%).</param>
            <param name="hStates">Specifies a handle to the state data in GPU memory.</param>
            <param name="lSeed">Specifies the random number-generator seed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetDropoutInfo(System.Int64,System.Int64,System.UInt64@,System.UInt64@)">
            <summary>
            Query the dropout state and reserved counts.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="ulStateCount">Returns the state count.</param>
            <param name="ulReservedCount">Returns the reserved count.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DropoutForward(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a dropout forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hDropoutDesc">Specifies a handle to the dropout descriptor.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hReserved">Specifies a handle to the reseved data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DropoutBackward(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a dropout backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hDropoutDesc">Specifies a handle to the dropout descriptor.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTop">Specifies a handle to the top data in GPU memory.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottom">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hReserved">Specifies a handle to the reseved data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateLRNDesc">
            <summary>
            Create a new instance of a LRN descriptor for use with [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The LRN descriptor handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeLRNDesc(System.Int64)">
            <summary>
            Free a LRN descriptor instance.
            </summary>
            <param name="h">Specifies the handle to the LRN descriptor instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetLRNDesc(System.Int64,System.UInt32,System.Double,System.Double,System.Double)">
            <summary>
            Set the LRN descriptor values.
            </summary>
            <param name="hHandle">Specifies a handle to an LRN descriptor.</param>
            <param name="nSize">Specifies the normalization window width.  Default = 5.</param>
            <param name="fAlpha">Specifies the alpha variance.  Caffe default = 1.0; cuDnn default = 1e-4.</param>
            <param name="fBeta">Specifies the beta power parameter.  Caffe and cuDnn default = 0.75.</param>
            <param name="fK">Specifies the normalization 'k' parameter.  Caffe default = 1.0; cuDnn default = 2.0.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.LRNCrossChannelForward(System.Int64,System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform LRN cross channel forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hNormDesc">Specifies a handle to an LRN descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDesc">Specifies a handle to the bottom tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDesc">Specifies a handle to the top tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.LRNCrossChannelBackward(System.Int64,System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform LRN cross channel backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hNormDesc">Specifies a handle to an LRN descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DivisiveNormalizationForward(System.Int64,System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Performs a Devisive Normalization forward pass.
            </summary>
            <remarks>
            See [What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?](https://arxiv.org/abs/1606.01535) by Jarrett, et al.
            </remarks>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hNormDesc">Specifies a handle to an LRN descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTemp1">Temporary data in GPU memory.</param>
            <param name="hTemp2">Temporary data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.DivisiveNormalizationBackward(System.Int64,System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Performs a Devisive Normalization backward pass.
            </summary>
            <remarks>
            See [What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?](https://arxiv.org/abs/1606.01535) by Jarrett, et al.
            </remarks>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hNormDesc">Specifies a handle to an LRN descriptor.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTemp1">Temporary data in GPU memory.</param>
            <param name="hTemp2">Temporary data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.TanhForward(System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Tanh forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.TanhBackward(System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Tanh backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.EluForward(System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Elu forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.EluBackward(System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Elu backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SigmoidForward(System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Sigmoid forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SigmoidBackward(System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Sigmoid backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ReLUForward(System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a ReLU forward pass.
            </summary>
            <remarks>
            See [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://www.semanticscholar.org/paper/Rectifier-Nonlinearities-Improve-Neural-Network-Maas-Hannun/367f2c63a6f6a10b3b64b8729d601e69337ee3cc) by 
            Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013),  In ICML Workshop on Deep Learning
            for Audio, Speech, and Language Processing.
            </remarks>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ReLUBackward(System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a ReLU backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SoftmaxForward(System.Int64,`0,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Softmax forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hBottomDataDesc">Specifies a handle to the bottom data tensor descriptor.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SoftmaxBackward(System.Int64,`0,System.Int64,System.Int64,System.Int64,System.Int64,`0,System.Int64,System.Int64)">
            <summary>
            Perform a Softmax backward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="fAlpha">Specifies a scaling factor applied to the result.</param>
            <param name="hTopDataDesc">Specifies a handle to the top data tensor descriptor.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hTopDiffDesc">Specifies a handle to the top diff tensor descriptor.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="fBeta">Specifies a scaling factor applied to the prior destination value.</param>
            <param name="hBottomDiffDesc">Specifies a handle to the bottom diff tensor descriptor.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateRnnDataDesc">
            <summary>
            Create the RNN Data Descriptor.
            </summary>
            <returns>A handle to the RNN Data descriptor is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeRnnDataDesc(System.Int64)">
            <summary>
            Free an existing RNN Data descriptor.
            </summary>
            <param name="h">Specifies the handle to the RNN Data descriptor created with CreateRnnDataDesc</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetRnnDataDesc(System.Int64,MyCaffe.common.RNN_DATALAYOUT,System.Int32,System.Int32,System.Int32,System.Int32[])">
            <summary>
            Sets the RNN Data Descriptor values.
            </summary>
            <param name="hRnnDataDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="layout">Specifies the input data layout (either SEQUENCE major or BATCH major).</param>
            <param name="nMaxSeqLen">Specifies the maximum sequence length.</param>
            <param name="nBatchSize">Specifies the batch count.</param>
            <param name="nVectorSize">Specifies the input vector count.</param>
            <param name="rgSeqLen">Specifies the sequence lengths - currently this should be <i>null</i> which sets all sequence lengths to nMaxSeqLen.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateRnnDesc">
            <summary>
            Create the RNN Descriptor.
            </summary>
            <returns>A handle to the RNN descriptor is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeRnnDesc(System.Int64)">
            <summary>
            Free an existing RNN descriptor.
            </summary>
            <param name="h">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetRnnDesc(System.Int64,System.Int64,System.Int32,System.Int32,System.Int64,MyCaffe.common.RNN_MODE,System.Boolean)">
            <summary>
            Sets the RNN Descriptor values.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="nHiddenCount">Specifies the hidden input (typically the input) count.</param>
            <param name="nNumLayers">Specifies the number of layers.</param>
            <param name="hDropoutDesc">Specifies the handle to the Droput descriptor (or 0 to ignore).  The droput descriptor is only used with two or more layers.</param>
            <param name="mode">Specifies the RNN_MODE (LSTM, RNN_RELU, RNN_TANH) to use.</param>
            <param name="bUseTensorCores">Optionally, specifies whether or not to use the Tensor Cores (if available).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetRnnParamCount(System.Int64,System.Int64,System.Int64)">
            <summary>
            Returns the RNN parameter count.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="hXDesc">Specifies the handle to the first X descriptor.</param>
            <returns>The number of parameters (weights) is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetRnnWorkspaceCount(System.Int64,System.Int64,System.Int64,System.Int32@)">
            <summary>
            Returns the workspace and reserved counts.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="hXDesc">Specifies a handle to the data descriptor created with CreateRnnDataDesc.</param>
            <param name="nReservedCount">Returns the reserved count needed.</param>
            <returns>Returns the workspace count needed.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.GetRnnLinLayerParams(System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32@,System.Int64@,System.Int32@,System.Int64@)">
            <summary>
            Returns the linear layer parameters (weights).
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="nLayer">Specifies the current layer index.</param>
            <param name="hXDesc">Specifies the input data elelement descriptor.</param>
            <param name="hWtDesc">Specifies the weight descriptor.</param>
            <param name="hWtData">Specifies the weight memory containing all weights.</param>
            <param name="nLinLayer">Specifies the linear layer index (e.g. LSTM has 8 linear layers, RNN has 2)</param>
            <param name="nWtCount">Returns the number of weight items.</param>
            <param name="hWt">Returns a handle to the weight GPU memory.</param>
            <param name="nBiasCount">Returns the number of bias items.</param>
            <param name="hBias">Returns a handle to the bias GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RnnForward(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Run the RNN through a forward pass.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="hXDesc">Specifies a handle to the input data descriptor.</param>
            <param name="hXData">Specifies a handle to the input GPU data.</param>
            <param name="hHxDesc">Specifies a handle to the hidden data descriptor.</param>
            <param name="hHxData">Specifies a handle to the hidden GPU data.</param>
            <param name="hCxDesc">Specifies a handle to the cont data descriptor.</param>
            <param name="hCxData">Specifies a handle to the cont GPU data.</param>
            <param name="hWtDesc">Specifies a handle to the weight descriptor.</param>
            <param name="hWtData">Specifies a handle to the weight data.</param>
            <param name="hYDesc">Specifies a handle to the output data descriptor.</param>
            <param name="hYData">Specifies a handle to the output GPU data.</param>
            <param name="hHyDesc">Specifies a handle to the output hidden descriptor.</param>
            <param name="hHyData">Specifies a handle to the output hidden data.</param>
            <param name="hCyDesc">Specifies a handle to the output cont descriptor.</param>
            <param name="hCyData">Specifies a handle to the output cont data.</param>
            <param name="hWorkspace">Specifies a handle to the workspace GPU memory.</param>
            <param name="nWsCount">Specifies the number of items within the workspace.</param>
            <param name="hReserved">Specifies a handle to the reserved GPU memory.</param>
            <param name="nResCount">Specifies the number of items within the reserved memory.</param>
            <param name="bTraining">Specifies the whether the forward pass is during taining or not.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RnnBackwardData(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32)">
            <summary>
            Run the RNN backward pass through the data.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="hYDesc">Specifies a handle to the output data descriptor.</param>
            <param name="hYData">Specifies a handle to the output GPU data.</param>
            <param name="hYDiff">Specifies a handle to the output GPU gradients.</param>
            <param name="hHyDesc">Specifies a handle to the output hidden descriptor.</param>
            <param name="hHyDiff">Specifies a handle to the output hidden gradients.</param>
            <param name="hCyDesc">Specifies a handle to the output cont descriptor.</param>
            <param name="hCyDiff">Specifies a handle to the output cont gradients.</param>
            <param name="hWtDesc">Specifies a handle to the weight descriptor.</param>
            <param name="hWtData">Specifies a handle to the weight data.</param>
            <param name="hHxDesc">Specifies a handle to the hidden data descriptor.</param>
            <param name="hHxData">Specifies a handle to the hidden GPU data.</param>
            <param name="hCxDesc">Specifies a handle to the cont data descriptor.</param>
            <param name="hCxData">Specifies a handle to the cont GPU data.</param>
            <param name="hXDesc">Specifies a handle to the input data descriptor.</param>
            <param name="hXDiff">Specifies a handle to the input GPU gradients.</param>
            <param name="hdHxDesc">Specifies a handle to the input hidden descriptor for the gradients.</param>
            <param name="hHxDiff">Specifis a handle to the input hidden GPU gradients.</param>
            <param name="hdCxDesc">Specifies a handle to the input cont descriptor of the gradients.</param>
            <param name="hCxDiff">Specifies a handle to the input cont GPU gradients.</param>
            <param name="hWorkspace">Specifies a handle to the workspace GPU memory.</param>
            <param name="nWsCount">Specifies the number of items within the workspace.</param>
            <param name="hReserved">Specifies a handle to the reserved GPU memory.</param>
            <param name="nResCount">Specifies the number of items within the reserved memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RnnBackwardWeights(System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Run the RNN backward pass on the weights.
            </summary>
            <param name="hCuDnn">Specifies a handle to the instance of cuDnn.</param>
            <param name="hRnnDesc">Specifies the handle to the RNN descriptor created with CreateRnnDesc</param>
            <param name="hXDesc">Specifies a handle to the input data descriptor.</param>
            <param name="hXData">Specifies a handle to the input GPU data.</param>
            <param name="hHxDesc">Specifies a handle to the hidden data descriptor.</param>
            <param name="hHxData">Specifies a handle to the hidden GPU data.</param>
            <param name="hYDesc">Specifies a handle to the output data descriptor.</param>
            <param name="hYData">Specifies a handle to the output GPU data.</param>
            <param name="hWorkspace">Specifies a handle to the workspace GPU memory.</param>
            <param name="nWsCount">Specifies the number of items within the workspace.</param>
            <param name="hWtDesc">Specifies a handle to the weight descriptor.</param>
            <param name="hWtDiff">Specifies a handle to the weight gradients.</param>
            <param name="hReserved">Specifies a handle to the reserved GPU memory.</param>
            <param name="nResCount">Specifies the number of items within the reserved memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocPCAData(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Allocates the GPU memory for the PCA Data.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="nM">Specifies the data width (number of rows).</param>
            <param name="nN">Specifies the data height (number of columns).</param>
            <param name="nK">Specifies the number of components (K &lt;= N).</param>
            <param name="nCount">Returns the total number of items in the allocated data (nM * nN).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocPCAScores(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Allocates the GPU memory for the PCA scores.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="nM">Specifies the data width (number of rows).</param>
            <param name="nN">Specifies the data height (number of columns).</param>
            <param name="nK">Specifies the number of components (K &lt;= N).</param>
            <param name="nCount">Returns the total number of items in the allocated data (nM * nN).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocPCALoads(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Allocates the GPU memory for the PCA loads.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="nM">Specifies the data width (number of rows).</param>
            <param name="nN">Specifies the data height (number of columns).</param>
            <param name="nK">Specifies the number of components (K &lt;= N).</param>
            <param name="nCount">Returns the total number of items in the allocated data (nM * nN).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.AllocPCAEigenvalues(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Allocates the GPU memory for the PCA eigenvalues.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="nM">Specifies the data width (number of rows).</param>
            <param name="nN">Specifies the data height (number of columns).</param>
            <param name="nK">Specifies the number of components (K &lt;= N).</param>
            <param name="nCount">Returns the total number of items in the allocated data (nM * nN).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreatePCA(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Creates a new PCA instance and returns the handle to it.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="nMaxIterations">Specifies the number of iterations to run.</param>
            <param name="nM">Specifies the data width (number of rows).</param>
            <param name="nN">Specifies the data height (number of columns).</param>
            <param name="nK">Specifies the number of components (K less than or equal to N).</param>
            <param name="hData">Specifies a handle to the data allocated using AllocatePCAData.</param>
            <param name="hScoresResult">Specifies a handle to the data allocated using AllocatePCAScores.</param>
            <param name="hLoadsResult">Specifies a handle to the data allocated using AllocatePCALoads.</param>
            <param name="hResiduals">Specifies a handle to the data allocated using AllocatePCAData.</param>
            <param name="hEigenvalues">Specifies a handle to the data allocated using AllocatePCAEigenvalues.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.RunPCA(System.Int64,System.Int32,System.Int32@,System.Int32@)">
            <summary>
            Runs a number of steps of the iterative PCA algorithm.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="hPCA">Specifies a handle to the PCA instance to use.</param>
            <param name="nSteps">Specifies the number of steps to run.</param>
            <param name="nCurrentK">Returns the current component value.</param>
            <param name="nCurrentIteration">Returns the current iteration.</param>
            <returns><code>true</code> is returned when the maximum number of iterations have been run as specified in CreatePCA.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreePCA(System.Int64)">
            <summary>
            Free the PCA instance associated with handle.
            </summary>
            <remarks>
            See [Parallel GPU Implementation of Iterative PCA Algorithms](https://arxiv.org/abs/0811.1081) by Mircea Andrecut
            </remarks>
            <param name="hPCA">Specifies a handle to the PCA instance to free.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.CreateSSD(System.Int32,System.Boolean,System.Int32,System.Int32,System.Boolean,MyCaffe.common.SSD_MINING_TYPE,MyCaffe.common.SSD_MATCH_TYPE,System.Single,System.Boolean,MyCaffe.common.SSD_CODE_TYPE,System.Boolean,System.Boolean,System.Boolean,System.Boolean,MyCaffe.common.SSD_CONF_LOSS_TYPE,MyCaffe.common.SSD_LOC_LOSS_TYPE,System.Single,System.Single,System.Int32,System.Boolean,System.Boolean,System.Nullable{System.Single},System.Nullable{System.Int32},System.Nullable{System.Single})">
            <summary>
            Create an instance of the SSD GPU support.
            </summary>
            <param name="nNumClasses">Specifies the number of classes.</param>
            <param name="bShareLocation">Specifies whether or not to share the location.</param>
            <param name="nLocClasses">Specifies the number of location classes.</param>
            <param name="nBackgroundLabelId">Specifies the background label ID.</param>
            <param name="bUseDiffcultGt">Specifies whether or not to use difficult ground truths.</param>
            <param name="miningType">Specifies the mining type to use.</param>
            <param name="matchType">Specifies the matching method to use.</param>
            <param name="fOverlapThreshold">Specifies the overlap threshold for each box.</param>
            <param name="bUsePriorForMatching">Specifies whether or not to use priors for matching.</param>
            <param name="codeType">Specifies the code type to use.</param>
            <param name="bEncodeVariantInTgt">Specifies whether or not to encode the variant in the target.</param>
            <param name="bBpInside">Specifies whether or not the BP is inside or not.</param>
            <param name="bIgnoreCrossBoundaryBbox">Specifies whether or not to ignore cross boundary boxes.</param>
            <param name="bUsePriorForNms">Specifies whether or not to use priors for NMS.</param>
            <param name="confLossType">Specifies the confidence loss type.</param>
            <param name="locLossType">Specifies the location loss type.</param>
            <param name="fNegPosRatio">Specifies the negative/positive ratio to use.</param>
            <param name="fNegOverlap">Specifies the negative overlap to use.</param>
            <param name="nSampleSize">Specifies the sample size.</param>
            <param name="bMapObjectToAgnostic">Specifies whether or not to map objects to agnostic or not.</param>
            <param name="bNmsParam">Specifies whether or not the NMS parameters are specified.</param>
            <param name="fNmsThreshold">Specifies the NMS threshold, which is only used when the 'bNmsParam' = true.</param>
            <param name="nNmsTopK">Specifies the NMS top-k selection, which is only used when the 'bNmsParam' = true.</param>
            <param name="fNmsEta">Specifies the NMS eta, which is only used when the 'bNmsParam' = true.</param>
            <returns>A handle to the SSD instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SetupSSD(System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            Setup the SSD GPU support.
            </summary>
            <param name="hSSD">Specifies the handle to the SSD instance.</param>
            <param name="nNum">Specifies the number of items.</param>
            <param name="nNumPriors">Specifies the number of priors.</param>
            <param name="nNumGt">Specifies the number of ground truths.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.FreeSSD(System.Int64)">
            <summary>
            Free the instance of SSD GPU support.
            </summary>
            <param name="hSSD">Specifies the handle to the SSD instance.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SsdMultiBoxLossForward(System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Collections.Generic.List{MyCaffe.basecode.DictionaryMap{System.Collections.Generic.List{System.Int32}}}@,System.Collections.Generic.List{System.Collections.Generic.List{System.Int32}}@,System.Int32@)">
            <summary>
            Performs the SSD MultiBoxLoss forward operation.
            </summary>
            <param name="hSSD">Specifies the handle to the SSD instance.</param>
            <param name="nLocDataCount">Specifies the number of location data items.</param>
            <param name="hLocGpuData">Specifies the handle to the location data in GPU memory.</param>
            <param name="nConfDataCount">Specifies the number of confidence data items.</param>
            <param name="hConfGpuData">Specifies the handle to the confidence data in GPU memory.</param>
            <param name="nPriorDataCount">Specifies the number of prior box data.</param>
            <param name="hPriorGpuData">Specifies the prior box data in GPU memory.</param>
            <param name="nGtDataCount">Specifies the number of ground truth items.</param>
            <param name="hGtGpuData">Specifies the ground truth data in GPU memory.</param>
            <param name="rgAllMatchIndices">Returns all match indices found.</param>
            <param name="rgrgAllNegIndices">Returns all neg indices found.</param>
            <param name="nNumNegs">Returns the number of negatives.</param>
            <returns>The number of matches is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SsdEncodeLocPrediction(System.Int64,System.Int32,System.Int64,System.Int32,System.Int64)">
            <summary>
            Encodes the SSD data into the location prediction and location ground truths.
            </summary>
            <param name="hSSD">Specifies the handle to the SSD instance.</param>
            <param name="nLocPredCount">Specifies the number of location prediction items.</param>
            <param name="hLocPred">Specifies the location prediction data in GPU memory.</param>
            <param name="nLocGtCount">Specifies the location ground truth items.</param>
            <param name="hLocGt">Specifies the location ground truth data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.SsdEncodeConfPrediction(System.Int64,System.Int32,System.Int64,System.Int32,System.Int64)">
            <summary>
            Encodes the SSD data into the confidence prediction and confidence ground truths.
            </summary>
            <param name="hSSD">Specifies the handle to the SSD instance.</param>
            <param name="nConfPredCount">Specifies the number of confidence prediction items.</param>
            <param name="hConfPred">Specifies the confidence prediction data in GPU memory.</param>
            <param name="nConfGtCount">Specifies the confidence ground truth items.</param>
            <param name="hConfGt">Specifies the confidence ground truth data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.set(System.Int32,System.Int64,System.Double,System.Int32)">
            <summary>
            Set the values of GPU memory to a specified value of type <code>double</code>.
            </summary>
            <param name="nCount">Specifies the number of items to set.</param>
            <param name="hHandle">Specifies a handle to the memory on the GPU.</param>
            <param name="fVal">Specifies the value to set.</param>
            <param name="nIdx">When -1, all values in the GPU memory are set to the <i>fVal</i> value, otherwise, only the value at the index <i>nIdx</i> is set to the value.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.set(System.Int32,System.Int64,System.Single,System.Int32)">
            <summary>
            Set the values of GPU memory to a specified value of type <code>float</code>.
            </summary>
            <param name="nCount">Specifies the number of items to set.</param>
            <param name="hHandle">Specifies a handle to the memory on the GPU.</param>
            <param name="fVal">Specifies the value to set.</param>
            <param name="nIdx">When -1, all values in the GPU memory are set to the <i>fVal</i> value, otherwise, only the value at the index <i>nIdx</i> is set to the value.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.set(System.Int32,System.Int64,`0,System.Int32,System.Int32)">
            <summary>
            Set the values of GPU memory to a specified value of type 'T'.
            </summary>
            <param name="nCount">Specifies the number of items to set.</param>
            <param name="hHandle">Specifies a handle to the memory on the GPU.</param>
            <param name="fVal">Specifies the value to set.</param>
            <param name="nIdx">When -1, all values in the GPU memory are set to the <i>fVal</i> value, otherwise, only the value at the index <i>nIdx</i> is set to the value.</param>
            <param name="nXOff">Optionally specifies an offset into the GPU memory where the <i>set</i> starts.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.get_double(System.Int32,System.Int64,System.Int32)">
            <summary>
            Queries the GPU memory by copying it into an array of <code>double</code>
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hHandle">Specifies a handle to GPU memory.</param>
            <param name="nIdx">When -1, all values in the GPU memory are queried, otherwise, only the value at the index <i>nIdx</i> is returned.</param>
            <returns>An array of <code>double</code> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.get_float(System.Int32,System.Int64,System.Int32)">
            <summary>
            Queries the GPU memory by copying it into an array of <code>float</code>
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hHandle">Specifies a handle to GPU memory.</param>
            <param name="nIdx">When -1, all values in the GPU memory are queried, otherwise, only the value at the index <i>nIdx</i> is returned.</param>
            <returns>An array of <code>float</code> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.get(System.Int32,System.Int64,System.Int32)">
            <summary>
            Queries the GPU memory by copying it into an array of type 'T'.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hHandle">Specifies a handle to GPU memory.</param>
            <param name="nIdx">When -1, all values in the GPU memory are queried, otherwise, only the value at the index <i>nIdx</i> is returned.</param>
            <returns>An array of <code>T</code> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.copy(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int64,System.Nullable{System.Boolean},System.Nullable{System.Boolean})">
            <summary>
            Copy data from one block of GPU memory to another.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="nCount">Specifies the number of items (not bytes) to copy.</param>
            <param name="hSrc">Specifies a handle to GPU memory containing the source data.</param>
            <param name="hDst">Specifies a handle to GPU memory containing the destination data.</param>
            <param name="nSrcOffset">Optionally specifies the offset into the source data where the copying starts.</param>
            <param name="nDstOffset">Optionally specifies the offset into the destination data where the copying starts.</param>
            <param name="hStream">Optionally, specifies a handle to a stream to use for the operation.</param>
            <param name="bSrcHalfSizeOverride">Optionally, specifies and override for the half size state of the source (default = null, which is ignored).</param>
            <param name="bDstHalfSizeOverride">Optionally, specifies and override for the half size state of the destination (default = null, which is ignored).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.copy(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Boolean)">
            <summary>
            Copy similar items of length 'nDim' from hSrc1 (where hSimilar(i) = 1) and dissimilar items of length 'nDim' from hSrc2 (where hSimilar(i) = 0).
            </summary>
            <param name="nCount">Specifies the total data length of hSrc1, hSrc2 and hDst.</param>
            <param name="nNum">Specifis the number of outer items in hSrc1, hSrc2, hDst, and the number of elements in hSimilar.</param>
            <param name="nDim">Specifies the inner dimension of hSrc1, hSrc2 and hDst.</param>
            <param name="hSrc1">Specifies a handle to the GPU memory of source 1.</param>
            <param name="hSrc2">Specifies a handle to the GPU memory of source 2.</param>
            <param name="hDst">Specifies a handle to the GPU memory of the destination.</param>
            <param name="hSimilar">Specifies a handle to the GPU memory of the similar data.</param>
            <param name="bInvert">Optionally, specifies whether or not to invert the similar values (e.g. copy when similar = 0 instead of similar = 1)</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.copy_batch(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Copy a batch of labeled items into a cache organized by label where older data is removed and replaced by newer data.
            </summary>
            <param name="nCount">Specifies the total data length of hSrc.</param>
            <param name="nNum">Specifis the number of outer items in hSrc1, hSrc2, hDst, and the number of elements in hSimilar.</param>
            <param name="nDim">Specifies the inner dimension of hSrc1, hSrc2 and hDst.</param>
            <param name="hSrcData">Specifies a handle to the GPU memory of source data.</param>
            <param name="hSrcLbl">Specifies a handle to the GPU memory of source labels.</param>
            <param name="nDstCount">Specifies the total data length of the hDstCache</param>
            <param name="hDstCache">Specifies a handle to the GPU memory of the destination cache.</param>
            <param name="hWorkDevData">Specifies a handle to the GPU memory of the device work data that is the same size as the hDstCache.</param>
            <param name="nLabelStart">Specifies the first label of all possible labels.</param>
            <param name="nLabelCount">Specifies the total number of labels (expects labels to be sequential from 'nLabelStart').</param>
            <param name="nCacheSize">Specifies the size of each labeled data cache.</param>
            <param name="hCacheHostCursors">Specifies a handle to host memmory (allocated using AllocateHostBuffer) containing the label cursors - there should be 'nLabelCount' cursors.</param>
            <param name="hWorkDataHost">Specifies a handle to host memory (allocated using AllocateHostBuffer) used for work - must be nNum in item length.</param>
            <remarks>
            NOTE: The cache size must be set at a sufficient size that covers the maximum number items for any given label within a batch, otherwise cached items will be overwritten for items in the current batch.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.copy_sequence(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64,System.Boolean,System.Collections.Generic.List{System.Int64},System.Collections.Generic.List{System.Int32},System.Int64,System.Boolean,System.Int32)">
            <summary>
            Copy a sequence of cached items, organized by label, into an anchor, positive (if nK > 0), and negative blobs.
            </summary>
            <param name="nK">Specifies the output type expected where: nK = 0, outputs to 2 tops (anchor and one negative), or nK > 0, outputs to 2 + nK tops (anchor, positive, nK negatives).  The rghTop and rgnTopCount must be sized accordingly.</param>
            <param name="nNum">Specifis the number of outer items in hSrc1, hSrc2, hDst, and the number of elements in hSimilar.</param>
            <param name="nDim">Specifies the inner dimension of hSrc1, hSrc2 and hDst.</param>
            <param name="hSrcData">Specifies a handle to the GPU memory of source data.</param>
            <param name="hSrcLbl">Specifies a handle to the GPU memory of source labels.</param>
            <param name="nSrcCacheCount">Specifis the number of items in hSrcCache (nCacheSize * nLabelCount).</param>
            <param name="hSrcCache">Specifies a handle to the cached labeled data.</param>
            <param name="nLabelStart">Specifies the first label of all possible labels.</param>
            <param name="nLabelCount">Specifies the total number of labels (expects labels to be sequential from 'nLabelStart').</param>
            <param name="nCacheSize">Specifies the size of each labeled data cache.</param>
            <param name="hCacheHostCursors">Specifies a handle to host memmory containing the label cursors - there should be 'nLabelCount' cursors.</param>
            <param name="bOutputLabels">Specifies whether or not to output labels or not.  When true, one additional top is expected for the labels.</param>
            <param name="rghTop">Specifies a list of the GPU memory for each top item.  The number of top items expected depends on the 'nK' value.</param>
            <param name="rgnTopCount">Specifies a list of the item count for each top item.  The number of top items expected depends on the 'nK' value.</param>
            <param name="hWorkDataHost">Specifies a handle to host memory (allocated using AllocateHostBuffer) used for work - must be nNum in item length and must be the same hWorkDataHost passed to 'copy_batch'.</param>
            <param name="bCombinePositiveAndNegative">Optionally, specifies to combine the positive and negative items by alternating between each and placing both in Top[1], while also making sure the output labels reflect the alternation.</param>
            <param name="nSeed">Optionally, specifies a seed for the random number generator (default = 0, which igores this parameter).</param>
            <remarks>
            Receiving an error ERROR_BATCH_TOO_SMALL indicates that the batch size is too small and does not have enough labels to choose from.  Each batch should have at least two instances of each labeled item.
            
            NOTE: When 'nK' = 1 and 'bCombinePositiveAndNegative' = true, the label output has a dimension of 2, and and the tops used are as follows: top(0) = anchor; top(1) = alternating negative/positive, top(2) = labels if 'bOutputLabels' = true.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.copy_expand(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Expand a vector of length 'nNum' into a matrix of size 'nNum' x 'nDim' by copying each value of the vector
            into all elements of the corresponding matrix row.
            </summary>
            <param name="n">Specifies the total number of items in the matrix 'A'</param>
            <param name="nNum">Specifies the total number of rows in the matrix 'A' and the total number of items in the vector 'X'.</param>
            <param name="nDim">Specifies the total number of columns in the matrix 'A'.</param>
            <param name="hX">Specifies the 'nNum' length vector to expand.</param>
            <param name="hA">Specifies the 'nNum' x 'nDim' matrix.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.fill(System.Int32,System.Int32,System.Int64,System.Int32,System.Int32,System.Int64)">
            <summary>
            Fill data from the source data 'n' times in the destination.
            </summary>
            <param name="n">Specifies the number of times to copy the source data.</param>
            <param name="nDim">Specifies the number of source items to copy.</param>
            <param name="hSrc">Specifies a handle to the GPU memory of the source data.</param>
            <param name="nSrcOff">Specifies an offset into the GPU memory where the source data copy starts.</param>
            <param name="nCount">Specifies the total number of items in the destination.  This value must be >= n * nDim.</param>
            <param name="hDst">Specifies the handle to the GPU memory where the data is to be copied.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sort(System.Int32,System.Int64)">
            <summary>
            Sort the data in the GPU memory specified.
            </summary>
            <param name="nCount">Specifies the total number of items in the memory.</param>
            <param name="hY">Specifies the handle to the GPU memory of data to sort.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemm(System.Boolean,System.Boolean,System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Double,System.Int64)">
            <summary>
            Perform a matrix-matrix multiplication operation: C = alpha transB (B) transA (A) + beta C 
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="bTransB">Specifies whether or not to transpose B.</param>
            <param name="m">Specifies the width (number of columns) of A and C.</param>
            <param name="n">Specifies the height (number of rows) of B and C.</param>
            <param name="k">Specifies the width (number of columns) of A and B.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type <code>double</code></param>
            <param name="hA">Specifies a handle to the data for A in GPU memory.</param>
            <param name="hB">Specifies a handle to the data for B in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by C where the scalar is of type <code>double</code></param>
            <param name="hC">Specifies a handle to the data for C in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemm(System.Boolean,System.Boolean,System.Int32,System.Int32,System.Int32,System.Single,System.Int64,System.Int64,System.Single,System.Int64)">
            <summary>
            Perform a matrix-matrix multiplication operation: C = alpha transB (B) transA (A) + beta C 
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="bTransB">Specifies whether or not to transpose B.</param>
            <param name="m">Specifies the width (number of columns) of A and C.</param>
            <param name="n">Specifies the height (number of rows) of B and C.</param>
            <param name="k">Specifies the width (number of columns) of A and B.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type <code>float</code></param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hB">Specifies a handle to the data for matrix B in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by C where the scalar is of type <code>float</code></param>
            <param name="hC">Specifies a handle to the data for matrix C in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemm(System.Boolean,System.Boolean,System.Int32,System.Int32,System.Int32,`0,System.Int64,System.Int64,`0,System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a matrix-matrix multiplication operation: C = alpha transB (B) transA (A) + beta C 
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="bTransB">Specifies whether or not to transpose B.</param>
            <param name="m">Specifies the width (number of columns) of A and C.</param>
            <param name="n">Specifies the height (number of rows) of B and C.</param>
            <param name="k">Specifies the width (number of columns) of A and B.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type 'T'.</param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hB">Specifies a handle to the data for matrix B in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by C where the scalar is of type 'T'.</param>
            <param name="hC">Specifies a handle to the data for matrix C in GPU memory.</param>
            <param name="nAOffset">Specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOffset">Specifies an offset (in items, not bytes) into the memory of B.</param>
            <param name="nCOffset">Specifies an offset (in items, not bytes) into the memory of C.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemm(System.Boolean,System.Boolean,System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Double,System.Int64,System.UInt32,System.UInt32,System.UInt32)">
            <summary>
            Perform a matrix-matrix multiplication operation: C = alpha transB (B) transA (A) + beta C 
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="bTransB">Specifies whether or not to transpose B.</param>
            <param name="m">Specifies the width (number of columns) of A and C.</param>
            <param name="n">Specifies the height (number of rows) of B and C.</param>
            <param name="k">Specifies the width (number of columns) of A and B.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type 'T'.</param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hB">Specifies a handle to the data for matrix B in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by C where the scalar is of type 'T'.</param>
            <param name="hC">Specifies a handle to the data for matrix C in GPU memory.</param>
            <param name="lda">Specifies the leading dimension of A.</param>
            <param name="ldb">Specifies the leading dimension of B.</param>
            <param name="ldc">Specifies the leading dimension of C.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemv(System.Boolean,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Double,System.Int64)">
            <summary>
            Perform a matrix-vector multiplication operation: y = alpha transA (A) x + beta y (where x and y are vectors)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="m">Specifies the width (number of columns) of A.</param>
            <param name="n">Specifies the height (number of rows) of A.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type <code>double</code></param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hX">Specifies a handle to the data for vector x in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by y where the scalar is of type <code>double</code></param>
            <param name="hY">Specifies a handle to the data for vectory y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemv(System.Boolean,System.Int32,System.Int32,System.Single,System.Int64,System.Int64,System.Single,System.Int64)">
            <summary>
            Perform a matrix-vector multiplication operation: y = alpha transA (A) x + beta y (where x and y are vectors)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="m">Specifies the width (number of columns) of A.</param>
            <param name="n">Specifies the height (number of rows) of A.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type <code>float</code></param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hX">Specifies a handle to the data for vector x in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by y where the scalar is of type <code>float</code></param>
            <param name="hY">Specifies a handle to the data for vectory y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gemv(System.Boolean,System.Int32,System.Int32,`0,System.Int64,System.Int64,`0,System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a matrix-vector multiplication operation: y = alpha transA (A) x + beta y (where x and y are vectors)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="bTransA">Specifies whether or not to transpose A.</param>
            <param name="m">Specifies the width (number of columns) of A.</param>
            <param name="n">Specifies the height (number of rows) of A.</param>
            <param name="fAlpha">Specifies a scalar multiplied by the data where the scalar is of type 'T'.</param>
            <param name="hA">Specifies a handle to the data for matrix A in GPU memory.</param>
            <param name="hX">Specifies a handle to the data for vector X in GPU memory.</param>
            <param name="fBeta">Specifies a scalar multiplied by Y where the scalar is of type 'T'</param>
            <param name="hY">Specifies a handle to the data for vectory y in GPU memory.</param>
            <param name="nAOffset">Specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nXOffset">Specifies an offset (in items, not bytes) into the memory of X.</param>
            <param name="nYOffset">Specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ger(System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Int64)">
            <summary>
            Perform a vector-vector multiplication operation: A = x * (fAlpha * y) (where x and y are vectors and A is an m x n Matrix)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="m">Specifies the length of X and rows in A (m x n).</param>
            <param name="n">Specifies the length of Y and cols in A (m x n).</param>
            <param name="fAlpha">Specifies a scalar multiplied by y where the scalar is of type 'T'.</param>
            <param name="hX">Specifies a handle to the data for matrix X (m in length) in GPU memory.</param>
            <param name="hY">Specifies a handle to the data for vector Y (n in length) in GPU memory.</param>
            <param name="hA">Specifies a handle to the data for matrix A (m x n) in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ger(System.Int32,System.Int32,System.Single,System.Int64,System.Int64,System.Int64)">
            <summary>
            Perform a vector-vector multiplication operation: A = x * (fAlpha * y) (where x and y are vectors and A is an m x n Matrix)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="m">Specifies the length of X and rows in A (m x n).</param>
            <param name="n">Specifies the length of Y and cols in A (m x n).</param>
            <param name="fAlpha">Specifies a scalar multiplied by y where the scalar is of type 'T'.</param>
            <param name="hX">Specifies a handle to the data for matrix X (m in length) in GPU memory.</param>
            <param name="hY">Specifies a handle to the data for vector Y (n in length) in GPU memory.</param>
            <param name="hA">Specifies a handle to the data for matrix A (m x n) in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.ger(System.Int32,System.Int32,`0,System.Int64,System.Int64,System.Int64)">
            <summary>
            Perform a vector-vector multiplication operation: A = x * (fAlpha * y) (where x and y are vectors and A is an m x n Matrix)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas) but with a different parameter ordering.
            </remarks>
            <param name="m">Specifies the length of X and rows in A (m x n).</param>
            <param name="n">Specifies the length of Y and cols in A (m x n).</param>
            <param name="fAlpha">Specifies a scalar multiplied by y where the scalar is of type 'T'.</param>
            <param name="hX">Specifies a handle to the data for matrix X (m in length) in GPU memory.</param>
            <param name="hY">Specifies a handle to the data for vector Y (n in length) in GPU memory.</param>
            <param name="hA">Specifies a handle to the data for matrix A (m x n) in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpy(System.Int32,System.Double,System.Int64,System.Int64)">
            <summary>
            Multiply the vector X by a scalar and add the result to the vector Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type <code>double</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpy(System.Int32,System.Single,System.Int64,System.Int64)">
            <summary>
            Multiply the vector X by a scalar and add the result to the vector Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type <code>float</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpy(System.Int32,`0,System.Int64,System.Int64,System.Int32,System.Int32)">
            <summary>
            Multiply the vector X by a scalar and add the result to the vector Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type 'T'.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpby(System.Int32,System.Double,System.Int64,System.Double,System.Int64)">
            <summary>
            Scale the vector x and then multiply the vector X by a scalar and add the result to the vector Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type <code>double</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="fBeta">Specifies the scaling factor to apply to vector X, where the scaling factor is of type <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpby(System.Int32,System.Single,System.Int64,System.Single,System.Int64)">
            <summary>
            Scale the vector x and then multiply the vector X by a scalar and add the result to the vector Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type <code>float</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="fBeta">Specifies the scaling factor to apply to vector X, where the scaling factor is of type <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.axpby(System.Int32,`0,System.Int64,`0,System.Int64)">
            <summary>
            Scale the vector x by Alpha and scale vector y by Beta and then add both together.
            
            Y = (X * fAlpha) + (Y * fBeta)
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scalar to multiply where the scalar is of type 'T'.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="fBeta">Specifies the scaling factor to apply to vector X, where the scaling factor is of type 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.mulbsx(System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Boolean,System.Int64,System.Int32)">
            <summary>
            Multiply a matrix with a vector.
            </summary>
            <param name="n">Specifies the number of items.</param>
            <param name="hA">Specifies the matrix to multiply.</param>
            <param name="nAOff">Specifies the offset to apply to the GPU memory of hA.</param>
            <param name="hX">Specifies the vector to multiply.</param>
            <param name="nXOff">Specifies the offset to apply to the GPU memory of hX.</param>
            <param name="nC">Specifies the number of channels.</param>
            <param name="nSpatialDim">Specifies the spatial dimension.</param>
            <param name="bTranspose">Specifies whether or not to transpose the matrix.</param>
            <param name="hB">Specifies the output matrix.</param>
            <param name="nBOff">Specifies the offset to apply to the GPU memory of hB.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.divbsx(System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Boolean,System.Int64,System.Int32)">
            <summary>
            Divide a matrix by a vector.
            </summary>
            <param name="n">Specifies the number of items.</param>
            <param name="hA">Specifies the matrix to divide.</param>
            <param name="nAOff">Specifies the offset to apply to the GPU memory of hA.</param>
            <param name="hX">Specifies the divisor vector.</param>
            <param name="nXOff">Specifies the offset to apply to the GPU memory of hX.</param>
            <param name="nC">Specifies the number of channels.</param>
            <param name="nSpatialDim">Specifies the spatial dimension.</param>
            <param name="bTranspose">Specifies whether or not to transpose the matrix.</param>
            <param name="hB">Specifies the output matrix.</param>
            <param name="nBOff">Specifies the offset to apply to the GPU memory of hB.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.set_bounds(System.Int32,System.Double,System.Double,System.Int64)">
            <summary>
            Set the bounds of all items within the data to a set range of values.
            </summary>
            <param name="n">Specifies the number of items.</param>
            <param name="dfMin">Specifies the minimum value.</param>
            <param name="dfMax">Specifies the maximum value.</param>
            <param name="hX">Specifies a handle to the GPU data to be bound.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scal(System.Int32,System.Double,System.Int64,System.Int32)">
            <summary>
            Scales the data in X by a scaling factor.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scaling factor to apply to vector X, where the scaling factor is of type <code>double</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Specifies an offset (in items, not bytes) into the memory of X.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scal(System.Int32,System.Single,System.Int64,System.Int32)">
            <summary>
            Scales the data in X by a scaling factor.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scaling factor to apply to vector X, where the scaling factor is of type <code>float</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Specifies an offset (in items, not bytes) into the memory of X.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scal(System.Int32,`0,System.Int64,System.Int32)">
            <summary>
            Scales the data in X by a scaling factor.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scaling factor to apply to vector X, where the scaling factor is of type 'T'.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Specifies an offset (in items, not bytes) into the memory of X.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.dot_double(System.Int32,System.Int64,System.Int64)">
            <summary>
            Computes the dot product of X and Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <returns>The dot product is returned as a type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.dot_float(System.Int32,System.Int64,System.Int64)">
            <summary>
            Computes the dot product of X and Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <returns>The dot product is returned as a type <code>float</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.dot(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32)">
            <summary>
            Computes the dot product of X and Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
            <returns>The dot product is returned as a type 'T'.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.asum_double(System.Int32,System.Int64,System.Int32)">
            <summary>
            Computes the sum of absolute values in X.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <returns>the absolute sum is returned as a type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.asum_float(System.Int32,System.Int64,System.Int32)">
            <summary>
            Computes the sum of absolute values in X.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <returns>the absolute sum is returned as a type <code>float</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.asum(System.Int32,System.Int64,System.Int32)">
            <summary>
            Computes the sum of absolute values in X.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <returns>the absolute value sum is returned as a type 'T'.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scale(System.Int32,System.Double,System.Int64,System.Int64)">
            <summary>
            Scales the values in X and places them in Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scale value in type <code>double</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scale(System.Int32,System.Single,System.Int64,System.Int64)">
            <summary>
            Scales the values in X and places them in Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scale value in type <code>float</code></param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scale(System.Int32,`0,System.Int64,System.Int64,System.Int32,System.Int32)">
            <summary>
            Scales the values in X and places them in Y.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuBlas](https://developer.nvidia.com/cublas).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X and Y.</param>
            <param name="fAlpha">Specifies the scale value in type 'T'.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset (in items, not bytes) into the memory of X.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add_scalar(System.Int32,System.Double,System.Int64)">
            <summary>
            Adds a scalar value to each element of Y.
            </summary>
            <remarks>
            Y = Y + alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector Y.</param>
            <param name="fAlpha">Specifies the scalar value in type <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add_scalar(System.Int32,System.Single,System.Int64)">
            <summary>
            Adds a scalar value to each element of Y.
            </summary>
            <remarks>
            Y = Y + alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector Y.</param>
            <param name="fAlpha">Specifies the scalar value in type <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add_scalar(System.Int32,`0,System.Int64,System.Int32)">
            <summary>
            Adds a scalar value to each element of Y.
            </summary>
            <remarks>
            Y = Y + alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector Y.</param>
            <param name="fAlpha">Specifies the scalar value in type 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nYOff">Optionally, specifies an offset into Y.  The default is 0.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Adds A to B and places the result in Y.
            </summary>
            <remarks>
            Y = A + B
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add(System.Int32,System.Int64,System.Int64,System.Int64,System.Double)">
            <summary>
            Adds A to (B times scalar) and places the result in Y. 
            </summary>
            <remarks>
            Y = A + (B * alpha)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="dfAlpha">Specifies a scalar int type <code>double</code></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add(System.Int32,System.Int64,System.Int64,System.Int64,System.Single)">
            <summary>
            Adds A to (B times scalar) and places the result in Y.
            </summary>
            <remarks>
            Y = A + (B * alpha)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="fAlpha">Specifies a scalar int type <code>float</code></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.add(System.Int32,System.Int64,System.Int64,System.Int64,System.Double,System.Double,System.Int32,System.Int32,System.Int32)">
            <summary>
            Adds A to (B times scalar) and places the result in Y.
            </summary>
            <remarks>
            Y = A + (B * alpha)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="dfAlphaA">Specifies a scalar int type 'T' applied to A.</param>
            <param name="dfAlphaB">Specifies a scalar int type 'T' applied to B.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOff">Optionally, specifies an offset (in items, not bytes) into the memory of B.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sub(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Subtracts B from A and places the result in Y.
            </summary>
            <remarks>
            Y = A - B
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOff">Optionally, specifies an offset (in items, not bytes) into the memory of B.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
            <param name="nB">Optionally, specifies a number of 'B' items to subtract (default = 0 which causes ALL items in B to be subtracted).
            When 'nB' > 0, it must be a factor of 'n' and causes that number of B items to be subtracted as a block from A.
            </param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.mul(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            Multiplies each element of A with each element of B and places the result in Y.
            </summary>
            <remarks>
            Y = A * B (element by element)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOff">Optionally, specifies an offset (in items, not bytes) into the memory of B.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sub_and_dot(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            Subtracts every <i>nInnterNum</i> element of B from A and performs a dot product on the result.
            </summary>
            <remarks>
            Y[i] = (A[i] - B[i%nInnerNum]) * (A[i] - B[i%nInnerNum])
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="nN">Specifies the inner count.</param>
            <param name="nInnerNum">Specifies the dimension.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOff">Optionally, specifies an offset (in items, not bytes) into the memory of B.</param>
            <param name="nYOff">Optionally, specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.mul_scalar(System.Int32,System.Double,System.Int64)">
            <summary>
            Mutlipy each element of Y by a scalar.
            </summary>
            <remarks>
            Y = Y * alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors Y.</param>
            <param name="fAlpha">Specifies the scalar in type <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.mul_scalar(System.Int32,System.Single,System.Int64)">
            <summary>
            Mutlipy each element of Y by a scalar.
            </summary>
            <remarks>
            Y = Y * alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors Y.</param>
            <param name="fAlpha">Specifies the scalar in type <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.mul_scalar(System.Int32,`0,System.Int64)">
            <summary>
            Mutlipy each element of Y by a scalar.
            </summary>
            <remarks>
            Y = Y * alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors Y.</param>
            <param name="fAlpha">Specifies the scalar in type 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.div(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Divides each element of A by each element of B and places the result in Y.
            </summary>
            <remarks>
            Y = A / B (element by element)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.abs(System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the absolute value of A and places the result in Y.
            </summary>
            <remarks>
            Y = abs(X)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.exp(System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the exponent value of A and places the result in Y.
            </summary>
            <remarks>
            Y = exp(A)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.exp(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Double)">
            <summary>
            Calculates the exponent value of A * beta and places the result in Y.
            </summary>
            <remarks>
            Y = exp(A * beta)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nYOff">Specifies an offset (in items, not bytes) into the memory of Y.</param>
            <param name="dfBeta">Specifies the scalar as type <code>double</code></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.log(System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the log value of A and places the result in Y.
            </summary>
            <remarks>
            Y = log(A)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.log(System.Int32,System.Int64,System.Int64,System.Double,System.Double)">
            <summary>
            Calculates the log value of (A * beta) + alpha, and places the result in Y.
            </summary>
            <remarks>
            Y = log((A * beta) + alpha)
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="dfBeta">Specifies the scalar as type <code>double</code> that is multiplied with the log.</param>
            <param name="dfAlpha">Optionally, specifies a scalar added to the value before taking the log.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.powx(System.Int32,System.Int64,System.Double,System.Int64,System.Int32,System.Int32)">
            <summary>
            Calculates the A raised to the power alpha and places the result in Y.
            </summary>
            <remarks>
            Y = A^alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="fAlpha">Specifies the scalar in type <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies the offset for hA memory (default = 0).</param>
            <param name="nYOff">Optionally, specifies the offset for hY memory (default = 0).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.powx(System.Int32,System.Int64,System.Single,System.Int64,System.Int32,System.Int32)">
            <summary>
            Calculates the A raised to the power alpha and places the result in Y.
            </summary>
            <remarks>
            Y = A^alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="fAlpha">Specifies the scalar in type <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies the offset for hA memory (default = 0).</param>
            <param name="nYOff">Optionally, specifies the offset for hY memory (default = 0).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.powx(System.Int32,System.Int64,`0,System.Int64,System.Int32,System.Int32)">
            <summary>
            Calculates the A raised to the power alpha and places the result in Y.
            </summary>
            <remarks>
            Y = A^alpha
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="fAlpha">Specifies the scalar in type 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nAOff">Optionally, specifies the offset for hA memory (default = 0).</param>
            <param name="nYOff">Optionally, specifies the offset for hY memory (default = 0).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sign(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32)">
            <summary>
            Computes the sign of each element of X and places the result in Y.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nXOff">Specifies an offset (in items, not bytes) into the memory of X.</param>
            <param name="nYOff">Specifies an offset (in items, not bytes) into the memory of Y.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sqrt(System.Int32,System.Int64,System.Int64)">
            <summary>
            Computes the square root of each element of X and places the result in Y.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and Y.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sqrt_scale(System.Int32,System.Int64,System.Int64)">
            <summary>
            Scale the data by the sqrt of the data.  y = sqrt(abs(x)) * sign(x)
            </summary>
            <param name="nCount">Specifies the number of elements.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.compare_signs(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Compares the signs of each value in A and B and places the result in Y.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and Y.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.max(System.Int32,System.Int64,System.Int64@,System.Int32)">
            <summary>
            Finds the maximum value of A.
            </summary>
            <remarks>
            This function uses [NVIDIA's Thrust](https://developer.nvidia.com/thrust).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="lPos">Returns the position of the maximum value.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A (default = 0).</param>
            <returns>The maximum value is returned as type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.min(System.Int32,System.Int64,System.Int64@,System.Int32)">
            <summary>
            Finds the minimum value of A.
            </summary>
            <remarks>
            This function uses [NVIDIA's Thrust](https://developer.nvidia.com/thrust).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vectors A.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="lPos">Returns the position of the minimum value.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A (default = 0).</param>
            <returns>The minimum value is returned as type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.minmax(System.Int32,System.Int64,System.Int64,System.Int64,System.Boolean,System.Int32)">
            <summary>
            Finds the minimum and maximum values within A.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vector A.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hWork1">Specifies a handle to workspace data in GPU memory.  To get the size of the workspace memory, call this function with hA = 0.</param>
            <param name="hWork2">Specifies a handle to workspace data in GPU memory.  To get the size of the workspace memory, call this function with hA = 0.</param>
            <param name="bDetectNans">Optionally, specifies whether or not to detect Nans.</param>
            <param name="nAOff">Optionally, specifies an offset (in items, not bytes) into the memory of A.</param>
            <returns>A four element tuple is returned where the first item contains the minimum, the second item contains the maximum, the third contains the number
            of NaN values and the fourth contains the number of Infinity values.  
            When calling this function with <code>hA = 0</code> the function instead returns the required size of <i>hWork1</i>, <i>hWork2</i>, 0, 0 (in items, not bytes).</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.minmax(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64,System.Boolean)">
            <summary>
            Finds up to 'nK' minimum and maximum values within A.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vector A.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hWork1">Specifies a handle to workspace data in GPU memory.  To get the size of the workspace memory, call this function with hA = 0.</param>
            <param name="hWork2">Specifies a handle to workspace data in GPU memory.  To get the size of the workspace memory, call this function with hA = 0.</param>
            <param name="nK">Specifies the number of min and max values to find.</param>
            <param name="hMin">Specifies a handle to host memory allocated with AllocHostBuffer in the length 'nK' where the min values are placed.</param>
            <param name="hMax">Specifies a handle to host memory allocated with AllocHostBuffer in the length 'nK' where the min values are placed.</param>
            <param name="bNonZeroOnly">Specifies whether or not to exclude zero from the min and max calculations.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sumsq(System.Int32,System.Int64,System.Int64,System.Int32)">
            <summary>
            Calculates the sum of squares of A.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vectors A and W.</param>
            <param name="hW">Specifies a handle to workspace data in GPU memory.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="nAOff">Specifies an offset (in items, not bytes) into the memory of A.</param>
            <returns>The sum of squares of A is returned as type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sumsqdiff(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32)">
            <summary>
            Calculates the sum of squares of differences between A and B
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vectors A, B and W.</param>
            <param name="hW">Specifies a handle to workspace data in GPU memory.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hB">Specifies a handle to the vector B in GPU memory.</param>
            <param name="nAOff">Specifies an offset (in items, not bytes) into the memory of A.</param>
            <param name="nBOff">Specifies an offset (in items, not bytes) into the memory of B.</param>
            <returns>The sum of squared differences between A and B are returned as type <code>double</code></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.width(System.Int32,System.Int64,System.Int64,System.Int64,System.Double,System.Int64)">
            <summary>
            Calculates the width values.
            </summary>
            <param name="n">Specifies the number of items.</param>
            <param name="hMean">Specifies a handle to the mean values in GPU memory.</param>
            <param name="hMin">Specifies a handle to the min values in GPU memory.</param>
            <param name="hMax">Specifies a handle to the max values in GPU memory.</param>
            <param name="dfAlpha">Specifies the alpha value.</param>
            <param name="hWidth">Specifies the GPU memory where the width values are placed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.contains_point(System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Returns true if the point is contained within the bounds.
            </summary>
            <param name="n">Specifies the number of items.</param>
            <param name="hMean">Specifies a handle to the mean values in GPU memory.</param>
            <param name="hWidth">Specifies a handle to the width values in GPU memory.</param>
            <param name="hX">Specifies a handle to the X values in GPU memory.</param>
            <param name="hWork">Specifies a handle to the work data in GPU memory.</param>
            <param name="nXOff">Optionally, specifies an offset into the X vector (default = 0).</param>
            <returns>If the X values are within the bounds, <i>true</i> is returned, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.denan(System.Int32,System.Int64,System.Double)">
            <summary>
            Replaces all NAN values witin X with a replacement value.
            </summary>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="dfReplacement">Specifies the replacement value.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.im2col(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int32)">
            <summary>
            Rearranges image blocks into columns.
            </summary>
            <param name="hDataIm">Specifies a handle to the image block in GPU memory.</param>
            <param name="nDataImOffset">Specifies an offset into the image block memory.</param>
            <param name="nChannels">Specifies the number of channels in the image.</param>
            <param name="nHeight">Specifies the height of the image.</param>
            <param name="nWidth">Specifies the width of the image.</param>
            <param name="nKernelH">Specifies the kernel height.</param>
            <param name="nKernelW">Specifies the kernel width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nDilationH">Specifies the dilation along the height.</param>
            <param name="nDilationW">Specifies the dilation along the width.</param>
            <param name="hDataCol">Specifies a handle to the column data in GPU memory.</param>
            <param name="nDataColOffset">Specifies an offset into the column memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.im2col_nd(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Rearranges image blocks into columns.
            </summary>
            <param name="hDataIm">Specifies a handle to the image block in GPU memory.</param>
            <param name="nDataImOffset">Specifies an offset into the image block memory.</param>
            <param name="nNumSpatialAxes">Specifies the number of spatial axes.</param>
            <param name="nImCount">Specifies the number of kernels.</param>
            <param name="nChannelAxis">Specifies the axis containing the channel.</param>
            <param name="hImShape">Specifies a handle to the image shape data in GPU memory.</param>
            <param name="hColShape">Specifies a handle to the column shape data in GPU memory.</param>
            <param name="hKernelShape">Specifies a handle to the kernel shape data in GPU memory.</param>
            <param name="hPad">Specifies a handle to the pad data in GPU memory.</param>
            <param name="hStride">Specifies a handle to the stride data in GPU memory.</param>
            <param name="hDilation">Specifies a handle to the dilation data in GPU memory.</param>
            <param name="hDataCol">Specifies a handle to the column data in GPU memory.</param>
            <param name="nDataColOffset">Specifies an offset into the column memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.col2im(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int32)">
            <summary>
            Rearranges the columns into image blocks.
            </summary>
            <param name="hDataCol">Specifies a handle to the column data in GPU memory.</param>
            <param name="nDataColOffset">Specifies an offset into the column memory.</param>
            <param name="nChannels">Specifies the number of channels in the image.</param>
            <param name="nHeight">Specifies the height of the image.</param>
            <param name="nWidth">Specifies the width of the image.</param>
            <param name="nKernelH">Specifies the kernel height.</param>
            <param name="nKernelW">Specifies the kernel width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nDilationH">Specifies the dilation along the height.</param>
            <param name="nDilationW">Specifies the dilation along the width.</param>
            <param name="hDataIm">Specifies a handle to the image block in GPU memory.</param>
            <param name="nDataImOffset">Specifies an offset into the image block memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.col2im_nd(System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Rearranges the columns into image blocks.
            </summary>
            <param name="hDataCol">Specifies a handle to the column data in GPU memory.</param>
            <param name="nDataColOffset">Specifies an offset into the column memory.</param>
            <param name="nNumSpatialAxes">Specifies the number of spatial axes.</param>
            <param name="nColCount">Specifies the number of kernels.</param>
            <param name="nChannelAxis">Specifies the axis containing the channel.</param>
            <param name="hImShape">Specifies a handle to the image shape data in GPU memory.</param>
            <param name="hColShape">Specifies a handle to the column shape data in GPU memory.</param>
            <param name="hKernelShape">Specifies a handle to the kernel shape data in GPU memory.</param>
            <param name="hPad">Specifies a handle to the pad data in GPU memory.</param>
            <param name="hStride">Specifies a handle to the stride data in GPU memory.</param>
            <param name="hDilation">Specifies a handle to the dilation data in GPU memory.</param>
            <param name="hDataIm">Specifies a handle to the image block in GPU memory.</param>
            <param name="nDataImOffset">Specifies an offset into the image block memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_min(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the minimum value within each channel of X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_max(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the maximum value within each channel of X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_compare(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Compares the values of the channels from X and places the result in Y where 1 is set if the values are equal otherwise 0 is set.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory of length nOuterNum.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_fill(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int32,System.Int64,System.Int64)">
            <summary>
            Fills each channel with the channel item of Y with the data of X matching the label index specified by hLabels.
            </summary>
            <param name="nCount">Specifies the number of items in Y.</param>
            <param name="nOuterNum">Specifies the num of Y and Labels.</param>
            <param name="nChannels">Specifies the channel size of Y and X.</param>
            <param name="nInnerNum">Specifies the spatial dimension of X and Y, but is normally 1.</param>
            <param name="hX">Specifies the GPU memory containing the encodings (usually centroids) of each label 0, ... max label.</param>
            <param name="nLabelDim">Specifies the dimension of the label channels.  A value > 1 indicates that more than one label are stored per channel in which case only the first label is used.</param>
            <param name="hLabels">Specifies the label ordering that determines how Y is filled using data from X.</param>
            <param name="hY">Specifies the GPU memory of the output data.</param>
            <remarks>
            This function is used to fill a blob with data matching a set of labels.  For example in a 3 item encoding based system with
            4 labels:
            X = 4 channels of 3 items each (e.g. an encoding for each label).
            The values of hLabels show the ordering for which to fill hY with the labeled encodings.  So if hLabels = 0, 2, 1, 3, 1, then
            Y = size { 5, 3, 1, 1 }, 5 items each with encoding sizes of 3 items which are then filled with the encoding at position 0,
            (for label 0), followed by the encoding for label 2, then 1, 3 and ending with the encoding for 1 as specified by the labels.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_sub(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Subtracts the values of the channels from X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_sum(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the sum the the values within each channel of X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_div(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int32)">
            <summary>
            Divides the values of the channels from X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nMethod">Specifies the method of traversing the channel, <i>nMethod</i> = 1 (the default) is used by the SoftmaxLayer and <i>nMethod</i> = 2 is used by the GRNLayer.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_mul(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int32)">
            <summary>
            Multiplies the values of the channels from X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of images within X.</param>
            <param name="nChannels">Specifies the number of channels per image of X.</param>
            <param name="nInnerNum">Specifies the dimension of each image in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
            <param name="nMethod">Specifies the method of traversing the channel, <i>nMethod</i> = 1 (the default) is used by the SoftmaxLayer and <i>nMethod</i> = 2 is used by the GRNLayer.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.channel_dot(System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Calculates the dot product the the values within each channel of X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements.</param>
            <param name="nOuterNum">Specifies the number of images.</param>
            <param name="nChannels">Specifies the number of channels per image.</param>
            <param name="nInnerNum">Specifies the dimension of each image.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hA">Specifies a handle to the vector A in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sum(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Calculates the sum of inner values of X and places the result in Y.
            </summary>
            <param name="nCount">Specifies the number of elements in X.</param>
            <param name="nOuterNum">Specifies the number of outer items within X.</param>
            <param name="nInnerNum">Specifies the dimension of items to sum in X.</param>
            <param name="hX">Specifies a handle to the vector X in GPU memory.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_setseed(System.Int64)">
            <summary>
            Sets the random number generator seed used by random number operations.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand)
            </remarks>
            <param name="lSeed">Specifies the random number generator seed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_uniform(System.Int32,System.Double,System.Double,System.Int64)">
            <summary>
            Fill Y with random numbers using a uniform random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMin">Specifies the minimum value of the distribution with a type of <code>double</code></param>
            <param name="fMax">Specifies the maximum value of the distribution with a type of <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_uniform(System.Int32,System.Single,System.Single,System.Int64)">
            <summary>
            Fill Y with random numbers using a uniform random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMin">Specifies the minimum value of the distribution with a type of <code>float</code></param>
            <param name="fMax">Specifies the maximum value of the distribution with a type of <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_uniform(System.Int32,`0,`0,System.Int64)">
            <summary>
            Fill Y with random numbers using a uniform random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMin">Specifies the minimum value of the distribution with a type of 'T'.</param>
            <param name="fMax">Specifies the maximum value of the distribution with a type of 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_gaussian(System.Int32,System.Double,System.Double,System.Int64)">
            <summary>
            Fill Y with random numbers using a gaussian random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Guassian Distribution](https://en.wikipedia.org/wiki/Normal_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMu">Specifies the mean of the distribution with a type of <code>double</code></param>
            <param name="fSigma">Specifies the standard deviation of the distribution with a type of <code>double</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_gaussian(System.Int32,System.Single,System.Single,System.Int64)">
            <summary>
            Fill Y with random numbers using a gaussian random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Guassian Distribution](https://en.wikipedia.org/wiki/Normal_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMu">Specifies the mean of the distribution with a type of <code>float</code></param>
            <param name="fSigma">Specifies the standard deviation of the distribution with a type of <code>float</code></param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_gaussian(System.Int32,`0,`0,System.Int64)">
            <summary>
            Fill Y with random numbers using a gaussian random distribution.
            </summary>
            <remarks>
            This function uses [NVIDIA's cuRand](https://developer.nvidia.com/curand).  See also [Guassian Distribution](https://en.wikipedia.org/wiki/Normal_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fMu">Specifies the mean of the distribution with a type of 'T'.</param>
            <param name="fSigma">Specifies the standard deviation of the distribution with a type of 'T'.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_bernoulli(System.Int32,System.Double,System.Int64)">
            <summary>
            Fill Y with random numbers using a bernoulli random distribution.
            </summary>
            <remarks>
            See [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fNonZeroProb">Specifies the probability that a given value is set to non zero.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_bernoulli(System.Int32,System.Single,System.Int64)">
            <summary>
            Fill Y with random numbers using a bernoulli random distribution.
            </summary>
            <remarks>
            See [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fNonZeroProb">Specifies the probability that a given value is set to non zero.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rng_bernoulli(System.Int32,`0,System.Int64)">
            <summary>
            Fill Y with random numbers using a bernoulli random distribution.
            </summary>
            <remarks>
            See [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
            </remarks>
            <param name="n">Specifies the number of items (not bytes) in the vector X.</param>
            <param name="fNonZeroProb">Specifies the probability that a given value is set to non zero.</param>
            <param name="hY">Specifies a handle to the vector Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.accuracy_fwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Boolean,System.Nullable{System.Int32})">
            <summary>
            Performs the forward pass for the accuracy layer
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBottomLabel">Specifies a handle to the bottom labels in GPU memory.</param>
            <param name="hAccData">Specifies a handle to temporary accuracy data in GPU memory.</param>
            <param name="nOuterNum">Specifies the outer count.</param>
            <param name="nDim">Specifies the dimension.</param>
            <param name="nInnerNum">Specifies the inner count.</param>
            <param name="nNumLabels">Specifies the number of labels.</param>
            <param name="nTopK">Specifies the top items to include in the accuracy.</param>
            <param name="hCounts">Specifies a handle to the counts data in GPU memory.</param>
            <param name="bPerClass">Specifies whether (true) to caculate the accuracy for each class, or (false) globally.</param>
            <param name="nIgnoreLabel">Optionally, specifies a label to ignore, or <i>null</i> to ignore.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.batchreidx_fwd(System.Int32,System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the forward pass for batch re-index
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nInnerDim">Specifies the inner dimension.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hPermutData">Specifies a handle to the permuation data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.batchreidx_bwd(System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the backward pass for batch re-index
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nInnerDim">Specifies the inner dimension.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopIdx">Specifies a handle to the top indexes in GPU memory.</param>
            <param name="hBegins">Specifies a handle to the begin data in GPU memory.</param>
            <param name="hCounts">Specifies a handle to the counts in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.embed_fwd(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs the forward pass for embed 
            </summary>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hWeight">Specifies a handle to the weight data in GPU memory.</param>
            <param name="nM"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nN"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nK"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.embed_bwd(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs the backward pass for embed
            </summary>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nM"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nN"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nK"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hWeightDiff">Specifies a handle to the weight diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.pooling_fwd(MyCaffe.common.POOLING_METHOD,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the forward pass for pooling using Cuda
            </summary>
            <param name="method">Specifies the pooling method.</param>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="num">Specifies the number of inputs.</param>
            <param name="nChannels">Specifies the number of channels per input.</param>
            <param name="nHeight">Specifies the height of each input.</param>
            <param name="nWidth">Specifies the width of each input.</param>
            <param name="nPooledHeight">Specifies the height of the pooled data.</param>
            <param name="nPooledWidth">Specifies the width of the pooled data.</param>
            <param name="nKernelH">Specifies the height of the pooling kernel.</param>
            <param name="nKernelW">Specifies the width of the pooling kernel.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
            <param name="hTopMask">Specifies a handle to the top mask data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.pooling_bwd(MyCaffe.common.POOLING_METHOD,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the backward pass for pooling using Cuda
            </summary>
            <param name="method">Specifies the pooling method.</param>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="num">Specifies the number of inputs.</param>
            <param name="nChannels">Specifies the number of channels per input.</param>
            <param name="nHeight">Specifies the height of each input.</param>
            <param name="nWidth">Specifies the width of each input.</param>
            <param name="nPooledHeight">Specifies the height of the pooled data.</param>
            <param name="nPooledWidth">Specifies the width of the pooled data.</param>
            <param name="nKernelH">Specifies the height of the pooling kernel.</param>
            <param name="nKernelW">Specifies the width of the pooling kernel.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
            <param name="hTopMask">Specifies a handle to the top mask data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.unpooling_fwd(MyCaffe.common.POOLING_METHOD,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs the forward pass for unpooling using Cuda
            </summary>
            <param name="method">Specifies the pooling method.</param>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="num">Specifies the number of inputs.</param>
            <param name="nChannels">Specifies the number of channels per input.</param>
            <param name="nHeight">Specifies the height of each input.</param>
            <param name="nWidth">Specifies the width of each input.</param>
            <param name="nPooledHeight">Specifies the height of the pooled data.</param>
            <param name="nPooledWidth">Specifies the width of the pooled data.</param>
            <param name="nKernelH">Specifies the height of the pooling kernel.</param>
            <param name="nKernelW">Specifies the width of the pooling kernel.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.unpooling_bwd(MyCaffe.common.POOLING_METHOD,System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs the backward pass for unpooling using Cuda
            </summary>
            <param name="method">Specifies the pooling method.</param>
            <param name="nCount">Specifies the number of items in the bottom data.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="num">Specifies the number of inputs.</param>
            <param name="nChannels">Specifies the number of channels per input.</param>
            <param name="nHeight">Specifies the height of each input.</param>
            <param name="nWidth">Specifies the width of each input.</param>
            <param name="nPooledHeight">Specifies the height of the pooled data.</param>
            <param name="nPooledWidth">Specifies the width of the pooled data.</param>
            <param name="nKernelH">Specifies the height of the pooling kernel.</param>
            <param name="nKernelW">Specifies the width of the pooling kernel.</param>
            <param name="nStrideH">Specifies the stride along the height.</param>
            <param name="nStrideW">Specifies the stride along the width.</param>
            <param name="nPadH">Specifies the pad applied to the height.</param>
            <param name="nPadW">Specifies the pad applied to the width.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.clip_fwd(System.Int32,System.Int64,System.Int64,`0,`0)">
            <summary>
            Performs a Clip forward pass in Cuda.
            </summary>
            <remarks>
            Calculation @f$ Y[i] = \max(min, \min(max,X[i])) @f$
            </remarks>
            <param name="nCount">Specifies the number of items in the bottom and top data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="fMin">Specifies the bottom value to clip to.</param>
            <param name="fMax">Specifies the top value to clip to.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.clip_bwd(System.Int32,System.Int64,System.Int64,System.Int64,`0,`0)">
            <summary>
            Performs a Clip backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="fMin">Specifies the bottom value to clip to.</param>
            <param name="fMax">Specifies the top value to clip to.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.tanh_fwd(System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a TanH forward pass in Cuda.
            </summary>
            <remarks>
            Calculation @f$ Y[i] = tanh(X[i]) @f$
            
            @see [Hyperbolic Function](https://en.wikipedia.org/wiki/Hyperbolic_function).
            </remarks>
            <param name="nCount">Specifies the number of items in the bottom and top data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.tanh_bwd(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a TanH backward pass in Cuda.
            </summary>
            <remarks>
            @see [Hyperbolic Function](https://en.wikipedia.org/wiki/Hyperbolic_function).
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sigmoid_fwd(System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a Sigmoid forward pass in Cuda.
            </summary>
            <remarks>
            Calcuation @f$ Y[i] = 1.0 / (1.0 + exp(-X[i])) @f$
            
            @see [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function).
            </remarks>
            <param name="nCount">Specifies the number of items in the bottom and top data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sigmoid_bwd(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a Sigmoid backward pass in Cuda.
            </summary>
            <remarks>
            @see [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function).
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.swish_bwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Double)">
            <summary>
            Performs a Swish backward pass in Cuda.
            </summary>
            <remarks>
            @see [Activation Functions](https://arxiv.org/abs/1710.05941v2) by Prajit Ramachandran, Barret Zoph, Quoc V. Le., 2017.
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hSigmoidOutputData">Specifies a handle to the sigmoid output data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="dfBeta">Specifies the 'beta' value applied to the output.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.relu_fwd(System.Int32,System.Int64,System.Int64,`0)">
            <summary>
            Performs a Rectifier Linear Unit (ReLU) forward pass in Cuda.
            </summary>
            <remarks>
            Calculation @f$ Y[i] = (X[i] > 0) ? X[i] : X[i] * negativeSlope @f$
            
            @see [Rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), and
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Arora, et al., 2016,
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by He, et al., 2015
            </remarks>
            <param name="nCount">Specifies the number of items in the bottom and top data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="fNegativeSlope">Specifies the negative slope.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.relu_bwd(System.Int32,System.Int64,System.Int64,System.Int64,`0)">
            <summary>
            Performs a Rectifier Linear Unit (ReLU) backward pass in Cuda.
            </summary>
            <remarks>
            @see [Rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), and
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Arora, et al., 2016,
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by He, et al., 2015
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="fNegativeSlope">Specifies the negative slope.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.elu_fwd(System.Int32,System.Int64,System.Int64,System.Double)">
            <summary>
            Performs a Exponential Linear Unit (ELU) forward pass in Cuda.
            </summary>
            <remarks>
            Calculates @f$ Y[i] = (X[i] > 0) ? X[i] : alpha * (exp(X[i]) - 1) @f$
            
            @see [Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/abs/1604.04112) by Shah, et al., 2016
            </remarks>
            <param name="nCount">Specifies the number of items in the bottom and top data.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="dfAlpha">Specifies the alpha value.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.elu_bwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Double)">
            <summary>
            Performs a Exponential Linear Unit (ELU) backward pass in Cuda.
            </summary>
            <remarks>
            @see [Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/abs/1604.04112) by Shah, et al., 2016
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="dfAlpha">Specifies the alpha value.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.dropout_fwd(System.Int32,System.Int64,System.Int64,System.UInt32,`0,System.Int64)">
            <summary>
            Performs a dropout forward pass in Cuda.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Hinton, et al., 2012
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
            <param name="uiThreshold">Specifies the threshold value: when mask value are less than the threshold, the data item is 'dropped out' by setting the data item to zero.</param>
            <param name="fScale">Specifies a scale value applied to each item that is not dropped out.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.dropout_bwd(System.Int32,System.Int64,System.Int64,System.UInt32,`0,System.Int64)">
            <summary>
            Performs a dropout backward pass in Cuda.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Hinton, et al., 2012
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU memory.</param>
            <param name="uiThreshold">Specifies the threshold value: when mask value are less than the threshold, the data item is 'dropped out' by setting the data item to zero.</param>
            <param name="fScale">Specifies a scale value applied to each item that is not dropped out.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.bnll_fwd(System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a binomial normal log liklihod (BNLL) forward pass in Cuda.
            </summary>
            <remarks>
            Computes @f$ Y[i] = log(1 + exp(X[i])) @f$
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.bnll_bwd(System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a binomial normal log liklihod (BNLL) backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.prelu_fwd(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Performs Parameterized Rectifier Linear Unit (ReLU) forward pass in Cuda.
            </summary>
            <remarks>
            Calculation @f$ Y[i] = (X[i] > 0) ? X[i] : X[i] * slopeData @f$
            
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Arora, et al., 2016,
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by He, et al., 2015
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nChannels">Specifies the channels per input.</param>
            <param name="nDim">Specifies the dimension of each input.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hSlopeData">Specifies a handle to the slope data in GPU memory.</param>
            <param name="nDivFactor">Specifies the div factor applied to the channels.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.prelu_bwd_param(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs Parameterized Rectifier Linear Unit (ReLU) backward param pass in Cuda.
            </summary>
            <remarks>
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Arora, et al., 2016,
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by He, et al., 2015
            </remarks>
            <param name="nCDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nNum"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nTopOffset"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBackBuffDiff">Specifies a handle to the back buffer diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.prelu_bwd(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Performs Parameterized Rectifier Linear Unit (ReLU) backward pass in Cuda.
            </summary>
            <remarks>
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Arora, et al., 2016,
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by He, et al., 2015
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nChannels">Specifies the channels per input.</param>
            <param name="nDim">Specifies the dimension of each input.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="hSlopeData">Specifies a handle to the slope data in GPU memory.</param>
            <param name="nDivFactor">Specifies the div factor applied to the channels.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.softmaxloss_fwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64,System.Nullable{System.Int32})">
            <summary>
            Performs Softmax Loss forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hProbData">Specifies a handle to the probability data in GPU memory.</param>
            <param name="hLabel">Specifies a handle to the label data in GPU memory.</param>
            <param name="hLossData">Specifies a handle to the loss data in GPU memory.</param>
            <param name="nOuterNum"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nInnerNum"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hCounts">Specifies a handle to the counts in GPU memory.</param>
            <param name="nIgnoreLabel">Optionally, specifies a label to ignore.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.softmaxloss_bwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64,System.Nullable{System.Int32})">
            <summary>
            Performs Softmax Loss backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hLabel">Specifies a handle to the label data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
            <param name="nOuterNum"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nInnerNum"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hCounts">Specifies a handle to the counts in GPU memory.</param>
            <param name="nIgnoreLabel">Optionally, specifies a label to ignore.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.max_fwd(System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a max forward pass in Cuda.
            </summary>
            <remarks>
            Calculation: @f$ Y[i] = max(A[i], B[i]) @f$
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomDataA">Specifies a handle to the Bottom A data in GPU memory.</param>
            <param name="hBottomDataB">Specifies a handle to the Bottom B data in GPU memory.</param>
            <param name="nIdx">Specifies the blob index used to set the mask.</param>
            <param name="hTopData">Specifies a handle to the Top data in GPU memory.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.max_bwd(System.Int32,System.Int64,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a max backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nIdx">Specifies the blob index used to test the mask.</param>
            <param name="hMask">Specifies a handle to the mask data in GPU.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.crop_fwd(System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the crop forward operation.
            </summary>
            <param name="nCount">Specifies the count.</param>
            <param name="nNumAxes">Specifies the number of axes in the bottom.</param>
            <param name="hSrcStrides">Specifies a handle to the GPU memory containing the source strides.</param>
            <param name="hDstStrides">Specifies a handle to the GPU memory containing the destination strides.</param>
            <param name="hOffsets">Specifies a handle to the GPU memory containing the offsets.</param>
            <param name="hBottomData">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.crop_bwd(System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs the crop backward operation.
            </summary>
            <param name="nCount">Specifies the count.</param>
            <param name="nNumAxes">Specifies the number of axes in the bottom.</param>
            <param name="hSrcStrides">Specifies a handle to the GPU memory containing the source strides.</param>
            <param name="hDstStrides">Specifies a handle to the GPU memory containing the destination strides.</param>
            <param name="hOffsets">Specifies a handle to the GPU memory containing the offsets.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTopDiff">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.concat_fwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a concat forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="nNumConcats">Specifies the number of concatenations.</param>
            <param name="nConcatInputSize">Specifies the concatenation input size.</param>
            <param name="nTopConcatAxis">Specifies the top axis to concatenate.</param>
            <param name="nBottomConcatAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nOffsetConcatAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.concat_bwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a concat backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nNumConcats">Specifies the number of concatenations.</param>
            <param name="nConcatInputSize">Specifies the concatenation input size.</param>
            <param name="nTopConcatAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nBottomConcatAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nOffsetConcatAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hBottomDiff">Specifies a handle to the Bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.slice_fwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a slice forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="nNumSlices">Specifies the number of slices.</param>
            <param name="nSliceSize">Specifies the slice size.</param>
            <param name="nBottomSliceAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nTopSliceAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nOffsetSliceAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.slice_bwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a slice backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nNumSlices">Specifies the number of slices.</param>
            <param name="nSliceSize">Specifies the slice size.</param>
            <param name="nBottomSliceAxis">Specifies the bottom axis to concatenate.</param>
            <param name="nTopSliceAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nOffsetSliceAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hBottomDiff">Specifies a handle to the Bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.tile_fwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a tile forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="nInnerDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nTiles">Specifies the number of tiles.</param>
            <param name="nBottomTileAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.tile_bwd(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a tile backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTileSize">Specifies the size of each tile.</param>
            <param name="nTiles">Specifies the number of tiles.</param>
            <param name="nBottomTileAxis"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hBottomDiff">Specifies a handle to the Bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.bias_fwd(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int64)">
            <summary>
            Performs a bias forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="hBiasData">Specifies a handle to the bias data in GPU memory.</param>
            <param name="nBiasDim">Specifies the bias dimension.</param>
            <param name="nInnerDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.scale_fwd(System.Int32,System.Int64,System.Int64,System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a scale forward pass in Cuda.
            </summary>
            <remarks>
            Calculation: if hBias = 0:  @f$ Y[i] = X[i] * scaleData[(i / nInnerDim) % nScaleDim] @f$
                            othrerwise: @f$ Y[i] = X[i] * scaleData[(i / nInnerDim) % nScaleDim] + biasData[(i / nInnerDim) % nScaleDim] @f$
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hX">Specifies the input data X in GPU memory.</param>
            <param name="hScaleData"></param>
            <param name="nScaleDim"></param>
            <param name="nInnerDim"></param>
            <param name="hY">Specifies the output data Y in GPU memory.</param>
            <param name="hBiasData">Optionally, specifies the bias data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.threshold_fwd(System.Int32,System.Double,System.Int64,System.Int64)">
            <summary>
            Performs a threshold pass in Cuda.
            </summary>
            <remarks>
            Calculation: @f$ Y[i] = (X[i] > threshold) ? 1 : 0 @f$
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="dfThreshold">Specifies the threshold value.</param>
            <param name="hX">Specifies the input data X in GPU memory.</param>
            <param name="hY">Specifies the output data Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.cll_bwd(System.Int32,System.Int32,System.Double,System.Boolean,System.Double,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a contrastive loss layer backward pass in Cuda.
            </summary>
            <remarks>
            See [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) by Hadsel, et al., 2006
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nChannels">Specifies the number of channels.</param>
            <param name="dfMargin"><b><i>Specifies the margin to use.  The default is 1.0.</i></b></param>
            <param name="bLegacyVersion">When <code>false</code> the calculation proposed by Hadsell, et al., 2006 is used where @f$ (margin - d)^2 @f$, 
            otherwise the legacy version is used where @f$ (margin - d^2) @f$.  The default is <code>false</code></param>
            <param name="dfAlpha"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hY">Specifies the Y data in GPU memory used to determine similar pairs.</param>
            <param name="hDiff">Specifies the diff in GPU memory.</param>
            <param name="hDistSq">Specifies the distance squared data in GPU memory.</param>
            <param name="hBottomDiff">Specifies the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.smoothl1_fwd(System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs the forward operation for the SmoothL1 loss.
            </summary>
            <remarks>
            Calculation: 
                f(x) = 0.5 * x^2, if |x| lt 1
                     = |x| - 0.5, otherwise
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hX">Specifies the input data X in GPU memory.</param>
            <param name="hY">Specifies the output data Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.smoothl1_bwd(System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs the backward operation for the SmoothL1 loss.
            </summary>
            <remarks>
            Calculation: 
                f'(x) = x, if |x| lt 1
                      = sign(x), otherwise
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hX">Specifies the input data X in GPU memory.</param>
            <param name="hY">Specifies the output data Y in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.permute(System.Int32,System.Int64,System.Boolean,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64)">
            <summary>
            Performs data permutation on the input and reorders the data which is placed in the output.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottom">Specifies the input data.</param>
            <param name="bFwd">Specifies whether or not this is a forward (<i>true</i>) or backwards (<i>true</i>) operation.</param>
            <param name="hPermuteOrder">Specifies the permuation order values in GPU memory.</param>
            <param name="hOldSteps">Specifies the old step values in GPU memory.</param>
            <param name="hNewSteps">Specifies the new step values in GPU memory.</param>
            <param name="nNumAxes">Specifies the number of axes.</param>
            <param name="hTop">Specifies the output data.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lrn_fillscale(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,`0,`0,System.Int64)">
            <summary>
            Performs the fill scale operation used to calculate the LRN cross channel forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="nNum">Specifies the number of input items.</param>
            <param name="nChannels">Specifies the number of channels per input item.</param>
            <param name="nHeight">Specifies the height of each input item.</param>
            <param name="nWidth">Specifies the width of each input item.</param>
            <param name="nSize"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fAlphaOverSize">Specifies the alpha value over the size.</param>
            <param name="fK">Specifies the k value.</param>
            <param name="hScaleData">Specifies a handle to the scale data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lrn_computeoutput(System.Int32,System.Int64,System.Int64,`0,System.Int64)">
            <summary>
            Computes the output used to calculate the LRN cross channel forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="hScaleData">Specifies a handle to the scale data in GPU memory.</param>
            <param name="fNegativeBeta">Specifies the negative beta value.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lrn_computediff(System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,`0,`0,System.Int64)">
            <summary>
            Computes the diff used to calculate the LRN cross channel backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hBottomData">Specifies a handle to the Bottom data in GPU memory.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="hScaleData">Specifies a handle to the scale data in GPU memory.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nNum">Specifies the number of input items.</param>
            <param name="nChannels">Specifies the number of channels per input item.</param>
            <param name="nHeight">Specifies the height of each input item.</param>
            <param name="nWidth">Specifies the width of each input item.</param>
            <param name="nSize"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fNegativeBeta">Specifies the negative beta value.</param>
            <param name="fCacheRatio"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.sgd_update(System.Int32,System.Int64,System.Int64,`0,`0)">
            <summary>
            Perform the Stochastic Gradient Descent (SGD) update
            </summary>
            <remarks>
            See [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hHistoryData">Specifies a handle to the history data in GPU memory.</param>
            <param name="fMomentum">Specifies the momentum value.</param>
            <param name="fLocalRate">Specifies the local learning rate.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.nesterov_update(System.Int32,System.Int64,System.Int64,`0,`0)">
            <summary>
            Perform the Nesterov update
            </summary>
            <remarks>
            See [Lecture 6c The momentum method](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Hinton, et al., 2012,
            and [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981) by Botev, et al., 2016
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hHistoryData">Specifies a handle to the history data in GPU memory.</param>
            <param name="fMomentum">Specifies the momentum value.</param>
            <param name="fLocalRate">Specifies the local learning rate.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.adagrad_update(System.Int32,System.Int64,System.Int64,`0,`0)">
            <summary>
            Perform the AdaGrad update
            </summary>
            <remarks>
            See [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) by Duchi, et al., 2011
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hHistoryData">Specifies a handle to the history data in GPU memory.</param>
            <param name="fDelta">Specifies the numerical stability factor.</param>
            <param name="fLocalRate">Specifies the local learning rate.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.adadelta_update(System.Int32,System.Int64,System.Int64,System.Int64,`0,`0,`0)">
            <summary>
            Perform the AdaDelta update
            </summary>
            <remarks>
            See [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) by Zeiler, 2012
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hHistoryData1">Specifies a handle to history data in GPU memory.</param>
            <param name="hHistoryData2">Specifies a handle to history data in GPU memory.</param>
            <param name="fMomentum">Specifies the momentum to use.</param>
            <param name="fDelta">Specifies the numerical stability factor.</param>
            <param name="fLocalRate">Specifies the local learning rate.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.adam_update(System.Int32,System.Int64,System.Int64,System.Int64,`0,`0,`0,`0)">
            <summary>
            Perform the Adam update
            </summary>
            <remarks>
            See [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v9) by Kingma, et al., 2014
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hValM"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hValV"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fBeta1"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fBeta2"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fEpsHat"><b><i>NEEDS REVIEW</i></b></param>
            <param name="fCorrectedLocalRate"><b><i>NEEDS REVIEW</i></b></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.rmsprop_update(System.Int32,System.Int64,System.Int64,`0,`0,`0)">
            <summary>
            Perform the RMSProp update
            </summary>
            <remarks>
            See [Lecture 6e	rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Tieleman and Hinton, 2012,
            and [RMSProp and equilibrated adaptive learning rates for non-convex optimization](https://arxiv.org/abs/1502.04390v1) by Dauphin, et al., 2015
            </remarks>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hNetParamsDiff">Specifies a handle to the net params diff in GPU memory.</param>
            <param name="hHistoryData">Specifies a handle to the history data in GPU memory.</param>
            <param name="fRmsDecay">Specifies the decay value used by the Solver.  MeanSquare(t) = 'rms_decay' * MeanSquare(t-1) + (1 - 'rms_decay') * SquareGradient(t).</param>
            <param name="fDelta">Specifies the numerical stability factor.</param>
            <param name="fLocalRate">Specifies the local learning rate.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lstm_fwd(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64)">
            <summary>
            Peforms the simple LSTM foward pass in Cuda.
            </summary>
            <remarks>
            See [LSTM with Working Memory](https://arxiv.org/abs/1605.01988) by Pulver, et al., 2016
            </remarks>
            <param name="t">Specifies the step within the sequence.</param>
            <param name="nN">Specifies the batch size.</param>
            <param name="nH">Specifies the number of hidden units.</param>
            <param name="hWeight_h">Specifies a handle to the GPU memory holding the 'h' weights.</param>
            <param name="hWeight_i">Specifies a handle to the GPU memory holding the 'i' weights.</param>
            <param name="hClipData">Specifies a handle to the GPU memory holding the clip data.</param>
            <param name="nClipOffset">Specifies the clip offset for this step within the sequence.</param>
            <param name="hTopData">Specifies a handle to the top data in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top data memory.</param>
            <param name="hCellData">Specifies a handle to the GPU memory holding the 'c_t' data.</param>
            <param name="nCellOffset">Specifies the c_t offset for this step within the sequence.</param>
            <param name="hPreGateData">Specifies a handle to the GPU memory holding the pre-gate data.</param>
            <param name="nPreGateOffset">Specifies the pre-gate offset for this step within the sequence.</param>
            <param name="hGateData">Specifies a handle to the GPU memory holding the gate data.</param>
            <param name="nGateOffset">Specifies the gate data offset for this step within the sequence.</param>
            <param name="hHT1Data">Specifies a handle to the GPU memory holding the HT1 data.</param>
            <param name="nHT1Offset">Specifies the HT1 offset for this step within the sequence.</param>
            <param name="hCT1Data">Specifies a handle to the GPU memory holding the CT1 data.</param>
            <param name="nCT1Offset">Specifies the CT1 offset for this step within the sequence.</param>
            <param name="hHtoGateData">Specifies a handle to the GPU memory holding the H to Gate data.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lstm_bwd(System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64,System.Int32,System.Int64)">
            <summary>
            Peforms the simple LSTM backward pass in Cuda.
            </summary>
            <remarks>
            See [LSTM with Working Memory](https://arxiv.org/abs/1605.01988) by Pulver, et al., 2016
            </remarks>
            <param name="t">Specifies the step within the sequence.</param>
            <param name="nN">Specifies the batch size.</param>
            <param name="nH">Specifies the number of hidden units.</param>
            <param name="dfClippingThreshold"></param>
            <param name="hWeight_h">Specifies a handle to the GPU memory holding the 'h' weights.</param>
            <param name="hClipData">Specifies a handle to the GPU memory holding the clip data.</param>
            <param name="nClipOffset">Specifies the clip offset for this step within the sequence.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="nTopOffset">Specifies an offset into the top diff memory.</param>
            <param name="hCellData">Specifies a handle to the GPU memory holding the 'c_t' data.</param>
            <param name="hCellDiff">Specifies a handle to the GPU memory holding the 'c_t' gradients.</param>
            <param name="nCellOffset">Specifies the c_t offset for this step within the sequence.</param>
            <param name="hPreGateDiff">Specifies a handle to the GPU memory holding the pre-gate gradients.</param>
            <param name="nPreGateOffset">Specifies the pre-gate offset for this step within the sequence.</param>
            <param name="hGateData">Specifies a handle to the GPU memory holding the gate data.</param>
            <param name="hGateDiff">Specifies a handle to the GPU memory holding the gate gradients.</param>
            <param name="nGateOffset">Specifies the gate data offset for this step within the sequence.</param>
            <param name="hCT1Data">Specifies a handle to the GPU memory holding the CT1 data.</param>
            <param name="nCT1Offset">Specifies the CT1 offset for this step within the sequence.</param>
            <param name="hDHT1Diff">Specifies a handle to the GPU DHT1 gradients.</param>
            <param name="nDHT1Offset">Specifies the DHT1 offset for this step within the sequence.</param>
            <param name="hDCT1Diff">Specifies a handle to the DCT1 gradients.</param>
            <param name="nDCT1Offset">Specifies the DCT1 offset for this step within the sequence.</param>
            <param name="hHtoHData">Specifies a handle to the GPU memory holding the H to H data.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lstm_unit_fwd(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Peforms the simple LSTM foward pass in Cuda for a given LSTM unit.
            </summary>
            <remarks>
            See [LSTM with Working Memory](https://arxiv.org/abs/1605.01988) by Pulver, et al., 2016
            </remarks>
            <param name="nCount"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nHiddenDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nXCount"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hX"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hX_acts"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC_prev"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hCont"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hH"><b><i>NEEDS REVIEW</i></b></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.lstm_unit_bwd(System.Int32,System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64,System.Int64)">
            <summary>
            Peforms the simple LSTM backward pass in Cuda for a given LSTM unit.
            </summary>
            <remarks>
            See [LSTM with Working Memory](https://arxiv.org/abs/1605.01988) by Pulver, et al., 2016
            </remarks>
            <param name="nCount"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nHiddenDim"><b><i>NEEDS REVIEW</i></b></param>
            <param name="nXCount"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC_prev"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hX_acts"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hH"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hCont"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC_diff"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hH_diff"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hC_prev_diff"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hX_acts_diff"><b><i>NEEDS REVIEW</i></b></param>
            <param name="hX_diff"><b><i>NEEDS REVIEW</i></b></param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.coeff_sum_fwd(System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a coefficient sum foward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nDim"><b><i>Specifies the dimension of the data where the data is sized 'num' x 'dim'.</i></b></param>
            <param name="nNumOffset">Specifies the offset applied to the coefficent indexing.</param>
            <param name="dfCoeff">Specifies a primary coefficient value applied to each input before summing.</param>
            <param name="hCoeffData">Optionally specifies a handle to coefficient data that is applied to the primary coefficient.</param>
            <param name="hBottom">Specifies a handle to the bottom data in GPU memory.</param>
            <param name="hTop">Specifies a handle to the top data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.coeff_sum_bwd(System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64,System.Int64)">
            <summary>
            Performs a coefficient sum backward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nDim"><b><i>Specifies the dimension of the data where the data is sized 'num' x 'dim'.</i></b></param>
            <param name="nNumOffset">Specifies the offset applied to the coefficent indexing.</param>
            <param name="dfCoeff">Specifies a primary coefficient value applied to each input before summing.</param>
            <param name="hCoeffData">Optionally specifies a handle to coefficient data that is applied to the primary coefficient.</param>
            <param name="hTopDiff">Specifies a handle to the top diff in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.cross_entropy_fwd(System.Int32,System.Int64,System.Int64,System.Int64,System.Boolean,System.Int32,System.Int64)">
            <summary>
            Performs a sigmoid cross entropy forward pass in Cuda.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hInput">Specifies a handle to the input data in GPU memory.</param>
            <param name="hTarget">Specifies a handle to the target data in GPU memory.</param>
            <param name="hLoss">Specifies a handle to the loss data in GPU memory.</param>
            <param name="bHasIgnoreLabel">Specifies whether or not an ignore label is used.</param>
            <param name="nIgnoreLabel">Specifies the ignore label which is used when <i>bHasIgnoreLabel</i> is <code>true</code></param>
            <param name="hCountData">Specifies a handle to the count data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.cross_entropy_ignore(System.Int32,System.Int32,System.Int64,System.Int64)">
            <summary>
            Performs a sigmoid cross entropy backward pass in Cuda when an ignore label is specified.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="nIgnoreLabel">Specifies the label to ignore.</param>
            <param name="hTarget">Specifies a handle to the target data in GPU memory.</param>
            <param name="hBottomDiff">Specifies a handle to the bottom diff in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.matrix_meancenter_by_column(System.Int32,System.Int32,System.Int64,System.Int64,System.Int64,System.Boolean)">
            <summary>
            Mean center the data by columns, where each column is summed and then subtracted from each column value.
            </summary>
            <param name="nWidth">Number of columns in the matrix (dimension D)</param>
            <param name="nHeight">Number of rows in the matrix (dimension N)</param>
            <param name="hA">Input data matrix - N x D matrix (N rows, D columns)</param>
            <param name="hB">Column sums vector - D x 1 vector containing the sum of each column.</param>
            <param name="hY">Output data matrix - N x D matrix (N rows, D columns) containing mean centering of the input data matrix.</param>
            <param name="bNormalize">When true, each data item is divided by N to normalize each row item by column.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.gaussian_blur(System.Int32,System.Int32,System.Int32,System.Int32,System.Double,System.Int64,System.Int64)">
            <summary>
            The gaussian_blur runs a Gaussian blurring operation over each channel of the data using the sigma.
            </summary>
            <remarks>
            The gaussian blur operation runs a 3x3 patch, initialized with the gaussian distribution using the formula
            @f$
            G(x, y) = \frac{1}{{2\pi\sigma^2 }}e^{{{ - \left( {x^2 - y^2 } \right) } \mathord{\left/ {\vphantom {{ - \left( {x^2 - y^2 } \right) } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}
            @f$
            @see [Gaussian Blur](https://en.wikipedia.org/wiki/Gaussian_blur) on Wikipedia for more information.
            </remarks>
            <param name="n">Specifies the number of items in the memory of 'X'.</param>
            <param name="nChannels">Specifies the number of channels (i.e. 3 for RGB, 1 for B/W).</param>
            <param name="nHeight">Specifies the height of each item.</param>
            <param name="nWidth">Specifies the width of each item.</param>
            <param name="dfSigma">Specifies the sigma used in the gaussian blur.</param>
            <param name="hX">Specifies a handle to GPU memory containing the source data to blur.</param>
            <param name="hY">Specifies a handle to GPU memory where the blurred information is placed.</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.hamming_distance(System.Int32,System.Double,System.Int64,System.Int64,System.Int64,System.Int32,System.Int32,System.Int32)">
            <summary>
            The hamming_distance calculates the Hamming Distance between X and Y both of length <i>n</i>.
            </summary>
            <remarks>
            To calculate the hamming distance first, X and Y are bitified where each element is converted to 1 if > than the threshold, or 0 otherwise.
            Next, the bitified versions of X and Y are subtracted from one another, and the Asum of the result is returned, which is the
            number of bits that are different, thus the Hamming distance.
            </remarks>
            <param name="n">Specifies the number of elements to compare in both X and Y.</param>
            <param name="dfThreshold">Specifies the threshold used to 'bitify' both X and Y</param>
            <param name="hA">Specifies the handle to the GPU memory containing the first vector to compare.</param>
            <param name="hB">Specifies the handle to the GPU memory containing the second vector to compare.</param>
            <param name="hY">Specifies the handle to the GPU memory where the hamming difference (bitified A - bitified B) is placed.</param>
            <param name="nOffA">Optionally, specifies an offset into the GPU memory of A, the default is 0.</param>
            <param name="nOffB">Optionally, specifies an offset into the GPU memory of B, the default is 0.</param>
            <param name="nOffY">Optionally, specifies an offset into the GPU memory of Y, the default is 0.</param>
            <returns>The hamming distance is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnn`1.calculate_batch_distances(MyCaffe.common.DistanceMethod,System.Double,System.Int32,System.Int64,System.Int64,System.Int64,System.Int32[0:,0:])">
            <summary>
            The calculate_batch_distances method calculates a set of distances based on the DistanceMethod specified.
            </summary>
            <param name="distMethod">Specifies the DistanceMethod to use (i.e. HAMMING or EUCLIDEAN).</param>
            <param name="dfThreshold">Specifies the threshold used when binarifying the values for the HAMMING distance.  This parameter is ignored when calculating the EUCLIDEAN distance.</param>
            <param name="nItemDim">Specifies the dimension of a single item.</param>
            <param name="hSrc">Specifies the GPU memory containing the source items.</param>
            <param name="hTargets">Specifies the GPU memory containing the target items that are compared against the source items.</param>
            <param name="hWork">Specifies the GPU memory containing the work memory - this must be the same size as the maximum size of the src or targets.</param>
            <param name="rgOffsets">Specifies the array of offset pairs where the first offset is into the source and the second is into the target.</param>
            <returns>The array distances corresponding to each offset pair is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.CudaDnnMemoryTracker`1">
            <summary>
            The CudaDnnMemoryTracker is used for diagnostics in that it helps estimate the
            amount of memory that a Net will use.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.CudaDnnMemoryTracker`1.#ctor(System.Boolean)">
            <summary>
            The CudaDnnMemoryTracker constructor.
            </summary>
            <param name="bEnableMemoryTrace">Optionally, specifies to enable the memory trace (only supported in debug builds).</param>
        </member>
        <member name="M:MyCaffe.common.CudaDnnMemoryTracker`1.AllocMemory(System.Int64,System.Int32,System.Int64,System.UInt64,System.Boolean)">
            <summary>
            Simulate a memory allocation.
            </summary>
            <param name="hKernel">Specifies the CudaDnn kernel that holds the allocated memory.</param>
            <param name="nDeviceID">Specifies the CudaDnn device ID on which the memory was allocated.</param>
            <param name="hMemory">Specifies the CudaDnn handle to the memory.</param>
            <param name="lSize">Specifies the size of the memory (in items).</param>
            <param name="bHalf">Specifies whether or not half memory is used.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.CudaDnnMemoryTracker`1.FreeMemory(System.Int64,System.Int32,System.Int64)">
            <summary>
            Simulate a memory free.
            </summary>
            <param name="hKernel">Specifies the CudaDnn kernel that holds the allocated memory.</param>
            <param name="nDeviceID">Specifies the CudaDnn device ID on which the memory was allocated.</param>
            <param name="hMemory">Specifies the CudaDnn handle to the memory.</param>
        </member>
        <member name="P:MyCaffe.common.CudaDnnMemoryTracker`1.EnableMemoryTrace">
            <summary>
            Enable/disable the memory trace - this feature is only available in debug builds.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnnMemoryTracker`1.TotalItemsAllocated">
            <summary>
            Returns the total number of items allocated.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnnMemoryTracker`1.TotalMemoryUsed">
            <summary>
            Returns the total amount of memory used (in bytes).
            </summary>
        </member>
        <member name="P:MyCaffe.common.CudaDnnMemoryTracker`1.TotalMemoryUsedText">
            <summary>
            Returns a text string describing the total amount of memory used (in bytes).
            </summary>
        </member>
        <member name="T:MyCaffe.common.DebugInformation`1">
            <summary>
            The DebugInformation contains information used to help debug the Layers of a Net
            while it is training.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.#ctor(System.String,MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            The DebugInformation constructor.
            </summary>
            <param name="strName">Specifies a name for the DebugInformation.</param>
            <param name="work">Specifies a workspace Blob.</param>
            <param name="bDetectNans">Specifies whether or not to detect Nan's in the data.</param>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.Dispose">
            <summary>
            Releases the memory (GPU and Host) used by the DebugInformation including the Workspace.
            </summary>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.FindLayer(System.String)">
            <summary>
            Searches for the named layer.
            </summary>
            <param name="strLayer">Specifies the name of the layer to look for.</param>
            <returns>If found, the layer debug information is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.Add(MyCaffe.layers.Layer{`0},MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Adds a Layer and its Bottom and Top blob collections.
            </summary>
            <param name="layer">Specifies the Layer.</param>
            <param name="colBottomBlobs">Specifies the Bottom Blobs flowing into the Layer.</param>
            <param name="colTopBlobs">Specifies the Top Blobs flowing out of the Layer.</param>
        </member>
        <member name="P:MyCaffe.common.DebugInformation`1.Name">
            <summary>
            Returns the name of the DebugInformation.
            </summary>
        </member>
        <member name="P:MyCaffe.common.DebugInformation`1.ExtraInfo">
            <summary>
            Get/set extra information about the DebugInformation.
            </summary>
        </member>
        <member name="P:MyCaffe.common.DebugInformation`1.LayerInfoList">
            <summary>
            Returns an array of LayerDebugInformation corresponding to each Layer added.
            </summary>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.Compare(MyCaffe.common.DebugInformation{`0})">
            <summary>
            Compares this DebugInformation to another.
            </summary>
            <param name="dbg">Specifies the other DebugInformation to compare to.</param>
            <returns>If the two DebugInformations are the same, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.DetectFirstNaN(System.String@)">
            <summary>
            Searches for the first NaN within any of the Layers.
            </summary>
            <param name="strType">Returns the Layer type for which a NaN was detected (if any).</param>
            <returns>If found, the name of the Blob in which a NaN was detected is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.DetectLastNaN(System.String@)">
            <summary>
            Searches for the last NaN within any of the Layers.
            </summary>
            <param name="strType">Returns the Layer type for which a NaN was detected (if any).</param>
            <returns>If found, the name of the Blob in which a NaN was detected is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.DebugInformation`1.ToString">
            <summary>
            Returns a string representation of the DebugInformation.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.LayerDebugInformation`1">
            <summary>
            The LayerDebugInformation describes debug information relating to a given Layer in the Net.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.#ctor(MyCaffe.layers.Layer{`0},MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0},MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            The LayerDebugInformation constructor.
            </summary>
            <param name="layer">Specifies the Layer.</param>
            <param name="colBottom">Specifies the Bottom blobs flowing into the Layer.</param>
            <param name="colTop">Specifies the Top blobs flowing out of the Layer.</param>
            <param name="work">Specifies the Workspace data.</param>
            <param name="bDetectNans">Specifies whether or not to detect Nan's in the data.</param>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.Name">
            <summary>
            Returns the name of the Layer managed by the LayerDebugInformation.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.Type">
            <summary>
            Returns the type of the Layer managed by the LayerDebugInformation.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.ForwardTiming">
            <summary>
            Returns the timing of the last forward pass made by the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.ForwardTimingAverage">
            <summary>
            Returns the average timing of the forward passes made by the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.BackwardTiming">
            <summary>
            Returns the timing of the last backward pass made by the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.BackwardTimingAverage">
            <summary>
            Returns the average timing of the backward passes made by the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.common.LayerDebugInformation`1.BlobInfoList">
            <summary>
            Returns the collection of BlobDebugInformation for the Layer.
            </summary>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.FindBlob(System.String)">
            <summary>
            Searches for the named blob.
            </summary>
            <param name="strBlob">Specifies the name of the blob to look for.</param>
            <returns>If found, the blob debug information is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.Compare(MyCaffe.common.LayerDebugInformation{`0})">
            <summary>
            Compare this LayerDebugInformation to another.
            </summary>
            <param name="dbg">Specifies the other LayerDebugInformation to compare to.</param>
            <returns>If the two LayerDebugInformation's are the same, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.DetectFirstNaN(System.String@)">
            <summary>
            Searches for the first NaN within the Layer.
            </summary>
            <param name="strType">Returns the Layer type for which a NaN was detected (if any).</param>
            <returns>If found, the name of the Blob in which a NaN was detected is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.DetectLastNaN(System.String@)">
            <summary>
            Searches for the last NaN within the Layer.
            </summary>
            <param name="strType">Returns the Layer type for which a NaN was detected (if any).</param>
            <returns>If found, the name of the Blob in which a NaN was detected is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.LayerDebugInformation`1.ToString">
            <summary>
            Returns the string representation of the LayerDebugInformation.
            </summary>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.common.BlobDebugInformation`1">
            <summary>
            The BlobDebugInformation describes debug information relating to a given Blob in a given Layer.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="T:MyCaffe.common.BlobDebugInformation`1.BLOBTYPE">
            <summary>
            Defines the Blob type.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.BLOBTYPE.DATA">
            <summary>
            The Blob contains Data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.BLOBTYPE.PARAM">
            <summary>
            The Blob is a parameter blob (e.g. learnable parameters).
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.BLOBTYPE.INTERNAL">
            <summary>
            The Blob is an internal blob to the Layer.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BlobDebugInformation`1.LOCATION">
            <summary>
            Defines the location of the Blob.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.LOCATION.NONE">
            <summary>
            No location is specified.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.LOCATION.BOTTOM">
            <summary>
            The Blob is a Bottom Blob.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BlobDebugInformation`1.LOCATION.TOP">
            <summary>
            The Blob is a Top Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlobDebugInformation`1.#ctor(MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0},MyCaffe.common.BlobDebugInformation{`0}.BLOBTYPE,MyCaffe.common.BlobDebugInformation{`0}.LOCATION,System.Boolean)">
            <summary>
            The BlobDebugInformation constructor.
            </summary>
            <param name="b">Specifies the Blob.</param>
            <param name="work">Specifies the workspace.</param>
            <param name="type">Specifies the Blob type.</param>
            <param name="loc">Specifies the Blob location.</param>
            <param name="bDetectNans">Specifies whether or not to detect Nan's in the data.</param>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.Name">
            <summary>
            Returns the name of the Blob.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.Size">
            <summary>
            Returns the size of the Blob.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DataMinValue">
            <summary>
            Returns the minimum value of the Blob data.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DataMaxValue">
            <summary>
            Returns the maximum value of the Blob data.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DataNanCount">
            <summary>
            Returns the number of nans detected in the Blob data.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DataInfCount">
            <summary>
            Returns the number of infinity values detected in the Blob data.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DiffMinValue">
            <summary>
            Returns the minimum value of the Blob diff.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DiffMaxValue">
            <summary>
            Returns the maximum value of the Blob diff.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DiffNanCount">
            <summary>
            Returns the number of nans detected in the Blob diff.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.DiffInfCount">
            <summary>
            Returns the number of infinity values detected in the Blob diff.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.BlobType">
            <summary>
            Returns the Blob type.
            </summary>
        </member>
        <member name="P:MyCaffe.common.BlobDebugInformation`1.BlobLocation">
            <summary>
            Returns the Blob location.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlobDebugInformation`1.Compare(MyCaffe.common.BlobDebugInformation{`0})">
            <summary>
            Compares this BlobDebugInformation to another one.
            </summary>
            <param name="dbg">Specifies the other BlobDebugInformation to compare to.</param>
            <returns>If the two BlobDebugInformation's are the same, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobDebugInformation`1.DetectFirstNaN(System.String@)">
            <summary>
            Search for the first NaN in the Blob.
            </summary>
            <param name="strType">Returns the type of the Blob.</param>
            <returns>If a NaN is found, the name of the Blob is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobDebugInformation`1.DetectLastNaN(System.String@)">
            <summary>
            Search for the last NaN in the Blob.
            </summary>
            <param name="strType">Returns the type of the Blob.</param>
            <returns>If a NaN is found, the name of the Blob is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.BlobDebugInformation`1.ToString">
            <summary>
            Returns a string representation of the BlobDebugInformation.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.WorkspaceArgs">
            <summary>
            The WorkspaceArgs are passed to both the Layer::OnSetWorkspace and Layer::OnGetWorkspace events.
            </summary>
            <remarks>
            These events allow for sharing workspace GPU memory among layers thus conserving overall GPU memory usage.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.WorkspaceArgs.#ctor(System.Int64,System.UInt64)">
            <summary>
            The WorkspaceArgs constructor.
            </summary>
            <param name="hData">Specifies a handle to the GPU memory.</param>
            <param name="lSize">Specifies the size of the workspace memory (in bytes).</param>
        </member>
        <member name="P:MyCaffe.common.WorkspaceArgs.Data">
            <summary>
            Get/set the handle to workspace data in GPU memory.
            </summary>
        </member>
        <member name="P:MyCaffe.common.WorkspaceArgs.Size">
            <summary>
            Get/set the size of the workspace memory (in bytes).
            </summary>
        </member>
        <member name="T:MyCaffe.common.GetConversionBlobArgs`1">
            <summary>
            The GetConversionBlobArgs are passed to the Layer::OnGetConversionBlobs event which fires each time a blob needs to be converted form half to base or back.
            </summary>
            <typeparam name="T">Specifies the default base type.</typeparam>
        </member>
        <member name="M:MyCaffe.common.GetConversionBlobArgs`1.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.common.GetConversionBlobArgs`1.BaseWork">
            <summary>
            The work blob in the base size.
            </summary>
        </member>
        <member name="P:MyCaffe.common.GetConversionBlobArgs`1.HalfWork">
            <summary>
            The work blob in the half size.
            </summary>
        </member>
        <member name="T:MyCaffe.common.GetWorkBlobArgs`1">
            <summary>
            The GetWorkBlobArgs are passed to the Layer::OnGetWorkBlob event which is supported for debugging only.
            </summary>
            <typeparam name="T">Specifies the default type.</typeparam>
        </member>
        <member name="M:MyCaffe.common.GetWorkBlobArgs`1.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.common.GetWorkBlobArgs`1.Blob">
            <summary>
            Specifies the blob.
            </summary>
        </member>
        <member name="T:MyCaffe.common.TestArgs">
            <summary>
            The TestArgs are passed to the Solver::OnTest event.
            </summary>
            <remarks>
            The Solver:OnTest event allows for overriding the Solver::Test functionality.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.TestArgs.#ctor(System.Int32,System.Int32)">
            <summary>
            The TestArgs constructor.
            </summary>
            <param name="nIterationOverride">When greater than 0, specifies a testing iteration override, otherwise the value is ignored.</param>
            <param name="nTestNetID">Specifies the test Net that the Solver would like to test.</param>
        </member>
        <member name="P:MyCaffe.common.TestArgs.IterationOverride">
            <summary>
            Returns the testing iteration override.  When set to -1, the solver description test iteration is used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TestArgs.TestNetID">
            <summary>
            Returns the test Net identifier of the Solver test Net to run.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TestArgs.Accuracy">
            <summary>
            Get/set the accuracy for the test run.  When overriding the testing, the override should set the accuracy value.
            </summary>
        </member>
        <member name="T:MyCaffe.common.TestingIterationArgs`1">
            <summary>
            Specifies the TestingIterationArgs sent to the Solver::OnTestingIteration, which is called at the end of a testing cycle.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.TestingIterationArgs`1.#ctor(System.Int32,System.Double,System.Double)">
            <summary>
            The TestingIterationArgs constructor.
            </summary>
            <param name="nIteration">Specifies the iteration of the test cycle.</param>
            <param name="dfAccuracy">Specifies the accuracy of the test cycle.</param>
            <param name="dfMsTiming">Specifies the timing (in ms.) of the test cycle.</param>
        </member>
        <member name="P:MyCaffe.common.TestingIterationArgs`1.Accuracy">
            <summary>
            Return the accuracy of the test cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TestingIterationArgs`1.Iteration">
            <summary>
            Return the iteration of the test cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TestingIterationArgs`1.Timing">
            <summary>
            Return the timing of the test cycle in ms.
            </summary>
        </member>
        <member name="T:MyCaffe.common.TrainingIterationArgs`1">
            <summary>
            The TrainingIterationArgs is sent to the Solver::OnTrainingIteration event that fires at the end of a training cycle.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.TrainingIterationArgs`1.#ctor(System.Int32,System.Double,System.Double,System.Double,System.Double,System.Boolean,System.String,System.String,System.String,System.String,System.Double,System.Double,MyCaffe.common.DebugInformation{`0})">
            <summary>
            The TrainingIterationArgs constructor.
            </summary>
            <param name="nIteration">Specifies the iteration of the training cycle.</param>
            <param name="dfAccuracy">Specifies the last accuracy recieved during the training cycle's last testing cycle.</param>
            <param name="dfLoss">Specifies the loss of the training cycle.</param>
            <param name="dfSmoothedLoss">Specifies the averaged loss of the training cycle.</param>
            <param name="dfBestSmoothedLoss">Specifies the best smoothed loss observed so far during the training.</param>
            <param name="bWeightsUpdated">Specifies whether or not the weights have been updated.</param>
            <param name="strActiveLabelCounts">Specifies the current active label counts observed.</param>
            <param name="strLabelQueryHitPercents">Specifies the current label query hit percentages observed.</param>
            <param name="strLabelQueryEpochs">Specifies the current label epoch count per label observed.</param>
            <param name="strBoostQueryHitPercents">Specifies the current boost query hit percentages observed.</param>
            <param name="dfLearningRate">Specifies the current learning rate.</param>
            <param name="dfMsTiming">Specifies the timing of the training cycle.</param>
            <param name="dbgInfo">Optionally, specifies the DebugInformation of the training cycle.  This value is set when Solver::EnableBlobDebugging == <i>true</i>.</param>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.Loss">
            <summary>
            Returns the loss of the training cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.SmoothedLoss">
            <summary>
            Retunrs the average loss after the training cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.BestSmoothedLoss">
            <summary>
            Returns the best smoothed loss observed during the training.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.WeightsUpdated">
            <summary>
            Returns whether or not the weights have been updated.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.ActiveLabelCounts">
            <summary>
            Returns the current active label counts as a string.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.LabelQueryHitPercents">
            <summary>
            Returns the current label query hit percentages observed at this point in training.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.LabelQueryEpochs">
            <summary>
            Returns the current label epochs per label observed at this point in training.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.BoostQueryHitPercents">
            <summary>
            Returns the current boost query hit percentages observed at this point in training.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.LearningRate">
            <summary>
            Return the current learning rate.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TrainingIterationArgs`1.DebugInformation">
            <summary>
            Returns the DebugInformation (if any).  The DebugInformation is set to null when Solver::EnableBlobDebugging == <i>false</i>.
            </summary>
        </member>
        <member name="T:MyCaffe.common.GetBytesArgs">
            <summary>
            The GetBytesArgs is passed along to the SnapshotArgs::OnGetWeights and SnapshotArgs::OnGetState events.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GetBytesArgs.#ctor">
            <summary>
            The GetBytesArgs constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.common.GetBytesArgs.Data">
            <summary>
            Get/set the data as an array of bytes.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SnapshotArgs">
            <summary>
            The SnapshotArgs is sent to the Solver::OnSnapshot event which fires each time the Solver::Snapshot method is called.
            </summary>
        </member>
        <member name="E:MyCaffe.common.SnapshotArgs.OnGetWeights">
            <summary>
            Specifies the OnGetWeights event which fires when the SnapshotArgs::UpdateWeights method is called.
            </summary>
            <remarks>
            The Solver hooks into these events so that it can access the training Net weights and return them to the caller of the SnapshotArgs::UpdateWeights method.
            </remarks>
        </member>
        <member name="E:MyCaffe.common.SnapshotArgs.OnGetState">
            <summary>
            Specifies the OnGetState event which fires when the SnapshotArgs::UpdateState method is called.
            </summary>
            <remarks>
            The Solver hooks into these events so that it can access the Solver state and return it to the caller of the SnapshotArgs::UpdateState method.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.SnapshotArgs.#ctor(System.Byte[],System.Byte[],System.Double,System.Double,System.Int32,MyCaffe.basecode.SNAPSHOT_WEIGHT_UPDATE_METHOD)">
            <summary>
            The SnapshotArgs constructor.
            </summary>
            <param name="rgState">Specifies the current Solver state as an array of bytes.</param>
            <param name="rgWeights">Specifies the current training Net weights as an array of bytes.</param>
            <param name="dfAccuracy">Specifies the last accuracy observed in the training Net.</param>
            <param name="dfError">Specifies the last error observed in the training Net.</param>
            <param name="nIteration">Specifies the current iteration of training.</param>
            <param name="favor">Specifies whether to favor the error value or the accuracy value when deciding whether or not a snapshot should take place.</param>
        </member>
        <member name="M:MyCaffe.common.SnapshotArgs.UpdateState">
            <summary>
            Retrieves the updated Solver state as an array of bytes.
            </summary>
            <returns>The state is returned as an array of bytes.</returns>
        </member>
        <member name="M:MyCaffe.common.SnapshotArgs.UpdateWeights">
            <summary>
            Retrieves the updated training Net weights as an array of bytes.
            </summary>
            <returns>The training Net weights are returned as an array of bytes.</returns>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.State">
            <summary>
            Get/set the Solver State.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Weights">
            <summary>
            Get/set the Weights.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Accuracy">
            <summary>
            Returns the last observed Solver accuracy for the current training session.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Error">
            <summary>
            Returns the last observed Solver error for the current training session.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Iteration">
            <summary>
            Returns the current iteration of the current training session.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Favor">
            <summary>
            Specifies whether to favor the error, the accuracy or both when deciding whether a snapshot should take place.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.IncludeWeights">
            <summary>
            Get/set whether or not to include the weights in the snapshot.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.IncludeState">
            <summary>
            Get/set whether or not to include the Solver state in the snapshot.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.SingleStep">
            <summary>
            Get/set the Solver single step.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Forced">
            <summary>
            Get/set whether or not the snapshot was forced or not.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SnapshotArgs.Scheduled">
            <summary>
            Get/set whether or not the snapshot is a regular scheduled snapshot (e.g. not an improved accuracy or forced snapshot)
            </summary>
        </member>
        <member name="T:MyCaffe.common.CustomForwardBackArgs`1">
            <summary>
            The CustomForwardBackArgs provide the arguments to the OnCustomForwardBack event within the Solver Step function.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.CustomForwardBackArgs`1.#ctor(MyCaffe.common.Net{`0},MyCaffe.common.TRAIN_STEP)">
            <summary>
            The constructor.
            </summary>
            <param name="net">Specifies the training network.</param>
            <param name="step">Specifies whether or not to step the operation.</param>
        </member>
        <member name="P:MyCaffe.common.CustomForwardBackArgs`1.net">
            <summary>
            Returns the training network.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CustomForwardBackArgs`1.step">
            <summary>
            Returns whether or not to step the operation.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CustomForwardBackArgs`1.FwdPassNanFree">
            <summary>
            Get/set whether or a NAN was detected in the forward pass.
            </summary>
        </member>
        <member name="P:MyCaffe.common.CustomForwardBackArgs`1.LocalLoss">
            <summary>
            Get/set the local loss of the pass.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ForwardArgs`1">
            <summary>
            The ForwardArgs are passed to the OnForward event of the EventLayer.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.ForwardArgs`1.#ctor(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The constructor.
            </summary>
            <param name="colBottom">Specifies the bottom blobs.</param>
            <param name="colTop">Specifies the top blobs.</param>
        </member>
        <member name="P:MyCaffe.common.ForwardArgs`1.BottomVec">
            <summary>
            Returns the bottom blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ForwardArgs`1.TopVec">
            <summary>
            Returns the top blobs.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BackwardArgs`1">
            <summary>
            The BackwardArgs are passed to the OnBackward event of the EventLayer.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.BackwardArgs`1.#ctor(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The constructor.
            </summary>
            <param name="colTop">Specifies the top blobs.</param>
            <param name="rgbPropagateDown">Specifies whether or not to backpropagate down.</param>
            <param name="colBottom">Specifies the bottom blobs.</param>
        </member>
        <member name="P:MyCaffe.common.BackwardArgs`1.PropagateDown">
            <summary>
            Returns the list on whether or not to propagate down.
            </summary>
        </member>
        <member name="T:MyCaffe.common.GradientsReadyArgs">
            <summary>
            The GradientsReadyArgs is sent to the Solver::OnGradientsReady event which fires at the end of each Solver::Step. 
            </summary>
            <remarks>
            The Solver::OnGradientReady event is used in multi-GPU training.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.GradientsReadyArgs.#ctor">
            <summary>
            The GradientReadyArgs constructor.
            </summary>
        </member>
        <member name="T:MyCaffe.common.GetIterationArgs">
            <summary>
            The GetIterationArgs is sent bubbled up to the solver when a layer needs to know
            the curret training iteration.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GetIterationArgs.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GetIterationArgs.SetIteration(MyCaffe.basecode.Phase,System.Int32)">
            <summary>
            The SetIteration method is used to set the iteration and the phase.
            </summary>
            <param name="p">Specifies the phase associated with the iteration.</param>
            <param name="nIteration">Specifies the iteration.</param>
        </member>
        <member name="P:MyCaffe.common.GetIterationArgs.Iteration">
            <summary>
            Returns the iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.common.GetIterationArgs.CurrentPhase">
            <summary>
            Returns the phase.
            </summary>
        </member>
        <member name="T:MyCaffe.common.HostBuffer`1">
            <summary>
            The HostBuffer helps manage host memory, often used when implementing CPU versions of a function or layer.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.#ctor(MyCaffe.common.CudaDnn{`0})">
            <summary>
            The constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn link to all low-level functionality.</param>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.Dispose">
            <summary>
            Release all resources used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.HostBuffer`1.Handle">
            <summary>
            Returns a handle to the host buffer.
            </summary>
        </member>
        <member name="P:MyCaffe.common.HostBuffer`1.Capacity">
            <summary>
            Returns the capacity of the host buffer.
            </summary>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.Free">
            <summary>
            Free the host buffer.
            </summary>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.CopyFrom(MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            Copy the gpu data from the blob to the host buffer.
            </summary>
            <param name="b">Specifies the blob to copy from.</param>
            <param name="bFromDiff">Optionally, specifies to topy from the diff (default = false, which copies from the data).</param>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.CopyTo(MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            Copy from the host buffer to the gpu data of the blob.
            </summary>
            <param name="b">Specifies the blob to copy to.</param>
            <param name="bToDiff">Optionally, specifies to copy to the diff (default = false, which copies to the data).</param>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.CopyFromGpu(System.Int32,System.Int64)">
            <summary>
            Copy data from the GPU into the host buffer making sure to grow the host buffer capacity if needed.
            </summary>
            <param name="nCount">Specifies the number of items to copy.</param>
            <param name="hGpu">Specifies the source GPU data to copy.</param>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.CopyToGpu(System.Int32,System.Int64)">
            <summary>
            Copy data from the host buffer into the GPU memory.
            </summary>
            <param name="nCount">Specifies the number of items to copy.</param>
            <param name="hGpu">Specifies the destination GPU memory where the data is to be copied.</param>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.GetHostData">
            <summary>
            Returns the host buffer data as an array of the base type.
            </summary>
            <returns>The data is returned as an array of the base type.</returns>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.GetHostDataAsDouble">
            <summary>
            Returns the host buffer data as an array of doubles.
            </summary>
            <returns>The data is returned as an array of doubles.</returns>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.GetHostDataAsFloat">
            <summary>
            Returns the host buffer data as an array of floats.
            </summary>
            <returns>The data is returned as an array of floats.</returns>
        </member>
        <member name="M:MyCaffe.common.HostBuffer`1.SetHostData(`0[])">
            <summary>
            Set the host buffer data, making to expand the capcity if needed.
            </summary>
            <param name="rgSrc">Specifies the source data to set in the host buffer.</param>
        </member>
        <member name="T:MyCaffe.common.onSetWorkspace">
            <summary>
            Delegate used to set the OnSetworkspace event.
            </summary>
            <param name="sender">Specifies the sender.</param>
            <param name="e">Specifies the arguments.</param>
        </member>
        <member name="T:MyCaffe.common.onGetWorkspace">
            <summary>
            Delegate used to set the OnGetworkspace event.
            </summary>
            <param name="sender">Specifies the sender.</param>
            <param name="e">Specifies the arguments.</param>
        </member>
        <member name="T:MyCaffe.common.BLOB_TYPE">
            <summary>
            Defines the tpe of data held by a given Blob.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.UNKNOWN">
            <summary>
            The blob is an unknown type.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.DATA">
            <summary>
            The Blob holds Data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.IP_WEIGHT">
            <summary>
            The Blob holds an inner product weight.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.WEIGHT">
            <summary>
            The Blob holds a general weight.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.LOSS">
            <summary>
            The Blob holds Loss Data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.ACCURACY">
            <summary>
            The Blob holds Accuracy Data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.CLIP">
            <summary>
            The blob holds Clip data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.MULTIBBOX">
            <summary>
            The blob holds multi-boundingbox data.
            </summary>
            <remarks>
            The multi-box data ordering is as follows:
            [0] index of num.
            [1] label
            [2] score
            [3] bbox xmin
            [4] bbox ymin
            [5] bbox xmax
            [6] bbox ymax
            
            continues for each of the top 'n' bboxes output.
            </remarks>
        </member>
        <member name="F:MyCaffe.common.BLOB_TYPE.INTERNAL">
            <summary>
            The blob is an internal blob used within the layer.
            </summary>
        </member>
        <member name="T:MyCaffe.common.TRAIN_STEP">
            <summary>
            Defines the training stepping method (if any).
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRAIN_STEP.NONE">
            <summary>
            No stepping.
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRAIN_STEP.FORWARD">
            <summary>
            Step only in the forward direction.
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRAIN_STEP.BACKWARD">
            <summary>
            Step only in the backward direction.
            </summary>
        </member>
        <member name="F:MyCaffe.common.TRAIN_STEP.BOTH">
            <summary>
            Step in both directions (one forward and one backward).
            </summary>
        </member>
        <member name="T:MyCaffe.common.IXDebugData`1">
            <summary>
            The IXDebugData interface is implemented by the DebugLayer to give access to the debug information managed by the layer.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="P:MyCaffe.common.IXDebugData`1.data">
            <summary>
            Returns the collection of data blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXDebugData`1.labels">
            <summary>
            Returns the collection of label blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXDebugData`1.name">
            <summary>
            Returns the name of the data set.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXDebugData`1.kernel_handle">
            <summary>
            Returns the handle to the kernel within the low-level Cuda Dnn DLL that where the data memory resides.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXDebugData`1.load_count">
            <summary>
            Returns the number of data items loaded into the collection.
            </summary>
        </member>
        <member name="T:MyCaffe.common.IXPersist`1">
            <summary>
            The IXPersist interface is used by the CaffeControl to load and save weights.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.SaveWeights(MyCaffe.common.BlobCollection{`0},System.Boolean)">
            <summary>
            Save the weights to a byte array.
            </summary>
            <remarks>
            NOTE: In order to maintain compatibility with the C++ %Caffe, extra MyCaffe features may be added to the <i>end</i> of the weight file.  After saving weights in the format
            used by the C++ %Caffe, MyCaffe writes the bytes "mycaffe.ai".  All information after these bytes are specific to MyCaffe and allow for loading weights for models by Blob name and shape
            and loosen the C++ %Caffe requirement that the 'number' of blobs match.  Adding this functionality allows for training model, changing the model structure, and then re-using the trained
            weights in the new model.  
            </remarks>
            <param name="colBlobs">Specifies the Blobs to save with the weights.</param>
            <param name="bSaveDiffs">Optionally, specifies to save the diff values.</param>
            <returns>The byte array containing the weights is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.LoadWeights(System.Byte[],System.Collections.Generic.List{System.String},MyCaffe.common.BlobCollection{`0},System.Boolean,System.Boolean@,System.Collections.Generic.List{System.String},System.Collections.Generic.List{System.String},System.String)">
            <summary>
            Loads new weights into a BlobCollection
            </summary>
            <remarks>
            NOTE: In order to maintain compatibility with the C++ %Caffe, extra MyCaffe features may be added to the <i>end</i> of the weight file.  After saving weights (see SaveWeights) in the format
            used by the C++ %Caffe, MyCaffe writes the bytes "mycaffe.ai".  All information after these bytes are specific to MyCaffe and allow for loading weights for models by Blob name and shape
            and loosen the C++ %Caffe requirement that the 'number' of blobs match.  Adding this functionality allows for training model, changing the model structure, and then re-using the trained
            weights in the new model.  
            </remarks>
            <param name="rgWeights">Specifies the weights themselves.</param>
            <param name="rgExpectedShapes">Specifies a list of expected shapes for each Blob where the weights are to be loaded.</param>
            <param name="colBlobs">Specifies the Blobs to load with the weights.</param>
            <param name="bSizeToFit">Optionally, specifies wether or not the weights should be re-sized.  Note: resizing can render the weights useless, especially in deeper, layers.</param>
            <param name="bLoadedDiffs">Returns whether or not the diffs were loaded.</param>
            <param name="inputWtInfo">Optionally, specifies the weight info describing the input weight blobs to import by name.  Note when used the number of blobs must match the number of <i>targetWtInfo</i> blobs.  Otherwise, when <i>null</i> this parameter is ignored.</param>
            <param name="targetWtInfo">Optionally, specifies the weight info describing the target weight blobs to import by name.  Note when used the number of blobs must match the number of <i>inputWtInfo</i> blobs.  Otherwise, when <i>null</i> this parameter is ignored.</param>
            <param name="strSkipBlobType">Optionally, specifies a blob type where weights are NOT loaded.  See Blob.BLOB_TYPE for the types of Blobs.</param>
            <returns>The collection of Blobs with newly loaded weights is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.SaveSolverState(MyCaffe.param.SolverState,MyCaffe.param.SolverParameter.SolverType)">
            <summary>
            Save the solver state to a byte array.
            </summary>
            <param name="state">Specifies the solver state to save.</param>
            <param name="type">Specifies the solver type.</param>
            <returns>A byte array containing the solver state is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.LoadSolverState(System.Byte[],MyCaffe.param.SolverParameter.SolverType)">
            <summary>
            Load the solver state from a byte array.
            </summary>
            <param name="rgState">Specifies the byte array containing the solver state.</param>
            <param name="type">Specifies the solver type.</param>
            <returns>The SolverState loaded is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.LoadWeightInfo(System.Byte[])">
            <summary>
            Returns the weight information describing the weights containined within the weight bytes.
            </summary>
            <param name="rgWeights">Specifies the bytes containing the weights.</param>
            <returns>The weight information is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXPersist`1.LoadWeightInfo(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Returns the weight information describing the weights containined within the Blob collection.
            </summary>
            <param name="colBlobs">Specifies the Blob collection containing the weights.</param>
            <returns>The weight information is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.IXMyCaffeState`1">
            <summary>
            The IXMyCaffeState interface contains functions related to the MyCaffeComponent state.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.SetOnTestOverride(System.EventHandler{MyCaffe.common.TestArgs})">
            <summary>
            Sets the root solver's onTest event function.
            </summary>
            <param name="onTest">Specifies the event handler called when testing.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.AddCancelOverrideByName(System.String)">
            <summary>
            Add a cancel override.
            </summary>
            <param name="strEvtCancel">Specifies the name of the new cancel event to add.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.AddCancelOverride(MyCaffe.basecode.CancelEvent)">
            <summary>
            Add a cancel override.
            </summary>
            <param name="evtCancel">Specifies the cancel event to add.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.RemoveCancelOverrideByName(System.String)">
            <summary>
            Remove a cancel override.
            </summary>
            <param name="strEvtCancel">Specifies the name of the new cancel event to remove.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.RemoveCancelOverride(MyCaffe.basecode.CancelEvent)">
            <summary>
            Remove a cancel override.
            </summary>
            <param name="evtCancel">Specifies the cancel event to remove.</param>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.EnableBlobDebugging">
            <summary>
            Enable/disable blob debugging.
            </summary>
            <remarks>
            Note, when enabled, training will dramatically slow down.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.EnableBreakOnFirstNaN">
            <summary>
            Enable/disable break training after first detecting a NaN.
            </summary>
            <remarks>
            This option requires that EnableBlobDebugging == <i>true</i>.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.EnableDetailedNanDetection">
            <summary>
            When enabled (requires EnableBlobDebugging = <i>true</i>), the detailed Nan (and Infinity) detection is perofmed on each blob when training Net.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.EnableSingleStep">
            <summary>
            Enable/disable single step training.
            </summary>
            <remarks>
            This option requires that EnableBlobDebugging == true.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.EnableLayerDebugging">
            <summary>
            Enable/disable layer debugging which causes each layer to check for NAN/INF on each forward/backward pass and throw an exception when found.
            </summary>
            <remarks>
            This option dramatically slows down training and is only recommended during debugging.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.Persist">
            <summary>
            Returns the persist used to load and save weights.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.ImageDatabase">
            <summary>
            Returns the CaffeImageDatabase used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.CancelEvent">
            <summary>
            Returns the CancelEvent used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.ActiveGpus">
            <summary>
            Returns a list of Active GPU's used by the control.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.ActiveLabelCounts">
            <summary>
            Returns a string describing the active label counts observed during training.
            </summary>
            <remarks>
            This string can help diagnos label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.LabelQueryHitPercents">
            <summary>
            Returns a string describing the label query hit percentages observed during training.
            </summary>
            <remarks>
            This string can help diagnose label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.LabelQueryEpochs">
            <summary>
            Returns a string describing the label query epochs observed during training.
            </summary>
            <remarks>
            This string can help diagnose label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.CurrentDevice">
            <summary>
            Returns the name of the current device used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.CurrentProject">
            <summary>
            Returns the name of the currently loaded project.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.CurrentIteration">
            <summary>
            Returns the current iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.common.IXMyCaffeState`1.MaximumIteration">
            <summary>
            Returns the maximum iteration.
            </summary>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.GetDeviceCount">
            <summary>
            Returns the total number of devices installed on this computer.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.GetDeviceName(System.Int32)">
            <summary>
            Returns the device name of a given device ID.
            </summary>
            <param name="nDeviceID">Specifies the device ID.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.ReInitializeParameters(System.String[])">
            <summary>
            Re-initializes each of the specified layers by re-running the filler (if any) specified by the layer.  
            When the 'rgstr' parameter is <i>null</i> or otherwise empty, the blobs of all layers are re-initialized. 
            </summary>
            <param name="rgstrLayers">Specifies the layers to reinitialize, when <i>null</i> or empty, all layers are re-initialized</param>
            <returns>If a layer is specified and found, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeState`1.VerifyCompute(System.String,System.Int32,System.Boolean)">
            <summary>
            VerifyCompute compares the current compute of the current device (or device specified) against the required compute of the current CudaDnnDLL.dll used.
            </summary>
            <param name="strExtra">Optionally, specifies extra information for the exception if one is thrown.</param>
            <param name="nDeviceID">Optionally, specifies a specific device ID to check, otherwise uses the current device used (default = -1, which uses the current device).</param>
            <param name="bThrowException">Optionally, specifies whether or not to throw an exception on a compute mis-match (default = true).</param>
            <returns>If the device's compute is >= to the required compute fo the CudaDnnDll.dll used, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.IXMyCaffe`1">
            <summary>
            The IXMyCaffe interface contains functions used to perform MyCaffe operations that work with the MyCaffeImageDatabase.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Load(MyCaffe.basecode.Phase,MyCaffe.basecode.ProjectEx,System.Nullable{MyCaffe.basecode.IMGDB_LABEL_SELECTION_METHOD},System.Nullable{MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD},System.Boolean,MyCaffe.basecode.IXImageDatabaseBase,System.Boolean,System.Boolean,System.String,System.Boolean)">
            <summary>
            Load a project and optionally the MyCaffeImageDatabase.
            </summary>
            <remarks>
            This load function uses the MyCaffeImageDatabase.
            </remarks>
            <param name="phase">Specifies the Phase for which the load should focus.</param>
            <param name="p">Specifies the Project to load.</param>
            <param name="labelSelectionOverride">Optionally, specifies the label selection override (overides the label selection in SettingsCaffe).  The label selection dictates how the label sets are selected.</param>
            <param name="imageSelectionOverride">Optionally, specifies the image selection override (overides the image selection in SettingsCaffe).  The image selection dictates how the images are selected from each label set.</param>
            <param name="bResetFirst">Optionally, resets the device before loading.  IMPORTANT: this functionality is only recommendned during testing, for resetting the device will throw off all other users of the device.</param>
            <param name="imgdb">Optionally, specifies the MyCaffeImageDatabase to use.  When <i>null</i>, an instance if the MyCaffeImageDatabase is created internally.</param>
            <param name="bUseImageDb">Optionally, specifies whehter or not to use the image database (default = true).</param>
            <param name="bCreateRunNet">Optionally, specifies whether or not to create the Run net.</param>
            <param name="strStage">Optionally, specifies the stage under which to load the model.</param>
            <param name="bEnableMemTrace">Optionally, specifies to enable the memory tracing (only available in debug builds).</param>
            <returns>If the project is loaded the function returns <i>true</i>, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Load(MyCaffe.basecode.Phase,System.String,System.String,System.Byte[],System.Nullable{MyCaffe.basecode.IMGDB_LABEL_SELECTION_METHOD},System.Nullable{MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD},System.Boolean,MyCaffe.basecode.IXImageDatabaseBase,System.Boolean,System.Boolean,System.String,System.Boolean)">
            <summary>
            Load a project and optionally the MyCaffeImageDatabase.
            </summary>
            <remarks>
            This load function uses the MyCaffeImageDatabase.
            </remarks>
            <param name="phase">Specifies the Phase for which the load should focus.</param>
            <param name="strSolver">Specifies the solver descriptor.</param>
            <param name="strModel">Specifies the model desciptor.</param>
            <param name="rgWeights">Optionally, specifies the weights to load, or <i>null</i> to ignore.</param>
            <param name="labelSelectionOverride">Optionally, specifies the label selection override (overides the label selection in SettingsCaffe).  The label selection dictates how the label sets are selected.</param>
            <param name="imageSelectionOverride">Optionally, specifies the image selection override (overides the image selection in SettingsCaffe).  The image selection dictates how the images are selected from each label set.</param>
            <param name="bResetFirst">Optionally, resets the device before loading.  IMPORTANT: this functionality is only recommendned during testing, for resetting the device will throw off all other users of the device.</param>
            <param name="imgdb">Optionally, specifies the MyCaffeImageDatabase to use.  When <i>null</i>, an instance if the MyCaffeImageDatabase is created internally.</param>
            <param name="bUseImageDb">Optionally, specifies whehter or not to use the image database (default = true).</param>
            <param name="bCreateRunNet">Optionally, specifies whether or not to create the Run net.</param>
            <param name="strStage">Optionally, specifies the stage under which to load the model.</param>
            <param name="bEnableMemTrace">Optionally, specifies to enable the memory tracing (only available in debug builds).</param>
            <returns>If the project is loaded the function returns <i>true</i>, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Unload(System.Boolean)">
            <summary>
            Unload the currently loaded project.
            </summary>
            <param name="bUnloadImageDb">Optionally, specifies whether or not to unload the image database. The default = <i>true</i>.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Train(System.Int32,System.Int32,MyCaffe.common.TRAIN_STEP,System.Double,System.Boolean)">
            <summary>
            Train the network a set number of iterations and allow for single stepping.
            </summary>
            <param name="nIterationOverride">Optionally, specifies number of iterations to run that override the iterations specified in the solver desctiptor.</param>
            <param name="nTrainingTimeLimitInMinutes">Optionally, specifies a maximum number of minutes to train.  When set to 0, this parameter is ignored and no time limit is imposed.</param>
            <param name="step">Optionally, specifies whether or not to single step the training on the forward pass, backward pass or both.  The default is <i>TRAIN_STEP.NONE</i> which runs the training to the maximum number of iterations specified.</param>
            <param name="dfLearningRateOverride">Optionally, specifies a learning rate override (default = 0 which ignores this parameter)</param>
            <param name="bReset">Optionally, reset the iterations to zero.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Test(System.Int32)">
            <summary>
            Test the network a given number of iterations.
            </summary>
            <param name="nIterationOverride">Optionally, specifies number of iterations to run that override the iterations specified in the solver desctiptor.</param>
            <returns>The accuracy value from the test is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.TestMany(System.Int32,System.Boolean,System.Boolean,MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD,System.Int32)">
            <summary>
            Test on a number of images by selecting random images from the database, running them through the Run network, and then comparing the results with the 
            expected results.
            </summary>
            <param name="nCount">Specifies the number of cycles to run.</param>
            <param name="bOnTrainingSet">Specifies on whether to select images from the training set, or when <i>false</i> the testing set of data.</param>
            <param name="bOnTargetSet">Optionally, specifies to test on the target dataset (if exists) as opposed to the source dataset.</param>
            <param name="imgSelMethod">Optionally, specifies the image selection method (default = RANDOM).</param>
            <param name="nImageStartIdx">Optionally, specifies the image start index (default = 0).</param>
            <returns>The list of SimpleDatum and their ResultCollections (after running the model on each) is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Run(System.Int32)">
            <summary>
            Run on a given image in the MyCaffeImageDatabase based on its image index.
            </summary>
            <param name="nImageIdx">Specifies the image index.</param>
            <returns>The result of the run is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Run(System.Collections.Generic.List{System.Int32})">
            <summary>
            Run on a set of images in the MyCaffeImageDatabase based on their image indexes.
            </summary>
            <param name="rgImageIdx">Specifies a list of image indexes.</param>
            <returns>A list of results from the run is returned - one result per image.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.Run(MyCaffe.basecode.SimpleDatum,System.Boolean,System.Boolean)">
            <summary>
            Run on a given Datum. 
            </summary>
            <param name="d">Specifies the Datum to run.</param>
            <param name="bSort">Optionally, specifies whether or not to sor the results.</param>
            <param name="bUseSolverNet">Optionally, specifies whether or not to use the training net vs. the run net.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetTestImage(MyCaffe.basecode.Phase,System.Int32@,System.String@)">
            <summary>
            Retrieves a random image from either the training or test set depending on the Phase specified.
            </summary>
            <param name="phase">Specifies whether to select images from the training set or testing set.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <param name="strLabel">Returns the expected label name for the image.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetTestImage(MyCaffe.basecode.Phase,System.Int32)">
            <summary>
            Retrieves a random image from either the training or test set depending on the Phase specified.
            </summary>
            <param name="phase">Specifies whether to select images from the training set or testing set.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetImageMean">
            <summary>
            Returns the image mean used by the solver network used during training.
            </summary>
            <returns>The image mean is returned as a SimpleDatum.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetDataset">
            <summary>
            Returns the current dataset used when training and testing.
            </summary>
            <returns>The DatasetDescriptor is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetWeights">
            <summary>
            Retrieves the weights of the training network.
            </summary>
            <returns>The weights are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.UpdateRunWeights">
            <summary>
            Loads the weights from the training net into the Net used for running.
            </summary>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.UpdateWeights(System.Byte[])">
            <summary>
            Loads the training Net with new weights.
            </summary>
            <param name="rgWeights">Specifies the weights to load.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffe`1.GetLicenseText(System.String)">
            <summary>
            Returns the license text for MyCaffe.
            </summary>
            <param name="strOtherLicenses">Specifies other licenses to append to the license text.</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.common.IXMyCaffeNoDb`1">
            <summary>
            The IXMyCaffeNoDb interface contains functions used to perform MyCaffe operations that run in a light-weight manner without the MyCaffeImageDatabase.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeNoDb`1.LoadToRun(System.String,System.Byte[],MyCaffe.param.BlobShape,MyCaffe.basecode.SimpleDatum,MyCaffe.param.TransformationParameter,System.Boolean)">
            <summary>
            The LoadToRun method loads the MyCaffeControl for running only (e.g. deployment).
            </summary>
            <remarks>
            This method does not use the MyCaffeImageDatabase.
            </remarks>
            <param name="strModel">Specifies the model description to load.</param>
            <param name="rgWeights">Specifies the trained weights to load.</param>
            <param name="shape">Specifies the expected shape to run on.</param>
            <param name="sdMean">Optionally, specifies the simple datum mean to subtract from input images that are run.</param>
            <param name="bForceBackward">Optionally, enables the force backward.</param>
            <param name="transParam">Optionally, specifies the TransformationParameter to use.  When using a 'deployment' model that has no data layers, you should supply a transformation parameter
            that matches the transformation used during training.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeNoDb`1.CreateDataBlob(MyCaffe.basecode.SimpleDatum,MyCaffe.common.Blob{`0})">
            <summary>
            Create a data blob from a SimpleDatum by transforming the data and placing the results in the blob returned.
            </summary>
            <param name="d">Specifies the datum to load into the blob.</param>
            <param name="blob">Optionally, specifies a blob to use instead of creating a new one.</param>
            <returns>The data blob containing the transformed data is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeNoDb`1.Run(System.Drawing.Bitmap,System.Boolean)">
            <summary>
            Run on a given bitmap image.
            </summary>
            <remarks>
            This method does not use the MyCaffeImageDatabase.
            </remarks>
            <param name="img">Specifies the input image.</param>
            <param name="bSort">Specifies whether or not to sort the results.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeNoDb`1.Run(MyCaffe.basecode.SimpleDatum,System.Boolean)">
            <summary>
            Run on a given Datum. 
            </summary>
            <param name="d">Specifies the Datum to run.</param>
            <param name="bSort">Optionally, specifies whether or not to sort the results.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="T:MyCaffe.common.IXMyCaffeExtension`1">
            <summary>
            The IXMyCaffeExtension interface allows for easy extension management of the low-level software that interacts directly with CUDA.
            </summary>
            <typeparam name="T">Specifies the base type of <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeExtension`1.CreateExtension(System.String)">
            <summary>
            Create and load a new extension DLL.
            </summary>
            <param name="strExtensionDLLPath">Specifies the path to the extension DLL.</param>
            <returns>The handle to the extension is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeExtension`1.FreeExtension(System.Int64)">
            <summary>
            Free an existing extension and unload it.
            </summary>
            <param name="hExtension">Specifies the handle to the extension to free.</param>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeExtension`1.RunExtension(System.Int64,System.Int64,`0[])">
            <summary>
            Run a function on an existing extension.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeExtension`1.RunExtensionD(System.Int64,System.Int64,System.Double[])">
            <summary>
            Run a function on an existing extension using the <i>double</i> base type.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.IXMyCaffeExtension`1.RunExtensionF(System.Int64,System.Int64,System.Single[])">
            <summary>
            Run a function on an existing extension using the <i>float</i> base type.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="T:MyCaffe.common.InternalThread`1">
            <summary>
            The InternalThread manages an internal thread used for Parallel and data collection operations.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="E:MyCaffe.common.InternalThread`1.DoWork">
            <summary>
            The DoWork event is the working thread function.
            </summary>
        </member>
        <member name="E:MyCaffe.common.InternalThread`1.OnPreStop">
            <summary>
            The OnPreStop event fires just after signalling the thread to stop.
            </summary>
        </member>
        <member name="E:MyCaffe.common.InternalThread`1.OnPreStart">
            <summary>
            The OnPreStart event fires just before starting the thread.
            </summary>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.#ctor(System.Boolean)">
            <summary>
            The InternalThread constructor.
            </summary>
            <param name="bUseThreadVsTask">Optionally, specifies to use a Thread vs a Task (default = false, e.g. use Task).</param>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.Dispose(System.Boolean)">
            <summary>
            Releases all resources used by the InernalThread.
            </summary>
            <param name="bDisposing">Set to <i>true</i> when called from Dispose().</param>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.Dispose">
            <summary>
            Releases all resources used by the InernalThread.
            </summary>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.StartInternalThread(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Int32,System.Object,System.Int32)">
            <summary>
            Starts running the internal thread function which then calls the DoWork event.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda placed in the ActionStartArgs passed along to DoWork.</param>
            <param name="log">Specifies the Log for output, placed in the ActionStartArgs passed along to DoWork.</param>
            <param name="nDeviceID">Optionally, specifies the DeviceID placed in the ActionStartArgs passed along to DoWork.</param>
            <param name="arg">Optionally, specifies an argument defined by the caller.</param>
            <param name="nInitialDelay">Optionally, specifies an initial delay in ms (default = 0).</param>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.StopInternalThread">
            <summary>
            Stops the internal thread.
            </summary>
        </member>
        <member name="M:MyCaffe.common.InternalThread`1.InternalThreadEntry(System.Object)">
            <summary>
            Specifies the internal thread entry.
            </summary>
            <param name="obj"></param>
        </member>
        <member name="P:MyCaffe.common.InternalThread`1.CancellationPending">
            <summary>
            Returns whether or not a cancellation is pending.
            </summary>
        </member>
        <member name="P:MyCaffe.common.InternalThread`1.IsStarted">
            <summary>
            Returns whether or not the internal thread has been started.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ActionStateArgs`1">
            <summary>
            The ActionStateArgs are sent to the DoWork event when fired from the InternalThreadEntry.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.ActionStateArgs`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.basecode.CancelEvent,System.Int32,System.Object,System.Int32)">
            <summary>
            The ActionStateArgs constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="evtCancel">Specifies the CancelEvent that when Set signals to DoWork that it should terminate.</param>
            <param name="nDeviceID">Optionally, specifies the DeviceID.</param>
            <param name="arg">Optionally, specifies an argument defined by the caller.</param>
            <param name="nInitialDelay">Optionally, specifies an initial delay for the thread in ms. (default = 0).</param>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.cuda">
            <summary>
            Get/set the Cuda Dnn connection to Cuda.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.log">
            <summary>
            Returns the Log used for output.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.CancelEvent">
            <summary>
            Returns the CancelEvent used to cancel the thread.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.DeviceID">
            <summary>
            Returns the Device ID of the device to use in the thread.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.Arg">
            <summary>
            Returns the user supplied argument.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ActionStateArgs`1.InitialDelay">
            <summary>
            Returns the initial delay in ms (if any).
            </summary>
        </member>
        <member name="T:MyCaffe.common.Net`1">
            <summary>
            Connects Layer's together into a direct acrylic graph (DAG)
            specified by a NetParameter
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="E:MyCaffe.common.Net`1.OnGetWorkspace">
            <summary>
            Specifies the OnGetWorkspace event that fires when the getWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="E:MyCaffe.common.Net`1.OnSetWorkspace">
            <summary>
            Specifies the OnSetWorkspace event that fires when the setWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="E:MyCaffe.common.Net`1.OnGetIteration">
            <summary>
            Specifies the OnGetIteration event that fires when a layer needs to get the current iteration from the solver.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.NetParameter,MyCaffe.basecode.CancelEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.basecode.Phase,System.Threading.AutoResetEvent,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The Net constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the NetParameter used to initialize the Net.</param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel operations run by the Net.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="phaseOverride">Optionally, specifies an override of the Phase for which the Net is used.</param>
            <param name="evtTrainingCompleted">Optionally, specifies an auto reset event that is set after training has completed.</param>
            <param name="sharedNet">Specifies another Net that shares the GPU memory created by this Net.</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.Dispose(System.Boolean)">
            <summary>
            Releases all resources (GPU and Host) used by the Net.
            </summary>
            <param name="bDisposing">Set to <i>true</i> when called from Dispose().</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.Dispose">
            <summary>
            Releases all resources (GPU and Host) used by the Net.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.Init(MyCaffe.param.NetParameter,MyCaffe.basecode.Phase,System.Threading.AutoResetEvent)">
            <summary>
            Initialize a network with a NetParameter.
            </summary>
            <param name="p">Specifies the NetParameter.</param>
            <param name="phaseOverride">Optionally, specifies a Phase override for which the Net will run under.</param>
            <param name="evtTrainingCompleted">Optionally, specifies an auto reset event that is set upon the completion of training.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.SetPhase(MyCaffe.basecode.Phase)">
            <summary>
            Change the phase of the network.
            </summary>
            <param name="phase">Specifies the new phase.</param>
            <returns>The previous phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.RestorePhase">
            <summary>
            Restore the network phase to its original state.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.EnableBreakOnFirstNaN">
            <summary>
            Enable/disable break the first NaN functionality where training stops immediately upon detecting a NaN in one of the Layer blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.EnableDetailedNanDetection">
            <summary>
            Enable/disable whether or not detailed nans are detected - this will make debugging slower and is only recommended
            when running on a TCC enabled driver (as opposed to an WDM driver one used with the monitor).
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.EnableLayerDebugging">
            <summary>
            Enable/disable layer debugging which causes each layer to check for NAN/INF on each forward/backward pass and throw an exception when found.
            </summary>
            <remarks>
            This option dramatically slows down training and is only recommended during debugging.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.Net`1.ActiveLabelCounts">
            <summary>
            Returns the active label counts observed during training.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.LabelQueryHitPercents">
            <summary>
            Return the label query hit percentages for the active datasource.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.LabelQueryEpochs">
            <summary>
            Return the label query epochs for the active datasource.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.BoostQueryHitPercents">
            <summary>
            Return the boost query hit percentages for the active datasource.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.SetEnablePassthrough(System.Boolean)">
            <summary>
            Enables/disables passthrough on each layer of the net.
            </summary>
            <remarks>
            If enabled by a given layer, the Bottom inputs are copied directly to the Top outputs during the forward pass and the forward pass returns.  This is used by the BatchDataLayer.
            </remarks>
            <param name="bEnable">Specifies whether or not to enable passthrough.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.FilterNet(MyCaffe.param.NetParameter)">
            <summary>
            Removes layers that the user specified should be excluded given the current
            phase, level and stage.
            </summary>
            <param name="param">Specifies the NetParameter to filter.</param>
            <returns>The newly filtered NetParmeter is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.StateMeetsRule(MyCaffe.param.NetState,MyCaffe.param.NetStateRule,System.String)">
            <summary>
            Returns whether NetState state meets NetStateRule rule.
            </summary>
            <param name="state">Specifies the NetState to test.</param>
            <param name="rule">Specifies the NetStateRul to test against the NetState.</param>
            <param name="strLayer">Specifies the name of the Layer for which the test is taking place.</param>
            <returns>If the NetState of the Layer meets the NetStateRule, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.AppendTop(MyCaffe.param.NetParameter,System.Int32,System.Int32,System.Collections.Generic.List{System.String},MyCaffe.common.DictionaryEx{System.String,System.Int32})">
            <summary>
            Append a new input or top blob to the net.
            </summary>
            <param name="param">Specifies the NetParameter used.</param>
            <param name="layer_id">Specifies the Layer index associated with the Blob.</param>
            <param name="top_id">Specifies the Blob index of the (top) Blob.</param>
            <param name="available_blobs">Specifies the list of available Blob names.</param>
            <param name="blob_name_to_idx">Specifies the Blob name to index dictionary lookup.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.AppendBottom(MyCaffe.param.NetParameter,System.Int32,System.Int32,System.Collections.Generic.List{System.String},MyCaffe.common.DictionaryEx{System.String,System.Int32})">
            <summary>
            Append a new bottom blob to the net.
            </summary>
            <param name="param">Specifies the NetParameter used.</param>
            <param name="layer_id">Specifies the Layer index associated with the Blob.</param>
            <param name="bottom_id">Specifies the Blob index of the (bottom) Blob.</param>
            <param name="available_blobs">Specifies the list of available Blob names.</param>
            <param name="blob_name_to_idx">Specifies the Blob name to index dictionary lookup.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.AppendParam(MyCaffe.param.NetParameter,System.Int32,System.Int32)">
            <summary>
            Append a new parameter blob to the net.
            </summary>
            <param name="param">Specifies the NetParameter used.</param>
            <param name="layer_id">Specifies the Layer index associated with the Blob.</param>
            <param name="param_id">Specifies the Blob index of the (parameter) Blob.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.ForwardFromTo(System.Int32,System.Int32)">
            <summary>
            The FromTo variant of forward and backwarde operate on the
            (topological) ordering by which the net is specified.  For general DAG
            netowrks, note that (1) computing from one layer to another might entail
            extra computation on unrelated branches, and (2) computation starting in
            the middle may be incorrect if all the layers of a fan-in are not
            included.
            </summary>
            <param name="nStart">Optionally, specifies the index of the first Layer where the Forward operation is to start.</param>
            <param name="nEnd">Optionally, specifies the index of the last Layer to run the Forward operation on.</param>
            <returns>The loss is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.Forward">
            <summary>
            Run forward with the input Blob's already fed separately.
            </summary>
            <remarks>
            You can get the input blobs using input_blobs().
            </remarks>
            <returns>The collection of output Blobs is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.Forward(System.Double@)">
            <summary>
            Run forward with the input Blob's already fed separately.
            </summary>
            <remarks>
            You can get the input blobs using input_blobs().
            </remarks>
            <param name="dfLoss">Returns the loss of the operation.</param>
            <returns>The collection of output Blobs is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.Forward(MyCaffe.common.BlobCollection{`0},System.Double@,System.Boolean)">
            <summary>
            Run forward using a set of bottom blobs and return the result.
            </summary>
            <param name="colBottom"></param>
            <param name="dfLoss"></param>
            <param name="bReshape">Optionally, specifies to reshape the input to the size of the colBottom blobs supplied.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.Backward(System.Int32,System.Int32)">
            <summary>
            The network backward should take no input and output, since it solely computes the 
            gradient w.r.t. the parameters, and the data has already been provided during the
            forward pass.
            </summary>
            <param name="nStart">Specifies the Layer index where the Backward operation is to start.</param>
            <param name="nEnd">Specifies the Layer index of the last Layer that the Backward operation is run.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.InputDebugInfo(System.Int32)">
            <summary>
            Helper for displaying debug info in Forward about input blobs.
            </summary>
            <param name="input_id">Specifies the index of the input Blob within input_blobs().</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.ForwardDebugInfo(System.Int32)">
            <summary>
            Helper for displaying debug info in Forward.
            </summary>
            <param name="layer_id">Specifies the Layer index associated with the Blob.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.BackwardDebugInfo(System.Int32)">
            <summary>
            Helper for displaying debug info in Backward.
            </summary>
            <param name="layer_id">Specifies the Layer index associated with the Blob.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.UpdateDebugInfo(System.Int32)">
            <summary>
            Helper for displaying debug info in Update.
            </summary>
            <param name="param_id">Specifies the parameter index associated with the Blob.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.ShareTrainedLayersWith(MyCaffe.common.Net{`0},System.Boolean)">
            <summary>
            For an already initialized net, implicitly compies (i.e., using no
            additional memory) the pre-trained layers from another Net.
            </summary>
            <param name="srcNet">Specifies the source Net whos blobs are shared with the calling Net.</param>
            <param name="bEnableLog">Optionally, specifies to enable the output log (default = false).</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.CopyInternalBlobsTo(MyCaffe.common.Net{`0})">
            <summary>
            Copy the internal blobs from one net to another.
            </summary>
            <param name="dstNet">Specifies the destination net.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.CopyTrainedLayersTo(MyCaffe.common.Net{`0})">
            <summary>
            Copies the trained layer of this Net to another Net.
            </summary>
            <param name="dstNet">Specifies the Net where the trained layers are to be copied.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.CopyTrainedLayersTo(MyCaffe.common.Net{`0},MyCaffe.common.DictionaryEx{System.String,System.String},System.Boolean)">
            <summary>
            Copies the trained layers of this Net to another Net.
            </summary>
            <param name="dstNet">Specifies the destination Net where the trained layers are to be copied.</param>
            <param name="rgLayerNames">Specifies the layer name dictionary lookup, where only the weights of layer names within the Dictionary lookup are copied.</param>
            <param name="bTranspose">Specifies whether or not to copy the weights and transpose the copy.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.Reshape">
            <summary>
            Reshape all layers from the bottom to the top.
            </summary>
            <remarks>
            This is useful to propagate changes to layer sizes without running
            a forward pass, e.g. to compute output feature size.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.Net`1.CopyTrainedLayersFrom(MyCaffe.param.NetParameter)">
            <summary>
            For an already initialized net, CopyTrainedLayersFrom copies the already
            trained layers from another net parameter instance.
            </summary>
            <param name="param">Specifies the NetParameter to use.</param>
        </member>
        <member name="P:MyCaffe.common.Net`1.net_param">
            <summary>
            Returns the net parameter.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.ToProto(System.Boolean)">
            <summary>
            Writes the net to a proto.
            </summary>
            <returns>A new NetParameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.Update">
            <summary>
            Updates the network weights based on the diff values computed.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.ClearParamDiffs">
            <summary>
            Zero out the diffs of all netw parameters.  This should be run before Backward.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.ShareWeights">
            <summary>
            Shares weight data of owner blobs with shared blobs.
            </summary>
            <remarks>
            Note: this is called by Net::Init, and thus should normally not be called
            manually.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.Net`1.ForwardBackward(MyCaffe.common.BlobCollection{`0},System.Double@,MyCaffe.common.TRAIN_STEP)">
            <summary>
            Runs a Forward pass followed by a Backward pass.
            </summary>
            <param name="colBottom">Optionally, specifies input data passed to the Forward pass.</param>
            <param name="dfLocalLoss">Returns the local loss of the Forward pass.</param>
            <param name="step">Optionally, specifies to step forward or backward.</param>
            <returns>If EnableBreakOnFirstNaN == <i>true</i> and a NaN is detected, this function returns <i>false</i>, otherwise <i>true</i> is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Net`1.name">
            <summary>
            Returns the network name.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.layer_names">
            <summary>
            Returns the layer names.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.blob_names">
            <summary>
            Returns the blob names.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.blobs">
            <summary>
            Returns the blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.layers">
            <summary>
            Returns the layers.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.phase">
            <summary>
            Returns the network phase: TRAIN or TEST
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.bottom_vecs">
            <summary>
            Returns the bottom vecs for each layer -- usually you won't
            need this unless you do per-layer checks such as gradients.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.top_vecs">
            <summary>
            Returns the top vecs for each layer -- usually y ou won't
            need this unless you do per-layer checks such as gradients.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.top_ids(System.Int32)">
            <summary>
            Returns the ids of the top blobs of layer i.
            </summary>
            <param name="layer_id">Specifies the index of the Layer.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.bottom_ids(System.Int32)">
            <summary>
            Returns the ids of the bottom blobs of layer i.
            </summary>
            <param name="layer_id">Specifies the index of the Layer.</param>
            <returns></returns>
        </member>
        <member name="P:MyCaffe.common.Net`1.bottom_need_backward">
            <summary>
            Returns the collection of lists that tell whether or not the bottom of each layer needs a backward pass or not.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.blob_loss_weights">
            <summary>
            Returns the collection of blob loss weights.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.layer_need_backward">
            <summary>
            Returns a collection of items that tell whether each layer nees a backward pass or not.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.parameters">
            <summary>
            Returns the parameters.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.layer_blobs(System.String)">
            <summary>
            Returns the collection of Blobs internal to a Layer.
            </summary>
            <param name="strLayerName">Specifies the name of the Layer.</param>
            <returns>The Layer's internal Blobs are returned in a collection.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.SetLearnedParameters(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Sets the learned parameters.
            </summary>
            <param name="col">Specifies a collection of Blobs containing the learned parameters.</param>
        </member>
        <member name="P:MyCaffe.common.Net`1.learnable_parameters">
            <summary>
            Returns the learnable parameters.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.params_lr">
            <summary>
            Returns the learnable parameter learning rate multipliers.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.params_weight_decay">
            <summary>
            Returns the learnable parameter decay multipliers.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.param_names_index">
            <summary>
            Returns the dictionary look for parameter names to their indexes.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.param_owners">
            <summary>
            Returns the list of parameter owner indexes.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.param_display_names">
            <summary>
            Returns the list of parameter display names.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.param_by_name(System.String,System.Boolean)">
            <summary>
            Returns a parameter given its name.
            </summary>
            <param name="strName">Specifies the Parameter's name.</param>
            <param name="bThrowExceptionOnError">Optionally, specifies to throw an exception when the layer is not found.  The default = <i>true</i>.</param>
            <returns>The Blob with the given name is returned, or <i>null</i> if not found.</returns>
        </member>
        <member name="P:MyCaffe.common.Net`1.num_inputs">
            <summary>
            Returns the number of inputs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.num_outputs">
            <summary>
            Returns the number of outputs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.input_blobs">
            <summary>
            Returns the collection of input Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.output_blobs">
            <summary>
            Returns the collection of output Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.output_blob_indices">
            <summary>
            Returns a list of the output Blob indexes.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Net`1.input_blob_indices">
            <summary>
            Returns a list of the input Blob indexes.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Net`1.has_blob(System.String)">
            <summary>
            Returns whether or not the Net contains a given Blob.
            </summary>
            <param name="strBlobName">Specifies the Blob name.</param>
            <returns>If the Net has the Blob, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.blob_by_name(System.String,System.Boolean)">
            <summary>
            Returns a blob given its name.
            </summary>
            <param name="strName">Specifies the Blob's name.</param>
            <param name="bThrowExceptionOnError">Optionally, specifies to throw an exception when the layer is not found.  The default = <i>true</i>.</param>
            <returns>The Blob with the given name is returned, or <i>null</i> if not found.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.blob_index_by_name(System.String)">
            <summary>
            Returns the index of a blob given its name.
            </summary>
            <param name="strName">Specifies the name of the blob to look for.</param>
            <returns>The index of the blob within the 'blobs' is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.has_layer(System.String)">
            <summary>
            Returns whether or not the Net has a given Layer by its name.
            </summary>
            <param name="strLayer">Specifies the Layer name.</param>
            <returns>If the Net contains the Layer, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.layer_by_name(System.String,System.Boolean)">
            <summary>
            Returns a Layer given its name.
            </summary>
            <param name="strLayer">Specifies the Layer name.</param>
            <param name="bThrowExceptionOnError">Optionally, specifies to throw an exception when the layer is not found.  The default = <i>true</i>.</param>
            <returns>The Layer with the given name is returned, or <i>null</i> if not found.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.layer_index_by_name(System.String)">
            <summary>
            Returns a Layer's index given its name.
            </summary>
            <param name="strLayer">Specifies the Layer name.</param>
            <returns>The index of the Layer with the given name is returned, or <i>-1</i> if not found.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.set_debug_info(System.Boolean)">
            <summary>
            Sets the debug information flag.
            </summary>
            <remarks>
            When set, extra debug information is output during each Forward and Backward pass, which will slow down training.
            </remarks>
            <param name="bVal">Specifies whether to enable/disable debug information.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.InsertSplits(MyCaffe.param.NetParameter)">
            <summary>
            Create a new NetParameter and insert splits into it based on a given NetParameter.
            </summary>
            <remarks>
            Splits are used when a given input (top) is used by more than one Layer.  For example a DataLayer 'label' top may
            used by both an AccuracyLayer and SoftmaxLossLayer.  In such a case a split is created that allows the 'label' top
            to be sent to both.
            </remarks>
            <param name="param">Specifies the original NetParameter.</param>
            <returns>A new NetParameter containing split layers is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.LoadWeights(System.Byte[],MyCaffe.common.IXPersist{`0},System.Collections.Generic.List{System.String},System.Collections.Generic.List{System.String},System.String)">
            <summary>
            Loads new weights into the Net.
            </summary>
            <param name="rgWeights">Specifies the weights themselves.</param>
            <param name="persist">Specifies an interface to the persistance object used to load the weights.</param>
            <param name="inputWtInfo">Optionally, specifies the input blobs to import.  Note, when set, the <i>targetWtInfo</i> must also be specified.  When <i>null</i>, this parameter is ignored.</param>
            <param name="targetWtInfo">Optionally, specifies the target blobs to import into.  Note, when set, the <i>inputWtInfo</i> must also be specified.  When <i>null</i>, this parameter is ignored.</param>
            <param name="strSkipBlobType">Optionally, specifies a blob type where weights are NOT loaded.  See Blob.BLOB_TYPE for the types of Blobs.</param>
        </member>
        <member name="M:MyCaffe.common.Net`1.SaveWeights(MyCaffe.common.IXPersist{`0},System.Boolean)">
            <summary>
            Save the weights to a byte array.
            </summary>
            <param name="persist">Specifies an interface to the persistance object used to save the weights.</param>
            <param name="bSaveDiff">Specifies to save the diff values.</param>
            <returns>The byte array containing the weights is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindLayerOwningBlob(MyCaffe.common.Blob{`0})">
            <summary>
            Finds the Layer that owns a given Blob.
            </summary>
            <param name="b">Specifies the Blob to search for.</param>
            <returns>If found, the Layer is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindLayerIndexOwningBlob(MyCaffe.common.Blob{`0})">
            <summary>
            Finds the index of the Layer that owns a given Blob.
            </summary>
            <param name="b">Specifies the Blob to search for.</param>
            <returns>If found, the Layer index is returned, otherwise -1 is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindBlob(System.String)">
            <summary>
            Finds a Blob in the Net by name.
            </summary>
            <param name="strName">Specifies the Blob name.</param>
            <returns>If found, the Blob is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindBottomBlobsOfLayer(System.String)">
            <summary>
            Returns the collection of bottom blobs for a given layer.
            </summary>
            <param name="strLayer">Specifies the layer name.</param>
            <returns>The collection of bottom blobs is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindTopBlobsOfLayer(System.String)">
            <summary>
            Returns the collection of top blobs for a given layer.
            </summary>
            <param name="strLayer">Specifies the layer name.</param>
            <returns>The collection of top blobs is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.GetDebugInformation(System.Boolean)">
            <summary>
            Returns the DebugInformation for the Net.
            </summary>
            <param name="bDetectNans">Specifies whether or not to detect Nan's in the data.</param>
            <returns>The DebugInformation for the Net is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.GetDataSource">
            <summary>
            Returns the data source used by the network.
            </summary>
            <returns>The data source name is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.ReInitializeParameters(System.String[])">
            <summary>
            Re-initializes the blobs and each of the specified layers by re-running the filler (if any) specified by the layer.  
            When the 'rgstr' parameter is <i>null</i> or otherwise empty, the blobs of all layers are re-initialized. 
            </summary>
            <param name="rgstrLayers">Specifies the layers to reinitialize, when <i>null</i> or empty, all layers are re-initialized</param>
            <returns>If a layer is specified and found, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindLayer(System.Nullable{MyCaffe.param.LayerParameter.LayerType},System.String)">
            <summary>
            Find the layer with the matching type, name and or both.
            </summary>
            <param name="type">Specifies the layer type.</param>
            <param name="strName">Specifies the layer name.</param>
            <returns>The layer found (if any) is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindLayer(System.String,System.String)">
            <summary>
            Find the layer with the matching type, name and or both.
            </summary>
            <param name="strType">Specifies the layer type.</param>
            <param name="strName">Specifies the layer name.</param>
            <returns>The layer found (if any) is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.Net`1.FindLastLayer(MyCaffe.param.LayerParameter.LayerType)">
            <summary>
            Find the last layer with the matching type.
            </summary>
            <param name="type">Specifies the layer type.</param>
            <returns>The layer found (if any) is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.Net`1.Cuda">
            <summary>
            Returns the instance of CudaDnn used by this network.
            </summary>
        </member>
        <member name="T:MyCaffe.common.PersistCaffe`1">
            <summary>
            The PersistCaffe class is used to load and save weight files in the .caffemodel format.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.#ctor(MyCaffe.basecode.Log,System.Boolean)">
            <summary>
            The PersistCaffe constructor.
            </summary>
            <param name="log">Specifies the log used for output.</param>
            <param name="bFailOnFirstTry">Specifies whether or not to try to load the weights file.  On the first try the Caffe model format is attempted, and on the second the MyCaffe format is used.</param>
        </member>
        <member name="P:MyCaffe.common.PersistCaffe`1.MyCaffeTag">
            <summary>
            This tag is used to mark the ending section of each weighting file with 
            'MyCaffe' specific information.
            </summary>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.IsMyCaffe(System.Byte[],System.String@)">
            <summary>
            This method returns whether or not the weights have been marked as 'mycaffe.ai'.
            </summary>
            <param name="rgWeights">Specifies the weights.</param>
            <param name="strVer">Returns the version of this file.</param>
            <returns>If the weights contain mycaffe weights, <i>true</i> is returned, false otherwise.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.SaveSolverState(MyCaffe.param.SolverState,MyCaffe.param.SolverParameter.SolverType)">
            <summary>
            Save the solver state to a byte array.
            </summary>
            <param name="state">Specifies the solver state to save.</param>
            <param name="type">Specifies the solver type.</param>
            <returns>A byte array containing the solver state is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadSolverState(System.Byte[],MyCaffe.param.SolverParameter.SolverType)">
            <summary>
            Load the solver state from a byte array.
            </summary>
            <param name="rgState">Specifies the byte array containing the solver state.</param>
            <param name="type">Specifies the solver type.</param>
            <returns>The SolverState loaded is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadWeights(System.Byte[],System.Collections.Generic.List{System.String},MyCaffe.common.BlobCollection{`0},System.Boolean,System.Boolean@,System.Collections.Generic.List{System.String},System.Collections.Generic.List{System.String},System.String)">
            <summary>
            Loads new weights into a BlobCollection
            </summary>
            <remarks>
            NOTE: In order to maintain compatibility with the C++ %Caffe, extra MyCaffe features may be added to the <i>end</i> of the weight file.  After saving weights (see SaveWeights) in the format
            used by the C++ %Caffe, MyCaffe writes the bytes "mycaffe.ai".  All information after these bytes are specific to MyCaffe and allow for loading weights for models by Blob name and shape
            and loosen the C++ %Caffe requirement that the 'number' of blobs match.  Adding this functionality allows for training model, changing the model structure, and then re-using the trained
            weights in the new model.  
            </remarks>
            <param name="rgWeights">Specifies the weights themselves.</param>
            <param name="rgExpectedShapes">Specifies a list of expected shapes for each Blob where the weights are to be loaded.</param>
            <param name="colBlobs">Specifies the Blobs to load with the weights.</param>
            <param name="bSizeToFit">Specifies wether or not the weights should be re-sized.  Note: resizing can render the weights useless, especially in deeper, layers.</param>
            <param name="bLoadedDiffs">Returns whether or not the diffs were loaded.</param>
            <param name="inputWtInfo">Optionally, specifies the weight info describing the input weight blobs to import by name.  Note when used the number of blobs must match the number of <i>targetWtInfo</i> blobs.  Otherwise, when <i>null</i> this parameter is ignored.</param>
            <param name="targetWtInfo">Optionally, specifies the weight info describing the target weight blobs to import by name.  Note when used the number of blobs must match the number of <i>inputWtInfo</i> blobs.  Otherwise, when <i>null</i> this parameter is ignored.</param>
            <param name="strSkipBlobType">Optionally, specifies a blob type where weights are NOT loaded.  See Blob.BLOB_TYPE for the types of Blobs.</param>
            <returns>The collection of Blobs with newly loaded weights is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadWeightInfo(System.Byte[])">
            <summary>
            Returns the weight information describing the weights containined within the weight bytes.
            </summary>
            <param name="rgWeights">Specifies the bytes containing the weights.</param>
            <returns>The weight information is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadWeightInfo(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Returns the weight information describing the weights containined within the Blob collection.
            </summary>
            <param name="colBlobs">Specifies the Blob collection containing the weights.</param>
            <returns>The weight information is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.SaveWeights(MyCaffe.common.BlobCollection{`0},System.Boolean)">
            <summary>
            Save the weights to a byte array.
            </summary>
            <remarks>
            NOTE: In order to maintain compatibility with the C++ %Caffe, extra MyCaffe features may be added to the <i>end</i> of the weight file.  After saving weights in the format
            used by the C++ %Caffe, MyCaffe writes the bytes "mycaffe.ai".  All information after these bytes are specific to MyCaffe and allow for loading weights for models by Blob name and shape
            and loosen the C++ %Caffe requirement that the 'number' of blobs match.  Adding this functionality allows for training model, changing the model structure, and then re-using the trained
            weights in the new model.  
            </remarks>
            <param name="colBlobs">Specifies the Blobs to save with the weights.</param>
            <param name="bSaveDiffs">Optionally, specifies to save the diff values - currently this parameter is not used.</param>
            <returns>The byte array containing the weights is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadBlobProto(System.Byte[],System.Int32)">
            <summary>
            The LoadBlobProto function loads a BlobProto from a proto buffer.
            </summary>
            <param name="rg">Specifies the bytes containing the BlobProto in proto buffer format.</param>
            <param name="nFieldId">Specifies the field ID to use for the BlobProto.</param>
            <returns>The new BlobProt is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PersistCaffe`1.LoadBlobProto(System.String,System.Int32)">
            <summary>
            The LoadBlobProto function loads a BlobProto from a file.
            </summary>
            <param name="strFile">Specifies the binary file containing the blob proto.</param>
            <param name="nFieldId">Specifies the field ID to use for the BlobProto.</param>
            <returns>The new BlobProt is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.PropertyTree">
            <summary>
            The PropertyTree class implements a simple property tree similar to the ptree in Boost.
            </summary>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.Put(System.String,System.String)">
            <summary>
            Add a new property string value.
            </summary>
            <param name="str">Specifies the key.</param>
            <param name="strVal">Specifies the string value.</param>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.Put(System.String,System.Double)">
            <summary>
            Add a new property numeric value.
            </summary>
            <param name="str">Specifies the key.</param>
            <param name="dfVal">Specifies the numeric value.</param>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.AddChild(System.String,MyCaffe.common.PropertyTree)">
            <summary>
            Add a new child to the Property tree.
            </summary>
            <param name="str">Specifies the key name of the child.</param>
            <param name="pt">Specifies the property child tree.</param>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.Get(System.String)">
            <summary>
            Retrieves a property at the current level of the tree.
            </summary>
            <param name="strName">Specifies the name of the property.</param>
            <returns>The property is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.GetChildren(System.String)">
            <summary>
            Retrieves all properties with the given key at the current level of the tree.
            </summary>
            <param name="strName">Specifies the name of the children.</param>
            <returns>The list of properties for the children are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.Clear">
            <summary>
            Clear all nodes and values from the tree.
            </summary>
        </member>
        <member name="P:MyCaffe.common.PropertyTree.Children">
            <summary>
            Returns a list of all child property trees within the tree.
            </summary>
        </member>
        <member name="M:MyCaffe.common.PropertyTree.ToJson">
            <summary>
            Converts the property tree to a Json representation.
            </summary>
            <remarks>
            THIS METHOD IS NOT COMPLETE YET.
            </remarks>
            <returns>The Json string representing the tree is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.Property">
            <summary>
            The Property class stores both a numeric and text value.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Property.#ctor(System.String,System.Nullable{System.Double})">
            <summary>
            The constructor.
            </summary>
            <param name="strVal">Specifies the text value.</param>
            <param name="dfVal">Optionally, specifies the numeric value.</param>
        </member>
        <member name="M:MyCaffe.common.Property.#ctor(System.Double,System.String)">
            <summary>
            The constructor.
            </summary>
            <param name="dfVal">Specifies the numeric value.</param>
            <param name="strVal">Optionally, specifies the text value.</param>
        </member>
        <member name="P:MyCaffe.common.Property.Value">
            <summary>
            Returns the text value.
            </summary>
        </member>
        <member name="P:MyCaffe.common.Property.Numeric">
            <summary>
            Returns the numeric value.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ResultCollection">
            <summary>
            The ResultCollection contains the result of a given CaffeControl::Run.
            </summary>
        </member>
        <member name="T:MyCaffe.common.ResultCollection.RESULT_TYPE">
            <summary>
            Defines the type of result.
            </summary>
        </member>
        <member name="F:MyCaffe.common.ResultCollection.RESULT_TYPE.NONE">
            <summary>
            Specifies that no result type for the data.
            </summary>
        </member>
        <member name="F:MyCaffe.common.ResultCollection.RESULT_TYPE.PROBABILITIES">
            <summary>
            Specifies that the results represent probabilities.
            </summary>
        </member>
        <member name="F:MyCaffe.common.ResultCollection.RESULT_TYPE.DISTANCES">
            <summary>
            Specifies that the results represent distances.
            </summary>
        </member>
        <member name="F:MyCaffe.common.ResultCollection.RESULT_TYPE.MULTIBOX">
            <summary>
            Specifies that the results represent multibox results.
            </summary>
        </member>
        <member name="M:MyCaffe.common.ResultCollection.#ctor(System.Collections.Generic.List{MyCaffe.basecode.Result},MyCaffe.param.LayerParameter.LayerType)">
            <summary>
            The ResultCollection constructor.
            </summary>
            <param name="rgResults">Specifies the results.</param>
            <param name="outputLayerType">Specifies the output layer type.</param>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.ResultType">
            <summary>
            Returns the result type of the result data: PROBABILITIES (Sigmoid), DISTANCES (Decode), or NONE (Unknown).
            </summary>
        </member>
        <member name="M:MyCaffe.common.ResultCollection.GetEncoding">
            <summary>
            Returns the data encoding values.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.ResultsOriginal">
            <summary>
            Returns the original results.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.ResultsSorted">
            <summary>
            Returns the original results in sorted order.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabelMaxSignal">
            <summary>
            Returns the detected label with the maximum signal.
            </summary>
            <remarks>
            The maximum signal label is used to detect the output from a SoftMax where each 
            label is given a probability and the label with the highest probability is the
            detected label.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabelOutputMaxSignal">
            <summary>
            Returns the detected label output with the maximum signal.
            </summary>
            <remarks>
            The maximum signal label is used to detect the output from a SoftMax where each 
            label is given a probability and the label with the highest probability is the
            detected label.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabelMinSignal">
            <summary>
            Returns the detected label with the minimum signal.
            </summary>
            <remarks>
            The minimum signal label is used to detect the output from a Decode alyer where each 
            label is given the distance from which the data's encoding is from the centroid of the
            label - the encoding with the minimum distance signifies the detected label.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabelOutputMinSignal">
            <summary>
            Returns the detected label output of the label with the minimum signal.
            </summary>
            <remarks>
            The minimum signal label is used to detect the output from a Decode alyer where each 
            label is given the distance from which the data's encoding is from the centroid of the
            label - the encoding with the minimum distance signifies the detected label.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabel">
            <summary>
            Returns the detected label depending on the result type (distance or probability) with a default type of probability (max label signal) used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.DetectedLabelOutput">
            <summary>
            Returns the detected label output depending on the result type (distance or probability) with a default type of probability (max label signal) used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.ResultCollection.Labels">
            <summary>
            Returns the dictionary lookup of the labels and their names.
            </summary>
        </member>
        <member name="M:MyCaffe.common.ResultCollection.SetLabels(System.Collections.Generic.List{MyCaffe.basecode.descriptors.LabelDescriptor})">
            <summary>
            Sets the label names in the label dictionary lookup.
            </summary>
            <param name="rgLabels"></param>
        </member>
        <member name="M:MyCaffe.common.ResultCollection.ToString">
            <summary>
            Returns a string representation of the results.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.common.ResultCollection.ToImage(MyCaffe.basecode.ColorMapper)">
            <summary>
            Converts the result collection into an image.
            </summary>
            <param name="clrMap">Optionally, specifies a colormap to use.</param>
            <returns>The image respresentation of the result collection is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.SsdSampler`1">
            <summary>
            The SsdSampler is used by the SSD algorithm to sample BBoxes.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log)">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.Dispose">
            <summary>
            Free all resources used.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.GroupObjectBBoxes(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Find all annotated NormalizedBBox.
            </summary>
            <param name="anno_datum">Specifies the annotated datum.</param>
            <returns>The grouped object BBoxes are returned.</returns>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.SatisfySampleConstraint(MyCaffe.basecode.NormalizedBBox,System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},MyCaffe.param.ssd.SamplerConstraint)">
            <summary>
            Check if the sampled bbox satisfies the constraints with all object bboxes.
            </summary>
            <param name="sampledBBox">Specifies the sampled BBox.</param>
            <param name="rgObjectBboxes">Specifies the list of object normalized BBoxes.</param>
            <param name="sampleConstraint">Specifies the sample constraint.</param>
            <returns>Returns whether or not the sample constraints are satisfied.</returns>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.GenerateSamples(MyCaffe.basecode.NormalizedBBox,System.Collections.Generic.List{MyCaffe.basecode.NormalizedBBox},MyCaffe.param.ssd.BatchSampler)">
            <summary>
            Generate samples from the NormalizedBBox using the BatchSampler.
            </summary>
            <param name="sourceBBox">Specifies the source BBox.</param>
            <param name="rgObjectBboxes">Specifies the object normalized BBoxes.</param>
            <param name="batchSampler">Specifies the batch sampler.</param>
            <returns>The list of normalized BBoxes generated is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.SsdSampler`1.GenerateBatchSamples(MyCaffe.basecode.SimpleDatum,System.Collections.Generic.List{MyCaffe.param.ssd.BatchSampler})">
            <summary>
            Generate samples from the annotated Datum using the list of BatchSamplers.
            </summary>
            <param name="anno_datum"></param>
            <param name="rgBatchSamplers"></param>
            <returns>All samples bboxes that satisfy the constraints defined in the BatchSampler are returned.</returns>
        </member>
        <member name="T:MyCaffe.common.SyncedMemory`1">
            <summary>
            The SyncedMemory manages the low-level connection between the GPU and host memory.
            </summary>
            <remarks>
            The GPU memory is represented by a handle into the memory look-up table managed by the low-level CudaDnn DLL.  The host memory
            is copied and stored in a local array of type 'T'.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Int64,System.Object,System.Boolean)">
            <summary>
            The SyncedMemory constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="lCapacity">Optionally, specifies the capacity of the SyncedMemory (in items).</param>
            <param name="tag">Optionally, specifies a tag used for debugging (the default = <i>null</i>).</param>
            <param name="bUseHalfSize">Optionally, specifies to use half size (FP16) for both data and diff.  This option is only available when using the <i>float</i> base type 'T'.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Dispose">
            <summary>
            Releases all GPU and host resources used.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Allocate(System.Int64,System.Boolean)">
            <summary>
            Allocate a number of items in GPU memory and save the handle.
            </summary>
            <param name="lCount">Specifies the number of items.</param>
            <param name="bUseHalfSize">Optionally, specifis to use half sized memory (default = false).  This only applies to the 'float' base type.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Allocate(`0[],System.Boolean)">
            <summary>
            Allocate a number of items and copy the given array into the memory on the GPU.
            </summary>
            <param name="rg">Specifies the array of items to copy.</param>
            <param name="bUseHalfSize">Optionally, specifis to use half sized memory (default = false).  This only applies to the 'float' base type.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Zero">
            <summary>
            Set all items in the GPU memory up to the Count, to zero.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.ZeroAll">
            <summary>
            Set all items in the GPU memory up to the Capacity, to zero.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Set(System.Double)">
            <summary>
            Set all items up to Count to a given value.
            </summary>
            <param name="dfVal">Specifies the value.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.SetAt(System.Int32,`0)">
            <summary>
            Set a specific item at a given index to a value.
            </summary>
            <param name="nIdx">Specifies the index.</param>
            <param name="fVal">Specifies the value.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.GetAt(System.Int32)">
            <summary>
            Return a value at a given index.
            </summary>
            <param name="nIdx">Specifies the index.</param>
            <returns>The value at the index is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Copy(MyCaffe.common.SyncedMemory{`0},System.Int64)">
            <summary>
            Copy another SyncedMemory into this one.
            </summary>
            <param name="src">Specifies the SyncedMemory to copy.</param>
            <param name="hDstHostBuffer">Optionally, specifies a host buffer used to copy between kernels (default = 0, not used).</param>
            <returns>When used the dst host buffer handle is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.Clone">
            <summary>
            Copy this SyncedMemory.
            </summary>
            <returns>A new SynedMemory that is a copy of this one, is returned.</returns>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.HalfSize">
            <summary>
            Returns whether or not the sync memory is half-sized memory.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.Tag">
            <summary>
            Get/set data associated with the synced memory.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.DeviceID">
            <summary>
            Returns the Device ID on which the GPU memory of this SyncedMemory was allocated.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.Capacity">
            <summary>
            Returns the total amount of GPU memory held by this SyncedMemory.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.Count">
            <summary>
            Returns the current count of items in this SyncedMemory.  Note, the Count may be less than the Capacity.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.gpu_data">
            <summary>
            Returns the handle to the GPU memory.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.set_gpu_data(System.Int64,System.Int64,System.Int64)">
            <summary>
            Copies a new Memory Pointer within the low-level CudaDnnDLL where a Memory Pointer 
            uses another already allocated block of GPU memory and just indexes into it.
            </summary>
            <param name="hData">Specifies a handle to the already allocated GPU memory that the new Memory Pointer will index into.</param>
            <param name="lCount">Specifies the number of items in this 'virtual memory'</param>
            <param name="lOffset">Specifies the offset into the GPU data where the Memory Pointer should start.</param>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.mutable_gpu_data">
            <summary>
            Returns the mutable handle to GPU data.
            </summary>
            <remarks>
            Note: This is the same as gpu_data, but is provided for compatibility and readability with the original C++ %Caffe code.
            </remarks>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.cpu_data">
            <summary>
            Returns the data on the CPU that has already been transferred from GPU to CPU.
            </summary>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.SetData(`0[],System.Int32,System.Boolean)">
            <summary>
            Sets the array of host data on the GPU and re-allocates the GPU memory if needed.
            </summary>
            <param name="rgData">Specifies the host data to set.</param>
            <param name="nCount">Specifies the number of items in the host data to set, which may be less than the host data array length.</param>
            <param name="bSetCount">Optionally, specifies whether or not to set the count.  The count is always set when re-allocating the buffer.</param>
        </member>
        <member name="P:MyCaffe.common.SyncedMemory`1.mutable_cpu_data">
            <summary>
            Get/set the mutable host data.
            </summary>
            <remarks>
            When setting the mutable host data, the data is copied to the GPU.  When get'ing the host data, the data is
            transferred from the GPU first and then returned.
            </remarks>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.update_cpu_data(System.Int64)">
            <summary>
            Updates the host data by copying the GPU data to the host data.
            </summary>
            <param name="lCount">Optionally, specifies a count (less than Count) to transfer.</param>
            <returns>An array of the host data is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.set_cpu_data_locally(`0[])">
            <summary>
            This does not place the data on the GPU - call async_gpu_push() to move it to the GPU.
            </summary>
            <param name="rg">Specifies an array of host data.</param>
        </member>
        <member name="M:MyCaffe.common.SyncedMemory`1.async_gpu_push(System.Int64,`0[])">
            <summary>
            Pushes the host data, previously set with set_cpu_data_locally(), to the GPU.
            </summary>
            <remarks>
            Note, if necessary, this function re-allocates the GPU memory.
            </remarks>
            <param name="hStream"></param>
            <param name="rg"></param>
        </member>
        <member name="T:MyCaffe.common.TransferInput">
            <summary>
            The TransferInput class is used to transfer get and set input data.  
            </summary>
        </member>
        <member name="T:MyCaffe.common.TransferInput.fnSetInputData">
            <summary>
            This delegate is used to set input data.
            </summary>
            <param name="bi">Specifies the batch input data.</param>
        </member>
        <member name="T:MyCaffe.common.TransferInput.fnGetInputData">
            <summary>
            This delegate is used to get input data.
            </summary>
            <returns>The BatchInput previously set is returned.</returns>
        </member>
        <member name="M:MyCaffe.common.TransferInput.#ctor(MyCaffe.common.TransferInput.fnGetInputData,MyCaffe.common.TransferInput.fnSetInputData)">
            <summary>
            The TransferInput constructor.
            </summary>
            <param name="getInput">Specifies the delegate to get the input.</param>
            <param name="setInput">Specifies the delegate to set the input.</param>
        </member>
        <member name="P:MyCaffe.common.TransferInput.Get">
            <summary>
            Returns the delegate used to get the batch input.
            </summary>
        </member>
        <member name="P:MyCaffe.common.TransferInput.Set">
            <summary>
            Returns the delegate used to set the batch input.
            </summary>
        </member>
        <member name="T:MyCaffe.common.WeightInfo`1">
            <summary>
            The WeightInfo class describes the weights of a given weight set including
            the blob names and sizes of the weights.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.WeightInfo`1.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.common.WeightInfo`1.AddBlob(System.String,System.Collections.Generic.List{System.Int32},MyCaffe.common.BLOB_TYPE)">
            <summary>
            Add a blob name and shape to the WeightInfo.
            </summary>
            <param name="strName">Specifies the Blob name.</param>
            <param name="rgShape">Specifies the Blob shape.</param>
            <param name="type">Specifies the Blob type.</param>
        </member>
        <member name="M:MyCaffe.common.WeightInfo`1.AddBlob(MyCaffe.common.Blob{`0})">
            <summary>
            Add a blob name and shape to the WeightInfo.
            </summary>
            <param name="b">Specifies the Blob who's name and shape are to be added.</param>
        </member>
        <member name="P:MyCaffe.common.WeightInfo`1.Blobs">
            <summary>
            Returns the list of blob information describing the weights.  Each entry within the Dictionary returned contains
            the Blob's name and the Blob's dimensions (e.g. {num, channels, height, width}) as a List of integers.
            </summary>
        </member>
        <member name="T:MyCaffe.common.BlobName">
            <summary>
            The BlobName class is used to build unique blob names.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlobName.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.common.BlobName.GetName(System.String)">
            <summary>
            Returns a unique blob name.
            </summary>
            <param name="strName">Specifies the original name of the blob.</param>
            <returns>A unique blob name is returned.</returns>
        </member>
        <member name="T:MyCaffe.common.Params`1">
            <summary>
            The Params contains the base parameters used in multi-GPU training.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.common.Params`1.m_lCount">
            <summary>
            size of the buffers (in items).
            </summary>
        </member>
        <member name="F:MyCaffe.common.Params`1.m_lExtra">
            <summary>
            size of the padding added to the memory buffers.
            </summary>
        </member>
        <member name="F:MyCaffe.common.Params`1.m_hData">
            <summary>
            Handle to GPU memory containing the Net parameters.
            </summary>
        </member>
        <member name="F:MyCaffe.common.Params`1.m_hDiff">
            <summary>
            Handle to GPU memory containing the Net gradient.
            </summary>
        </member>
        <member name="F:MyCaffe.common.Params`1.m_nDeviceID">
            <summary>
            The Device ID.
            </summary>
        </member>
        <member name="M:MyCaffe.common.Params`1.#ctor(MyCaffe.solvers.Solver{`0})">
            <summary>
            The Param constructor.
            </summary>
            <param name="root_solver">Specifies the root Solver.</param>
        </member>
        <member name="P:MyCaffe.common.Params`1.count">
            <summary>
            Returns the size of the buffers (in items).
            </summary>
        </member>
        <member name="P:MyCaffe.common.Params`1.data">
            <summary>
            Returns the handle to the GPU memory containing the Net parameters. 
            </summary>
        </member>
        <member name="P:MyCaffe.common.Params`1.diff">
            <summary>
            Returns the handle to the GPU memory containing the Net gradients. 
            </summary>
        </member>
        <member name="T:MyCaffe.common.GPUParams`1">
            <summary>
            The GPUParams contains the connection to the low-level Cuda, and the stream associated with this instance.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.m_cuda">
            <summary>
            The instance of CudaDnn that provides the connection to Cuda.
            </summary>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.m_log">
            <summary>
            The Log used for output.
            </summary>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.m_hStream">
            <summary>
            The handle to the Cuda stream used for synchronization.
            </summary>
        </member>
        <member name="T:MyCaffe.common.GPUParams`1.Op">
            <summary>
            Defines the memory operation to perform. 
            </summary>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.Op.copy">
            <summary>
            Copy over the buffer.
            </summary>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.Op.replace_gpu">
            <summary>
            Replace the GPU portion of the data buffer.
            </summary>
        </member>
        <member name="F:MyCaffe.common.GPUParams`1.Op.replace_gpu_diff">
            <summary>
            Replace the GPU protion of the diff buffer.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GPUParams`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.solvers.Solver{`0},System.Int32)">
            <summary>
            The GPUParams constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="root_solver">Specifies the root Solver.</param>
            <param name="nDeviceID">Specifies the device ID to use for this instance.</param>
        </member>
        <member name="M:MyCaffe.common.GPUParams`1.Dispose">
            <summary>
            Release all GPU and Host resources used.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GPUParams`1.SynchronizeStream">
            <summary>
            Synchronize with the Cuda stream.
            </summary>
        </member>
        <member name="M:MyCaffe.common.GPUParams`1.Configure(MyCaffe.solvers.Solver{`0})">
            <summary>
            Configure the GPU Params by copying the Solver training Net parameters into the data and diff buffers.
            </summary>
            <param name="solver"></param>
        </member>
        <member name="M:MyCaffe.common.GPUParams`1.apply_buffers(MyCaffe.common.BlobCollection{`0},System.Int64,System.Int64,MyCaffe.common.GPUParams{`0}.Op)">
            <summary>
            Transfer between the data/diff buffers and a collection of Blobs (e.g. the learnable parameters).
            </summary>
            <param name="rgBlobs">Specifies the collection of Blobs to transfer data with.</param>
            <param name="hBuffer">Specifies a handle to the memory on the GPU to transfer with the Blob collection.</param>
            <param name="lTotalSize">Specifies the number of items to transfer.</param>
            <param name="op">Specifies the type of transfer to perform.</param>
        </member>
        <member name="T:MyCaffe.common.NCCL`1">
            <summary>
            The NCCL class manages the multi-GPU operations using the low-level NCCL functionality provided by the low-level Cuda Dnn DLL.
            </summary>
            <remarks>
            [NVIDIA's NCCL 'Nickel'](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/) is an NVIDIA library designed to 
            optimize communication between multiple GPUs.
            
            When using multi-GPU training, it is highly recommended to only train on TCC enabled drivers, otherwise driver timeouts may occur on large models.
            @see [NVIDIA Tesla Compute Cluster (TCC) Help](http://docs.nvidia.com/gameworks/content/developertools/desktop/tesla_compute_cluster.htm)
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.NCCL`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.solvers.Solver{`0},System.Int32,System.Int64,System.Collections.Generic.List{System.Threading.ManualResetEvent})">
            <summary>
            The NCCL constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="root_solver">Specifies the root Solver.</param>
            <param name="nDeviceID">Specifies the device ID to use for this instance.</param>
            <param name="hNccl">Specifies the handle to NCCL created using CudaDnn::CreateNCCL, or 0 for the root solver as this is set-up in NCCL::Run.</param>
            <param name="rgGradientReadyEvents">Specifies a list of events used to synchronize with the other Solvers.</param>
        </member>
        <member name="M:MyCaffe.common.NCCL`1.Dispose">
            <summary>
            Release all GPU and Host resources used.
            </summary>
        </member>
        <member name="M:MyCaffe.common.NCCL`1.Broadcast">
            <summary>
            Broadcast the data to all other solvers participating in the multi-GPU session.
            </summary>
        </member>
        <member name="M:MyCaffe.common.NCCL`1.Run(System.Collections.Generic.List{System.Int32},System.Int32)">
            <summary>
            Run the root Solver and coordinate with all other Solver's participating in the multi-GPU training.
            </summary>
            <remarks><b>IMPORTANT</b>: When performing multi-GPU training only GPU's that DO NOT have a monitor connected can be used.  Using the GPU
            with the monitor connected will cause an error.  However, you can use GPU's that are configured in either TCC mode or WDM mode, BUT,
            all GPU's participating must be configured to use the same mode, otherwise you may experience upredictable behavior, the NVIDIA driver
            may crash, or you may experience the infamouse BSOD ("Blue Screen of Death") - so as the saying goes, "Beware to all who enter here..."</remarks>
            <param name="rgGpus">Specifies all GPU ID's to use.</param>
            <param name="nIterationOverride">Optionally, specifies a training iteration override to use.</param>
        </member>
        <member name="T:MyCaffe.common.Worker`1">
            <summary>
            The Worker manages each 'non' root sover running, where each Worker operates on a different GPU.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.Worker`1.#ctor">
            <summary>
            The Worker constructor.
            </summary>
        </member>
        <member name="T:MyCaffe.common.SolverInfo`1">
            <summary>
            The SolverInfo defines the user supplied arguments passed to each Worker.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.common.SolverInfo`1.#ctor(MyCaffe.solvers.Solver{`0},System.Int64,System.Int64,System.Int32,System.Int32,System.String,System.Collections.Generic.List{System.Threading.ManualResetEvent},System.Threading.ManualResetEvent)">
            <summary>
            The SolverInfo constructor.
            </summary>
            <param name="rank0">Specifies rank Solver that will run in the Worker.</param>
            <param name="hSrcKernel">Specifies a handle to the kernel where the NCCL for this Solver was created (typically this is the kernel that also created the root Solver).</param>
            <param name="hSrcNccl">Specifies the handle to the NCCL instance for this Solver (typically this is created on the kernel that also created the root Solver, and must be transferred to the kernel of the CudaDnn instance running in the Worker).</param>
            <param name="nSolverRank">Specifies the rank of this Solver.</param>
            <param name="nIterationOverride">Specifies the training iteration override to use.</param>
            <param name="strCudaPath">Specifies the file path to the low-level CudaDnnDll.DLL file to use.  Note, when <i>null</i> or emtpy, the path of the executing Assembly is used.</param>
            <param name="rgGradientReadyEvents">Specifies the list of events used to coordinate with other Solvers.</param>
            <param name="evtAllCreated">Specifies an event used to coordinate the creation of all participating Workers.</param>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.Rank0">
            <summary>
            Returns rank Solver that will run in the Worker.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.CudaPath">
            <summary>
            Returns the file path to the low-level CudaDnnDll.DLL file to use.  Note, when <i>null</i> or emtpy, the path of the executing Assembly is used.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.IterationOverride">
            <summary>
            Returns the training iteration override to use.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.KernelHandle">
            <summary>
            Returns a handle to the kernel where the NCCL for this Solver was created (typically this is the kernel that also created the root Solver)
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.NcclHandle">
            <summary>
            Returns the handle to the NCCL instance for this Solver (typically this is created on the kernel that also created the root Solver, and must be transferred to the kernel of the CudaDnn instance running in the Worker).
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.SolverRank">
            <summary>
            Returns the rank of this Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.InitializedEvent">
            <summary>
            Returns the event that is set after the Worker has completed initializing.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.StartedEvent">
            <summary>
            Returns the event that is set after the Worker has started running.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.AllCreatedEvent">
            <summary>
            Returns the event that is set after all Workers have been created.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.GradientReadyEvents">
            <summary>
            Returns the event that is set after the gradients of the Solver in this Worker are ready.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.Error">
            <summary>
            Returns the error (if any) that occured when running the solver thread.
            </summary>
        </member>
        <member name="P:MyCaffe.common.SolverInfo`1.ErrorEvent">
            <summary>
            Returns the event that is set when an error occurs.
            </summary>
        </member>
        <member name="T:MyCaffe.data.DataTransformer`1">
            <summary>
            Applies common transformations to the input data, such as
            scaling, mirroring, subtracting the image mean...
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.TransformationParameter,MyCaffe.basecode.Phase,System.Int32,System.Int32,System.Int32,MyCaffe.basecode.SimpleDatum)">
            <summary>
            The DataTransformer constructor.
            </summary>
            <param name="cuda">Specifies the connection to the CudaDnn dll which is only needed when using the bbox or image transformation functionality.</param>
            <param name="log">Specifies the Log used for output.</param>
            <param name="p">Specifies the TransformationParameter used to create the DataTransformer.</param>
            <param name="phase">Specifies the Phase under which the DataTransformer is run.</param>
            <param name="nC">Specifies the channels.</param>
            <param name="nH">Specifies the height.</param>
            <param name="nW">Specifies the width.</param>
            <param name="imgMean">Optionally, specifies the image mean to use.</param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Update(System.Int32,MyCaffe.basecode.SimpleDatum)">
            <summary>
            Resync the transformer with changes in its parameter.
            </summary>
        </member>
        <member name="P:MyCaffe.data.DataTransformer`1.param">
            <summary>
            Returns the TransformationParameter used.
            </summary>
        </member>
        <member name="P:MyCaffe.data.DataTransformer`1.ImageMean">
            <summary>
            Get/set the image mean.
            </summary>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.InferBlobShape(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Infers the shape the transformed blob will have when 
            the transformation is applied to the data.
            </summary>
            <param name="d">Data containing the data to be transformed.</param>
            <returns>The inferred shape.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.InferBlobShape(MyCaffe.basecode.SimpleDatum,System.Int32[])">
            <summary>
            Infers the shape the transformed blob will have when 
            the transformation is applied to the data.
            </summary>
            <param name="d">Data containing the data to be transformed.</param>
            <param name="rgShape">Specifies the shape vector to fill.</param>
            <returns>The inferred shape.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.InferBlobShape(System.Collections.Generic.List{MyCaffe.basecode.Datum},System.Int32[])">
            <summary>
            Infers the shape the transformed blob will have when 
            the transformation is applied to the data.
            </summary>
            <param name="rgD">A list of data containing the data to be transformed.</param>
            <param name="rgShape">Specifies the shape vector.</param>
            <returns>The inferred shape.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.InferBlobShape(System.Int32,System.Int32,System.Int32)">
            <summary>
            Infers the shape of the transformed blow will have with the given channel, width and height.
            </summary>
            <param name="nChannels">Specifies the channels.</param>
            <param name="nWidth">Specifies the width.</param>
            <param name="nHeight">Specifies the height.</param>
            <returns>The inferred blob shape is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.InitRand">
            <summary>
            Initialize the underlying random number generator.
            </summary>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Rand(System.Int32)">
            <summary>
            Generates a random integer from Uniform({0, 1, ..., n-1}).
            </summary>
            <param name="n">The upper bound (exclusive) value of the random number.</param>
            <returns>A uniformly random integer value from ({0, 1, ..., n-1}).</returns>
        </member>
        <member name="P:MyCaffe.data.DataTransformer`1.LastRange">
            <summary>
            Returns the last min/max observed.
            </summary>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.TransformLabel(MyCaffe.basecode.SimpleDatum)">
            <summary>
            When active (label_mapping.Active = true), transforms the label if mapped using the label and boost.  Otherwise
            if not active or not mapped, no label changes are made.
            </summary>
            <param name="sd">Specifies the SimpleDatum whos label is to be transformed.</param>
            <returns>The new label is returned - this value is also set as the new label for the SimpleDatum.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(System.Collections.Generic.List{MyCaffe.basecode.Datum},MyCaffe.common.Blob{`0},MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log)">
            <summary>
            Transforms a list of Datum and places the transformed data into a Blob.
            </summary>
            <param name="rgDatum">Specifies a List of Datum to be transformed.</param>
            <param name="blobTransformed">Specifies the Blob where all transformed data is placed.</param>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies a Log for all output.</param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(System.Collections.Generic.List{MyCaffe.basecode.SimpleDatum},MyCaffe.common.Blob{`0},MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Boolean)">
            <summary>
            Transforms a list of Datum and places the transformed data into a Blob.
            </summary>
            <param name="rgDatum">Specifies a List of SimpleDatum to be transformed.</param>
            <param name="blobTransformed">Specifies the Blob where all transformed data is placed.</param>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies a Log for all output.</param>
            <param name="bJustFill">Optionally, specifies to just fill the data blob with the data without actually transforming it.</param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum[],MyCaffe.common.Blob{`0},MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,System.Boolean)">
            <summary>
            Transforms a list of Datum and places the transformed data into a Blob.
            </summary>
            <param name="rgDatum">Specifies a Array of SimpleDatum to be transformed.</param>
            <param name="blobTransformed">Specifies the Blob where all transformed data is placed.</param>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies a Log for all output.</param>
            <param name="bJustFill">Optionally, specifies to just fill the data blob with the data without actually transforming it.</param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum,MyCaffe.common.Blob{`0})">
            <summary>
            Transforms a Datum and places the dat ainto a Blob.
            </summary>
            <param name="d">Specifies the Datum to transform.</param>
            <param name="blob">Specifies the Blob where the transformed data is placed.</param>
            <returns>When a datum contains annotations, the tranformed annotation groups are returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum,MyCaffe.common.Blob{`0},System.Boolean@)">
            <summary>
            Transforms a Datum and places the dat ainto a Blob.
            </summary>
            <param name="d">Specifies the Datum to transform.</param>
            <param name="blob">Specifies the Blob where the transformed data is placed.</param>
            <param name="bDoMirror">Returns whether or not a mirror took place.</param>
            <returns>When a datum contains annotations, the tranformed annotation groups are returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum,System.Boolean@,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Transform the data into an array of transformed values.
            </summary>
            <param name="d">Data to transform.</param>
            <param name="bMirror">Returns whether or not a mirror occurred.</param>
            <param name="crop_bbox">Optionally, specifies a crop bbox to fill out.</param>
            <returns>Transformed data.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Transform the data into an array of transformed values.
            </summary>
            <param name="d">Data to transform.</param>
            <returns>Transformed data.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.Transform(MyCaffe.basecode.SimpleDatum,System.Collections.Generic.List{MyCaffe.basecode.AnnotationGroup}@,System.Boolean@,System.Boolean)">
            <summary>
            Transform the data into an array of transformed values.
            </summary>
            <param name="d">Data to transform.</param>
            <param name="rgTransformedAnnoVec">Returns the list of transfomed annoations.</param>
            <param name="bMirror">Returns whether or not a mirror occurred.</param>
            <param name="bResize">Specifies to resize the data.</param>
            <returns>Transformed data.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.TransformAnnotation(MyCaffe.basecode.SimpleDatum,MyCaffe.basecode.NormalizedBBox,System.Boolean,System.Boolean)">
            <summary>
            Transform the annotation data.
            </summary>
            <param name="d">Data to transform.</param>
            <param name="crop_bbox">Specifies the crop_bbox defined for the data.</param>
            <param name="bMirror">Specifies to mirror the data.</param>
            <param name="bResize">Specifies to resize the data.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.SetRange(MyCaffe.common.Blob{`0})">
            <summary>
            Scales the data of a Blob to fit in a given  range based on the DataTransformers parameters.
            </summary>
            <param name="b"></param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.CropImage(MyCaffe.basecode.SimpleDatum,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Crop the SimpleDatum according to the bbox.
            </summary>
            <param name="d">Specifies the SimpleDatum to crop.</param>
            <param name="bbox">Specifies the bounding box.</param>
            <returns>The cropped SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.ExpandImage(MyCaffe.basecode.SimpleDatum,MyCaffe.basecode.NormalizedBBox,System.Single)">
            <summary>
            Expand the SimpleDatum according to the bbox.
            </summary>
            <param name="d">Specifies the SimpleDatum to expand.</param>
            <param name="expand_bbox">Specifies the bounding box.</param>
            <param name="fExpandRatio">Specifies the expansion ratio.</param>
            <returns>The expanded SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.ExpandImage(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Expand the datum and adjust the AnnotationGroup.
            </summary>
            <param name="d">Specifies the datum to expand.</param>
            <returns>The newly expanded datum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.DistortImage(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Distort the SimpleDatum.
            </summary>
            <param name="d">Specifies the SimpleDatum to distort.</param>
            <remarks>Note this function only applies when the distortion parameter 'use_gpu' = false, otherwise the
            distoration is applied after the data is transferred to the GPU.</remarks>
            <returns>The distorted SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.DistortImage(MyCaffe.common.Blob{`0})">
            <summary>
            Distort the images within a Blob.
            </summary>
            <param name="b">Specifies the Blob to distort.</param>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.MaskImage(MyCaffe.basecode.SimpleDatum)">
            <summary>
            Maks out portions of the SimpleDatum.
            </summary>
            <param name="d">Specifies the SimpleDatum to mask.</param>
            <returns>The masked SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.MaskData(System.Int32[],System.Single[])">
            <summary>
            Mask out the data based on the shape of the specified SimpleDatum.
            </summary>
            <param name="rgShape">Specifies the shape of the data.</param>
            <param name="rgData">Specifies the data.</param>
            <returns>The newly masked data is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.DataTransformer`1.UnTransform(MyCaffe.common.Blob{`0},System.Boolean)">
            <summary>
            Reverse the transformation made when calling Transform.
            </summary>
            <param name="blob">Specifies the input blob.</param>
            <param name="bIncludeMean">Specifies whether or not to add the mean back.</param>
            <returns>The de-processed output Datum is returned.</returns>
        </member>
        <member name="T:MyCaffe.data.DB`1">
            <summary>
            A generic database class used to connect to the underlying database and create a Cursor that traverses through it.
            </summary>
        </member>
        <member name="M:MyCaffe.data.DB`1.#ctor(MyCaffe.basecode.IXImageDatabaseBase)">
            <summary>
            The DB Constructor.
            </summary>
            <param name="imgDb">The underlying database - the CaffeImageDatabase.</param>
        </member>
        <member name="M:MyCaffe.data.DB`1.Open(System.String)">
            <summary>
            Opens the underlying database with a given data source.
            </summary>
            <param name="strSrc">Specifies the data source name.</param>
        </member>
        <member name="M:MyCaffe.data.DB`1.Close">
            <summary>
            Closes the last Open session.
            </summary>
        </member>
        <member name="M:MyCaffe.data.DB`1.NewCursor(MyCaffe.data.DataTransformer{`0},MyCaffe.basecode.Log)">
            <summary>
            Creates and returns a new Cursor used to traverse through a data source within the database.
            </summary>
            <param name="transformer">Specifies the data transformer used to transform the labels (when label mapping is active).</param>
            <param name="log">Optionally, specifies the output log for diagnostic information (default = null).</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.data.Cursor`1">
            <summary>
            The Cursor is used to traverse through a given data source within the database.
            </summary>
        </member>
        <member name="M:MyCaffe.data.Cursor`1.#ctor(MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.data.DataTransformer{`0},System.String,MyCaffe.basecode.Log)">
            <summary>
            The Cursor constructor.
            </summary>
            <param name="db">Specifies the underlying database.</param>
            <param name="transformer">Specifies the data transformer used to transform the lables (when active).</param>
            <param name="strSrc">Specifies the name of the data source to use.</param>
            <param name="log">Optionally, specifies an output log used for diagnostic information if specified (default = null).</param>
        </member>
        <member name="M:MyCaffe.data.Cursor`1.Reset">
            <summary>
            Resets the current index bact to the start.
            </summary>
        </member>
        <member name="M:MyCaffe.data.Cursor`1.Next">
            <summary>
            Traverses to the next item within the data source.
            </summary>
        </member>
        <member name="P:MyCaffe.data.Cursor`1.Count">
            <summary>
            Returns the number of items traversed.
            </summary>
        </member>
        <member name="P:MyCaffe.data.Cursor`1.IsValid">
            <summary>
            Queryies to see if we are still within the bounds of the data source, if so <i>true</i> is returned, 
            otherwise if the Cursor it as the end of the data source, <i>false</i> is returned.
            </summary>
        </member>
        <member name="M:MyCaffe.data.Cursor`1.SeekToFirst">
            <summary>
            Move the cursor to the beginning of the data source.
            </summary>
        </member>
        <member name="M:MyCaffe.data.Cursor`1.GetValue(System.Nullable{System.Int32},System.Boolean,System.Nullable{MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD})">
            <summary>
            Retrieve the Datum at the current cursor location within the data source.
            </summary>
            <param name="nLabel">Optionally, specifies a label for which the cursor should query from.</param>
            <param name="bLoadDataCriteria">Specifies whether or not to load the data criteria.</param>
            <param name="imgSel">Optionally, specifies the image selection method (default = null).</param>
            <returns>The Datum retrieved is returned.</returns>
        </member>
        <member name="T:MyCaffe.data.ImageTransforms`1">
            <summary>
            The ImageTransforms class provides several useful image transformation function used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.basecode.CryptoRandom)">
            <summary>
            The constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance used to communidate with Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="random">Specifies the random number generator.</param>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.UpdateBBoxByResizePolicy(MyCaffe.param.ssd.ResizeParameter,System.Int32,System.Int32,MyCaffe.basecode.NormalizedBBox)">
            <summary>
            Update the BBox size based on the Resize policy.
            </summary>
            <param name="p">Specifies the ResizeParameter with the resize policy.</param>
            <param name="nOldWidth">Specifies the old width.</param>
            <param name="nOldHeight">Specifies the old height.</param>
            <param name="bbox1">Specifies the BBox to update.</param>
            <returns>The update NormalizedBBox is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.InferNewSize(MyCaffe.param.ssd.ResizeParameter,System.Int32,System.Int32,System.Int32@,System.Int32@)">
            <summary>
            Infer the new shape based on the resize policy.
            </summary>
            <param name="p">Specifies the ResizeParameter with the resize policy.</param>
            <param name="nOldWidth">Specifies the old width.</param>
            <param name="nOldHeight">Specifies the old height.</param>
            <param name="nNewWidth">Specifies the new 'inferred' width.</param>
            <param name="nNewHeight">Specifies the new 'inferred' width.</param>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.ApplyDistort(MyCaffe.basecode.SimpleDatum,MyCaffe.param.ssd.DistortionParameter)">
            <summary>
            The ApplyDistort method applies the distortion policy to the simple datum.
            </summary>
            <param name="sd">Specifies the SimpleDatum to distort.</param>
            <param name="p">Specifies the distortion parameters that define the distortion policy.</param>
            <returns>The distorted SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.ApplyDistortEx(MyCaffe.basecode.SimpleDatum,MyCaffe.param.ssd.DistortionParameter)">
            <summary>
            The ApplyDistortEx method applies the distortion policy to the simple datum.
            </summary>
            <param name="sd">Specifies the SimpleDatum to distort.</param>
            <param name="p">Specifies the distortion parameters that define the distortion policy.</param>
            <returns>The distorted SimpleDatum is returned.</returns>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.ApplyNoise(MyCaffe.basecode.SimpleDatum,MyCaffe.param.ssd.NoiseParameter)">
            <summary>
            The ApplyNoise method applies the noise policy to the SimpleDatum.
            </summary>
            <param name="sd">Specifies the SimpleDatum for which noise is to be added.</param>
            <param name="p">Specifies the NoiseParameter that defines the noise policy.</param>
            <remarks>
            NOTE: This method is not yet complete.
            </remarks>
        </member>
        <member name="M:MyCaffe.data.ImageTransforms`1.ApplyResize(MyCaffe.basecode.SimpleDatum,MyCaffe.param.ssd.ResizeParameter)">
            <summary>
            The ApplyResize method resizes the SimpleDatum containing an image to a newly resized image as specified by the resize parameter.
            </summary>
            <param name="sd">Specifies the SimpleDatum to resize - must contain a 3 channel image.</param>
            <param name="p">Specifies the resize parameter to apply.</param>
            <returns>The newly resized SimpleDatum is returned.</returns>
        </member>
        <member name="T:MyCaffe.fillers.BilinearFiller`1">
            <summary>
            Fills a Blob with coefficients for bilinear interpolation.
            </summary>
            <remarks>
            A common use case is with the DeconvolutionLayer acting as unsampling.
            You can upsample a feature amp with shape of (B, C, H, W) by any integer factor
            using the following proto:
            
            <code>
            layer {
              name: "upsample", type: "Deconvolution"
              bottom: "{{bottom_name}}" top: "{{top_name}}"
              convolution_param {
                kernel_size: {{2 * factor - factor % 2}} stride: {{factor}}
                num_output: {{C}} group: {{C}}
                pad: {{ceil((factor - 1) / 2.0)}}
                weight_filler: { type: "bilinear" } bias_term: false
              }
              param { lr_mult: 0 decay_mult: 0 }
            }
            </code>
            
            Please use this by replacing '{{}}' with your values.  By specifying
            'num_output: {{C}} group: {{C}}', it behaves as
            channel-wise convolution.  The filter shape of this deconvolution layer will be
            (C, 1, K, K) where K is 'kernel_size', and this filler will set a (K, K)
            interpolation kernel for every channel of the filter identically.  The resulting
            shape of the top featur emap will be (B, C, factor * H, factory * W).
            
            Note, that the learning rate and the
            weight decay are set to 0 in order to keep coefficient values of bilinear
            interpolation uncahnged during training.  If you apply this to an image, this
            operation is equivalent to the following call in Python with Scikit.Image:
            
            <code>
            out = skimage.transform.rescale(img, factor, mode='constant', cval=0)
            </code>
            </remarks>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.BilinearFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.BilinearFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill a memory with bilinear values.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.ConstantFiller`1">
            <summary>
            Fills a Blob with constant values x = c.
            </summary>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.ConstantFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.ConstantFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with a constant value.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.Filler`1">
            <summary>
            Abstract Filler class used to fill blobs with values.
            </summary>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="F:MyCaffe.fillers.Filler`1.m_cuda">
            <summary>
            Specifies the CudaDnn instance used to communicate to the low-level Cuda Dnn DLL.
            </summary>
        </member>
        <member name="F:MyCaffe.fillers.Filler`1.m_log">
            <summary>
            Specifies the output log.
            </summary>
        </member>
        <member name="F:MyCaffe.fillers.Filler`1.m_param">
            <summary>
            Specifies the filler parameters.
            </summary>
        </member>
        <member name="M:MyCaffe.fillers.Filler`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.Filler`1.Fill(MyCaffe.common.Blob{`0})">
            <summary>
            Fill the blob with values based on the actual filler used.
            </summary>
            <param name="b">Specifies the blob to fill.</param>
        </member>
        <member name="M:MyCaffe.fillers.Filler`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with values based on the actual filler used.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="M:MyCaffe.fillers.Filler`1.Create(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Create a new Filler instance.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance.</param>
            <param name="log">Specifies the log for output.</param>
            <param name="p">Specifies the filler parameter.</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.fillers.GaussianFiller`1">
            <summary>
            Fills a Blob with Gaussian-distributed values @f$ x = a @f$.
            </summary>
            <remarks>
            @see [Guassian Distribution](https://en.wikipedia.org/wiki/Normal_distribution) Wikipedia.
            </remarks>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.GaussianFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.GaussianFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with random numbers from a guassian distribution.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.MsraFiller`1">
            <summary>
            Fills a Blob with values @f$ x \sim N(0, \sigma^2) @f$ where 
            @f$ \sigma^2 @f$ is set inversely proportionla to number of incomming
            nodes, outgoing nodes, or their average.
            </summary>
            <remarks>
            A filler based on the paper [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) by He, Zhang, Ren and Sun 2015, 
            which specifically accounts for ReLU nonlinearities.
            
            Aside: for another perspective on the scaling factor, see the derivation of
            [Learning hierarchical categories in deep neural networks](http://web.stanford.edu/class/psych209a/ReadingsByDate/02_15/SaxeMcCGanguli13.pdf) by Saxe, McClelland, and Ganguli 2013 (v3).
            
            It fills the incoming matrix by randomly sampling Gaussian data with @f$ std =
            sqrt(2 / n) @f$ where @f$ n @f$ is the fan_in, fan_out or their average, depending on 
            the variance_norm option.   You should make sure the input blob has shape (num,
            a, b, c) where @f$ a * b * c = fan_in @f$ and @f$ num * b * c = fan_out@f$.  Note that this
            is currently not the case for inner product layers.
            </remarks>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.MsraFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.MsraFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with random numbers from a MSRA distribution.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.PositiveUnitballFiller`1">
            <summary>
            Fills a Blob with values @f$ x \in [0, 1] @f$
                such that @f$ \forall i \sum_j x{ij} = 1 @f$.
            </summary>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.PositiveUnitballFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.PositiveUnitballFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with random numbers from a postive unitball distribution.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.UniformFiller`1">
            <summary>
            Fills a Blob with uniformly distributed values @f$ x\sim U(a, b) @f$
            </summary>
            <remarks>
            @see [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) Wikipedia.
            </remarks>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.UniformFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.UniformFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with random numbers from a uniform distribution.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.fillers.XavierFiller`1">
            <summary>
            Fills a Blob with values @f$ x\sim U(-a, a) @f$ where @f$ a @f$ is 
            set inversely proportional to number of incoming nodes, outgoing
            nodes, or their average.
            </summary>
            <remarks>
            A filler based on paper [Understanding the difficulty of training deep feedfrward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by Bengio and Glorot, 2010.
            
            It fills the incoming matrix by randomly sampling uniform data from @f$ [-scale,
            scale] @f$ where @f$ scale = sqrt(3 / n) @f$ where @f$ n @f$ is the fan_in, fan_out, or their
            average, depending on the variance_norm option.  You shold make sure the
            input blob has shape (num, a, b, c) where @f$ a * b * c = fan_in @f$ and @f$ num * b * c
            = fan_out@f$.  Note that this is currently not the case for inner product layers.
            </remarks>
            <typeparam name="T">The base type <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.fillers.XavierFiller`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.FillerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Instance of CudaDnn - connection to cuda.</param>
            <param name="log">Log used for output.</param>
            <param name="p">Filler parameter that defines the filler settings.</param>
        </member>
        <member name="M:MyCaffe.fillers.XavierFiller`1.Fill(System.Int32,System.Int64,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Fill the memory with random numbers from a xavier distribution.
            </summary>
            <param name="nCount">Specifies the number of items to fill.</param>
            <param name="hMem">Specifies the handle to GPU memory to fill.</param>
            <param name="nNumAxes">Optionally, specifies the number of axes (default = 1).</param>
            <param name="nNumOutputs">Optionally, specifies the number of outputs (default = 1).</param>
            <param name="nNumChannels">Optionally, specifies the number of channels (default = 1).</param>
            <param name="nHeight">Optionally, specifies the height (default = 1).</param>
            <param name="nWidth">Optionally, specifies the width (default = 1).</param>
        </member>
        <member name="T:MyCaffe.ILayerCreator">
            <summary>
            The ILayerCreator interface is implemented by each MyCaffe.layers.x layer extension dll and is used to create new instances
            of the layers supported by each layer extension dll.
            </summary>
        </member>
        <member name="M:MyCaffe.ILayerCreator.CreateDouble(MyCaffe.common.CudaDnn{System.Double},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent,MyCaffe.basecode.IXImageDatabaseBase)">
            <summary>
            Create the layers when using the <i>double</i> base type.
            </summary>
            <param name="cuda">Specifies the connection to the low-level CUDA interfaces.</param>
            <param name="log">Specifies the output log.</param>
            <param name="p">Specifies the layer parameter.</param>
            <param name="evtCancel">Specifies the cancellation event.</param>
            <param name="imgDb">Specifies an interface to the image database, who's use is optional.</param>
            <returns>If supported, the layer is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.ILayerCreator.CreateSingle(MyCaffe.common.CudaDnn{System.Single},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent,MyCaffe.basecode.IXImageDatabaseBase)">
            <summary>
            Create the layers when using the <i>float</i> base type.
            </summary>
            <param name="cuda">Specifies the connection to the low-level CUDA interfaces.</param>
            <param name="log">Specifies the output log.</param>
            <param name="p">Specifies the layer parameter.</param>
            <param name="evtCancel">Specifies the cancellation event.</param>
            <param name="imgDb">Specifies an interface to the image database, who's use is optional.</param>
            <returns>If supported, the layer is returned, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="T:MyCaffe.layers.ImageDataLayer`1">
            <summary>
            The ImageDataLayer loads data from the image files located in the root directory specified.
            This layer is initialized with the MyCaffe.param.ImageDataParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.ImageDataLayer`1.m_swTimerBatch">
            <summary>
            Specifies a first timer used to calcualte the batch time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ImageDataLayer`1.m_swTimerTransaction">
            <summary>
            Specfies a second timer used to calculate the transaction time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ImageDataLayer`1.m_dfReadTime">
            <summary>
            Specifies the read time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ImageDataLayer`1.m_dfTransTime">
            <summary>
            Specifies the transaction time.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent)">
            <summary>
            The ImageDataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter data_param</param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel any pre-fetching operations.</param>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.ImageDataLayer`1.ExactNumBottomBlobs">
            <summary>
            No bottom blobs are used by this layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ImageDataLayer`1.ExactNumTopBlobs">
            <summary>
            Specifies the exact number of top blobs as 2 for this layer: data, label.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.shuffleImages">
            <summary>
            Shuffle the images so that they are loaded in a random order.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.setupSourceDescriptor">
            <summary>
            Allows any derivative classes to pre-initialize the m_src which is used in LayerSetup before the DataLayerSetup.
            </summary>
            <returns>When used this method should return <i>true</i>, otherwise <i>false</i> is returned by default.</returns>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.DataLayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the ImageDataLayer by starting up the pre-fetching.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ImageDataLayer`1.load_batch(MyCaffe.layers.Batch{`0})">
            <summary>
            Load a batch of data in the background (this is run on an internal thread within the BasePrefetchingDataLayer class).
            </summary>
            <param name="batch">Specifies the Batch of data to load.</param>
        </member>
        <member name="T:MyCaffe.layers.DataNormalizerLayer`1">
            <summary>
            The DataNormalizerLayer normalizes the input data (and optionally label) based on the normalization operations specified in the layer parameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides DataNormalizerParameter data_normalizer_param
            with DataNormalizerLayer options:
             - across_data_and_label (optional bool, default false).
                     when <i>true</i> normalizes across data range within both data and label.
             - steps.
                     List of normalization steps to take, with each performed in the order
                     for which they are listed.  
                     Steps include:
                       center - subtract the average to center the data.
                       stdev - divide the standard deviation of the data.
                       range - fit the data into the range specified by 'output_min','output_max'
                       additive - add the current value to the previous.
                       returns - take the percentage change between the current and one previous.
                       log - take the log of the current value.
             - ignore_ch (optional int).
                     List of channel indexes to ignore and NOT normalize.
             - input_min (optional double, default 0)
                     Specifies the input minimum used with stationary data.  When both 'input_min' 
                     and 'input_max' are set to 0, the data min/max used to normalize is determined
                     dynamcially from the data itself.
             - input_max (optional double, default 0)
                     Specifies the input minimum used with stationary data.  When both 'input_min' 
                     and 'input_max' are set to 0, the data min/max used to normalize is determined
                     dynamcially from the data itself.
             - input_mean (optional double, default null)
                     When specified, used by the 'center' step.  When not specified, the input_mean
                     is determined dynamically from the data itself.
             - input_stdev (optional double, default null)
                     When specified, used by the 'stdev' step.  When not specified, 
                     the input_stdev is determined dynamically from the data itself.
             - output_min (optional double, default 0)
                     Specifies the output minimum used by the 'range' step.
             - output_max (optional double, default 0)
                     Specifies the output maximum used by the 'range' step.
            </param>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.dispose">
            <summary>
            Clean up any resources used.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataNormalizerLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of bottom blobs required: data
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataNormalizerLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of bottom blobs required: data, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataNormalizerLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of top blobs required: data
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataNormalizerLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of top blobs required: data, label
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 1 or 2)
             -# @f$ (N \times C \times H \times W) @f$
                the data.
             -# @f$ (N \times C \times H \times W) @f$
                the label (optional).
            </param>
            <param name="colTop">input blob vector (length 1 or 2)
             -# @f$ (N \times C \times H \times W) @f$
                the normalized data.
             -# @f$ (N \times C \times H \times W) @f$
                the normalized label (optional).
            </param>
        </member>
        <member name="M:MyCaffe.layers.DataNormalizerLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented.
        </member>
        <member name="T:MyCaffe.layers.ParameterLayer`1">
            <summary>
            The ParameterLayer passes its blob[0] data and diff to the top[0].
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ParameterLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ParameterLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter.</param>
        </member>
        <member name="P:MyCaffe.layers.ParameterLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: none.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ParameterLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: flatten
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ParameterLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ParameterLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ParameterLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Pass the blob[0] through to the top[0].
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs x, which are ignored.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the blob[0] values</param>
        </member>
        <member name="M:MyCaffe.layers.ParameterLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Does nothing.
            </summary>
            <param name="colTop">top output blob vector, which is ignored.</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward, which is ignored.</param>
            <param name="colBottom">bottom input blob vector, which is ignored.</param>
        </member>
        <member name="T:MyCaffe.layers.CropLayer`1">
            <summary>
            The CropLayer takes a Blob and crops it to the shape specified by the second input 
            Blob, across all dimensions after the specified axis.
            
            This layer is initialized with the MyCaffe.param.CropParameter.
            </summary>
            <remarks>
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long, Evan Shelhamer, and Trevor Darrell, 2014.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The CropLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type CROP and crop_param.
            </param>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.CropLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.CropLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input, shape
            </summary>
        </member>
        <member name="P:MyCaffe.layers.CropLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: crop
            </summary>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the data 
             -# @f$ (N \times C \times H \times W) @f$
                the size
            </param>
            <param name="colTop">top output blob (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the cropped output.
            </param>
        </member>
        <member name="M:MyCaffe.layers.CropLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the concatenation inputs.
            </summary>
            <param name="colTop">top output blob (length 1), providing the error gradient with
            respect to the outputs.</param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">input blob vector (length 2).</param>
        </member>
        <member name="T:MyCaffe.layers.GradientScaleLayer`1">
            <summary>
            The GradientScaleLayer which scales the deltas during the backpropagation.
            This layer is initialized with the MyCaffe.param.GradientScaleParameter.
            </summary>
            <remarks>
            Scaling is performed according to the schedule:
            @f$ y = \frac{2 \cdot height} {1 + \exp(-\alpha \cot progress)} - upper\_bound @f$,
            where @f$ height = upper\_bound - lower\_bound @f$,
            @f$ lower\_bound @f$ is the smallest scaling factor,
            @f$ upper\_bound @f$ is the largest scaling factor,
            @f$ \alpha @f$ controls how fast the transition occurs between the scaling factors,
            @f$ progress = \min(iter / max\_iter, 1) @f$ corresponds to the current transition
            state (@f$ iter @f$ is the current iteration of the solver).
            
            The GradientScaleLayer can be used to implement
            gradient reversals.
            
            @see [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) by Ganin et al., 2015, v4 in 2016.
            @see [Github: ddtm-caffe](https://github.com/ddtm/caffe) for original source.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.GradientScaleLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The GradientScaleLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type EXP with parameter exp_param,
            with options:
                - lower_bound (\b optional, default 0) the @f$ lower\_bound @f$
                
                - upper_bound (\b optional, default 1) the @f$ upper\_bound @f$ 
                
                - alpha (\b optional, default 10) the @f$ \alpha @f$
                
                - max_iter (\b optional, default 1) the @f$ max\_iter @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.GradientScaleLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.GradientScaleLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (identity)
             </param>
            <param name="colTop">top output Blob vector (identity)
            </param>
        </member>
        <member name="M:MyCaffe.layers.GradientScaleLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Scales the error gradient w.r.t. the GRADIENTSCALER value inputs.
            </summary>
            <param name="colTop">top output blob vector, providing the error gradient
            with respect to outputs
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector
            </param>
        </member>
        <member name="T:MyCaffe.layers.MemoryLossLayer`1">
            <summary>
            The MemoryLossLayer provides a method of performing a custom loss functionality.  Similar to the MemoryDataLayer,
            the MemoryLossLayer supports an event used to get the loss value.  This event is called OnGetLoss, which once
            retrieved is used for learning on the backward pass.
            </summary>
            <remarks>
            To use this layer, you must implement the OnGetLoss event.
            </remarks>
            <typeparam name="T"></typeparam>
        </member>
        <member name="E:MyCaffe.layers.MemoryLossLayer`1.OnGetLoss">
            <summary>
            The OnGetLoss event fires during each forward pass.  The value returned is saved,
            and applied on the backward pass during training.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.user_state">
            <summary>
            Optionally specifies a user-state that is passed to the OnGetLoss event.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs as variable.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (output) Blobs: input 1.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of required bottom (output) Blobs: input 1 and 2.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: loss.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score for eachy of
                the K = CHW classes.  This layer maps these scores to a
                probability distribution over classes using the softmax function @f$
                \hat{p}_{nk} = \exp(x_{nk}) /
                \left[\sum_{k'} \exp(x_{nk'})\right] @f$ (see MemoryLayer).
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer valued blob with values @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the K classes.</param>
            <param name="colTop">top output blob vector (length 1)
                the computed cross_entropy classification loss: @f$ E = 
                \frac{-1}{N} \sum\limits_{n=1}^N \log(\hat{p}_{n,l_n})
                @f$ for softmax output class probabilities @f$ \hat{p} @f$.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Backpropagates the previously acquired (within the forward pass) loss error gradient w.r.t the predictions.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * the loss stored during the forward pass.
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1-2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; backward propagates loss previously calculated
                in the forward pass.
            </param>
        </member>
        <member name="T:MyCaffe.layers.MemoryLossLayerGetLossArgs`1">
            <summary>
            The MemoryLossLayerGetLossArgs class is passed to the OnGetLoss event.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.#ctor(MyCaffe.common.BlobCollection{`0},System.Object,System.Double)">
            <summary>
            The constructor.
            </summary>
            <param name="colBottom">Specifes the bottom inputs to the forward pass.</param>
            <param name="userState">Specifies a user-state.</param>
            <param name="dfNormalizer">Specifies the normalizer value.</param>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.user_state">
            <summary>
            Specifies a user-state.
            </summary>
            <remarks>The user-state is set via the 'user_state' property on the MemoryLossLayer.</remarks>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.Bottom">
            <summary>
            Specifies the bottom passed in during the forward pass.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.Normalizer">
            <summary>
            Specifies the normalizer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.Loss">
            <summary>
            Get/set the externally calculated total loss.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryLossLayerGetLossArgs`1.EnableLossUpdate">
            <summary>
            Get/set enabling the loss update within the backpropagation pass.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1">
            <summary>
            The SoftmaxCrossEntropyLayer computes the cross-entropy (logisitic) loss and is
            often used for predicting targets interpreted as probabilities in reinforcement learning.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SoftmaxCrossEntropyLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SOFTMAXCROSSENTROPY_LOSS.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs as variable.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: loss.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of required top (output) Blobs: loss, loss values
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the scores @f$ x \in [-\infty, +\infty] @f$,
                which this layer maps to probability predictions @f$
                \hat{p}_n = \sigma(x_n) \in [0,1]
                @f$
                using the softmax function @f$ \sigma(.) @f$ (see SoftmaxLayer).
             -# @f$ (N \times C \times H \times W) @f$
                the targets @f$ y \in [0,1] @f$.
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed cross-entropy loss: @f$
                  E = \frac{-1}{n} \sum\limits_{n=1}^N \left[
                          p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n)
                      \right]
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxCrossEntropyLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the softmax cross-entropy loss error gradient w.r.t. the 
            predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the target inputs (bottom[1]),
            so this method ignores bottom[1] and requires propagate_down[1] == false, 
            crashing otherwise.
            </remarks>
            <param name="colTop">top output blob (length 1), providing the error gradient with
            respect to the otuputs
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                This blob's diff will simply contain the loss_weight * @f$ \lambda @f$,
                as @f$ \lambda @f$ is the coefficient of this layer's output
                @f$ \ell_i @f$ in the overall Net loss @f$
                  E = \lambda_i \ell_i + \mbox{other loss terms} @f$; hence @f$
                  \frac{\partial E}{\partial \ell_i} = \lambda_i.
                  @f$
                  (*Assuming that this top blob is not used as a bottom (input) by any
                  other layer of the Net.)</param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false
            as gradient computation with respect to the targets is not implemented.
            </param>
            <param name="colBottom">input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; Backward computes diff @f$
                  \frac{\partial E}{\partial x} = 
                    \frac{1}{n} \sum\limits_{n=1}^N (\hat{p}_n - p_n)
                @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
            </param>
        </member>
        <member name="T:MyCaffe.layers.ClipLayer`1">
            <summary>
            The ClipLayer provides a neuron layer that clips the data to fit within the 
            [min,max] range.
            This layer is initialized with the MyCaffe.param.ClipParameter.
            </summary>
            <remarks>
            Computes the clip function @f$ y = x \max(min, \min(max,x)) @f$.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ClipLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ClipLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type Clip with parameter Clip_param,
            with options:
                - min the value @f$ \min @f$
                - max the value @f$ \max @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ClipLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$ 
                    y = \max(min, \min(max,x))
                @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ClipLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the Clip value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$
                    \frac{\partial E}{\partial x} = \left\{
                    \begin{array}{lr}
                      0 \: \mathrm{if} \; x \textless min \vee x > max \\           
                      \frac{\partial E}{\partial y} \: \mathrm{if} \; x \ge min \wedge x \le max
                    \end{array} \right.
                @f$ 
            </param>
        </member>
        <member name="T:MyCaffe.layers.SwishLayer`1">
            <summary>
            The SwishLayer provides a novel activation function that tends to work better than ReLU.
            This layer is initialized with the MyCaffe.param.SwishParameter.
            </summary>
            <remarks>
            Computes the swish non-linearity @f$ y = x \sigma (\beta x) @f$.
            
            @see [Activation Functions](https://arxiv.org/abs/1710.05941v2) by Prajit Ramachandran, Barret Zoph, Quoc V. Le., 2017.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SwishLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type Swish with parameter Swish_param,
            with options:
                - beta (\b default 1) the value @f$ \beta @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.dispose">
            <summary>
            Release all resources used.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$ 
                    y = x \sigma (\beta x)
                @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SwishLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the Swish value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$
                    \frac{\partial E}{\partial x}
                        = \frac{\partial E}{\partial y}(\beta y + 
                          \sigma (\beta x)(1 - \beta y))
                @f$ if propagate_down[0]
            </param>
        </member>
        <member name="T:MyCaffe.layers.MemoryDataLayer`1">
            <summary>
            The MemoryDataLayer provides data to the Net from memory.
            This layer is initialized with the MyCaffe.param.MemoryDataParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="E:MyCaffe.layers.MemoryDataLayer`1.OnGetData">
            <summary>
            The OnGetData event fires on the DataLayerSetup call and each time the data wraps around (e.g. all data as already fed through) during the forward call.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.MemoryDataLayer`1.OnDataPack">
            <summary>
            The OnDataPack event fires from within the AddDatumVector method and is used to pack the data into a specific ordering needed by the layers connected
            downstream of the MemoryDataLayer, such as an LSTM layer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The BaseDataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type MEMORYDATA with memorydata_param options:
              - batch_size. The batch size of the data.
              
              - channels. The number of channels in the data.
              
              - height. The height of the data.
              
              - width. The width of the data.
            </param>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.DataLayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the MemoryDataLayer.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the internal data and outputs.
            </summary>
            <param name="colBottom">Specifies the inputs.</param>
            <param name="colTop">Specifies teh outputs.</param>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.ExactNumBottomBlobs">
            <summary>
            No bottom blobs are used by this layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of top blobs: data
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of top blobs: data, clip, label
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.AddDatum(MyCaffe.basecode.SimpleDatum,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            This method is used to add a single Datum to the memory.
            </summary>
            <param name="sd">The Data Datum to add.</param>
            <param name="nLblAxis">Optionally, specifies the axis on which the multi-label data is placed.  This field is not used on SINGLE label types.</param>
            <param name="bReset">Optionally, specifies to force reset the internal data.</param>
            <param name="bResizeBatch">Optionally, specifies whether or not to size the batch to the number of rgData.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.AddDatumVector(MyCaffe.basecode.Datum[],MyCaffe.basecode.Datum[],System.Int32,System.Boolean,System.Boolean)">
            <summary>
            This method is used to add a list of Datums to the memory.
            </summary>
            <param name="rgData">The list of Data Datums to add.</param>
            <param name="rgClip">Optionally, specifies the clip data, if any exits.</param>
            <param name="nLblAxis">Optionally, specifies the axis on which the multi-label data is placed.  This field is not used on SINGLE label types.</param>
            <param name="bReset">Optionally, specifies to force reset the internal data.</param>
            <param name="bResizeBatch">Optionally, specifies whether or not to size the batch to the number of rgData.</param>
            <remarks>
            The batch size is only resized when the clip data is null, otherwise, regardless of the 'bResizeBatch' setting, the batch size is not changed.</remarks>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.AddDatumVector(System.Collections.Generic.List{MyCaffe.basecode.Datum},System.Collections.Generic.List{MyCaffe.basecode.Datum},System.Int32,System.Boolean,System.Boolean)">
            <summary>
            This method is used to add a list of Datums to the memory.
            </summary>
            <param name="rgData">The list of Data Datums to add.</param>
            <param name="rgClip">Optionally, specifies the clip data, if any exits.</param>
            <param name="nLblAxis">Optionally, specifies the axis on which the multi-label data is placed.  This field is not used on SINGLE label types.</param>
            <param name="bReset">Optionally, specifies to force reset the internal data.</param>
            <param name="bResizeBatch">Optionally, specifies whether or not to size the batch to the number of rgData.</param>
            <remarks>
            The batch size is only resized when the clip data is null, otherwise, regardless of the 'bResizeBatch' setting, the batch size is not changed.</remarks>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.Reset(MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0},System.Int32)">
            <summary>
            Resets the data by copying the internal data to the parameters specified.
            </summary>
            <param name="data">Specifies the data Blob that will receive a copy of the internal data.</param>
            <param name="labels">Specifies the label Blob that will receive a copy of the internal lables.</param>
            <param name="n">Specifies the number runs to perform on each batch.</param>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.Copy(MyCaffe.layers.MemoryDataLayer{`0})">
            <summary>
            Copy the data by copying the src alyer data and label to the parameters specified.
            </summary>
            <param name="src">Specifies the source layer.</param>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.batch_size">
            <summary>
            Returns the batch size.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.channels">
            <summary>
            Returns the data channels.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.height">
            <summary>
            Returns the data height.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.width">
            <summary>
            Returns the data width.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.clip_size1">
            <summary>
            Returns the clip 1 size, if any exists.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayer`1.clip_size2">
            <summary>
            Returns the clip 2 size, if any exists.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation which loads the data into the top (output) Blob%s.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                The data.
            </param>
        </member>
        <member name="T:MyCaffe.layers.MemoryDataLayerGetDataArgs">
            <summary>
            The MemoryDataLayerGetDataArgs class is passed to the OnGetData event.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayerGetDataArgs.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bInit">Set to <i>true</i> when the event fires from within the DataLayerSetup and <i>false</i> otherwise.</param>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerGetDataArgs.Initialization">
            <summary>
            Returns whether the event was fired during the DataLayerSetup call or not.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.MemoryDataLayerPackDataArgs`1">
            <summary>
            The MemoryDataLayerPackDataArgs is passed to the OnDataPack event which fires each time the data received in AddDatumVector needs to be packed 
            into a specific ordering as is the case when using an LSTM network.
            </summary>
            <typeparam name="T">Specifies the base datatype of <i>float</i> or <i>double</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.#ctor(MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0},MyCaffe.common.Blob{`0},System.Collections.Generic.List{MyCaffe.basecode.Datum},System.Collections.Generic.List{MyCaffe.basecode.Datum},MyCaffe.param.LayerParameter.LayerType)">
            <summary>
            The constructor.
            </summary>
            <param name="blobData">Specifies the blob data to fill with the ordered data.</param>
            <param name="blobClip">Specifies the clip data to fill with the ordered data.</param>
            <param name="blobLabel">Specifies the labeld ata to fill with ordered data.</param>
            <param name="rgData">Specifies the raw data to use to fill.</param>
            <param name="rgClip">Specifies the raw clip data to use to fill.</param>
            <param name="type">Specifies the LSTM type.</param>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.LstmType">
            <summary>
            Returns the LSTM type.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.Data">
            <summary>
            Returns the blob data to fill with ordered data.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.Clip">
            <summary>
            Returns the clip data to fill with ordered data for clipping.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.Label">
            <summary>
            Returns the label data to fill with ordered label information.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.DataItems">
            <summary>
            Returns the raw data items to use to fill.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MemoryDataLayerPackDataArgs`1.ClipItems">
            <summary>
            Returns the raw clip items to use to fill.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.DebugLayer`1">
            <summary>
            The DebugLayer merely stores, up to max_stored_batches, batches of input which
            are then optionally used by various debug visualizers.
            This layer is initialized with the MyCaffe.param.DebugParameter.
            </summary>
            <remarks>
            The data collected by the DebugLayer can later be used for debugging analysis.
            </remarks> 
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The DebugLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides DebugParameter debug_param with options:
            - max_stored_batches. Specifies the number of batches that the DebugLayer should store.
            </param>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.data">
            <summary>
            Returns a collection of Blobs containing the data stored by the DebugLayer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.labels">
            <summary>
            Returns a collection of Blobs containing the labels stored by the DebugLayer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.name">
            <summary>
            Returns the name of the DebugLayer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.kernel_handle">
            <summary>
            Returns the handle to the CudaDnn kernel where the debug GPU memory resides.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.load_count">
            <summary>
            Returns the number of batches actually loaded into the DebugLayer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs: data, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DebugLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of top (output) Blobs: data (passthrough)
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the top (output) to match the bottom (input), and reshape internal buffers.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward cache and pass through
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DebugLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Backward passthrough
            </summary>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
            <param name="rgbPropagateDown">Specifies whether or not to propagate each blob back.</param>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
        </member>
        <member name="T:MyCaffe.layers.SoftmaxLayer`1">
            <summary>
            The SoftmaxLayer computes the softmax function.
            This layer is initialized with the MyCaffe.param.SoftmaxParameter.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580v1) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov, 2012.
            @see [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144v2) by Wu, et al., 2016.
            @see [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538v1) by Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean, 2017.
            @see [Exploring the Limits of Language Modeling](https://arxiv.org/abs/1602.02410v2) by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SoftmaxLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SOFTMAX with parameter softmax_param,
            with options:
              - engine. The engine to use, either Engine.CAFFE, or Engine.CUDNN.
              
              - axis (\b optional, default = 1). The axis along which to perform the softmax.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: softmax
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CAFFE mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CAFFE.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CUDNN.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs.
            </param>
        </member>
        <member name="T:MyCaffe.layers.SPPLayer`1">
            <summary>
            The SPPLayer does spatial pyramid pooling on the input image
            by taking the max, average, etc. within regions
            so that the result vector of different sized
            images are of the same size.
            This layer is initialized with the MyCaffe.param.SPPParameter.
            </summary>
            <remarks>
            @see [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2014.
            @see [Image-based Localization using Hourglass Networks](https://arxiv.org/abs/1703.07971v1) by Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, and Esa Rahtu, 2017.
            @see [Relative Camera Pose Estimation Using Convolutional Neural Networks](https://arxiv.org/abs/1702.01381v2) by Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, Esa Rahtu, 2017.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_split_layer">
            <summary>
            The internal Split layer that feeds the pooling layers.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_colBlobSplitTopVec">
            <summary>
            Top vector holder used in call to the underlying SplitLayer::Forward
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_rgPoolingBottomVec">
            <summary>
            Bottom vector holder used in call to the underlying PoolingLayer::Forward
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_rgPoolingLayers">
            <summary>
            The internal Pooling layers of different kernel sizes.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_rgPoolingTopVecs">
            <summary>
            The vector holders used in call to underlying PoolingLayer::Forward
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_colBlobPoolingOutputs">
            <summary>
            Pooling outputs stores the outputs of the PoolingLayers.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_rgFlattenLayers">
            <summary>
            The internal Flatten layers that the Pooling layers feed into.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_rgFlattenLayerTopVecs">
            <summary>
            The top vector holders used to call to the underlying FlattenLayer::Forward
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_colBlobFlattenOutputs">
            <summary>
            Flatten outputs stores the outputs of the FlattenLayers.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_colBlobConcatBottomVec">
            <summary>
            Bottom vector holder used in call to the underlying ConcatLayer::Forward
            </summary>
        </member>
        <member name="F:MyCaffe.layers.SPPLayer`1.m_concat_layer">
            <summary>
            The internal Concat layer that the Flatten layers feed into.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SPPLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SPP with parameter spp_param,
            with options:
              - engine. Specifies whether to use the Engine.CAFFE or Engine.CUDNN for pooling.
              
              - pyramid_height. The pyramid height.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.SPPLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SPPLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: spp.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.getPoolingParam(System.Int32,System.Int32,System.Int32,MyCaffe.param.SPPParameter)">
            <summary>
            Calculates the kernel and stride dimensions for the pooling layer,
            returns a correctly configured LayerParameter for a PoolingLayer.
            </summary>
            <param name="nPyramidLevel">Specifies the pyramid level.</param>
            <param name="nBottomH">Specifies the bottom height.</param>
            <param name="nBottomW">Specifies the bottom width.</param>
            <param name="spp_param">Specifies the SPPParameter used.</param>
            <returns>The pooling parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the output.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SPPLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
            </param>
        </member>
        <member name="T:MyCaffe.layers.BatchReindexLayer`1">
            <summary>
            The BatchReindexLayer provides an index into the input blob along its first axis.
            </summary>
            <remarks>
            This layer can be used to select, reorder, and even replicate examples in a
            batch.  The second blob is cast to int and treated as an index into the 
            first axis of the first Blob.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The BatchReindexLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type BATCHREINDEX.
            </param>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.BatchReindexLayer`1.internal_blobs">
            @copydoc Layer 
        </member>
        <member name="P:MyCaffe.layers.BatchReindexLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of bottom (input) Blobs required: input, axis
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BatchReindexLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of top (output) Blobs required: batchreidx
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 2+)
             -# @f$ (N \times ...) @f$
                the inputs @f$ x_1 @f$
             -# @f$ (M) @f$
                the inputs @f$ x_2 @f$
            </param>
            <param name="colTop">output blob vector (length 1)
             -# @f$ (M \times ...) @f$: 
                the reindexed array @f$
                    y = x_1[x_2]
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.BatchReindexLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the reordered input.
            </summary>
            <param name="colTop">top output Blob vector (length 1),
            providing the error gradient with respect to the outputs
                -# @f$ (M \times ...) @f$:
                    containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                    with respect to concatenated outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 2):
                -# @f$ \frac{\partial E}{\partial y} @f$ is de-indexed (summing where
                    required) back to the input @f$ x_1 @f$.
                -# This layer cannot backprop to @f$ x_2 @f$, i.e. propagate{down[1] must be
                    false.</param>
        </member>
        <member name="T:MyCaffe.layers.EmbedLayer`1">
            <summary>
            The EmbedLayer is a layer for learning 'embeddings' of one-hot vector input.
            This layer is initialized with the MyCaffe.param.EmbedParameter.
            </summary>
            <remarks>
            Equivalent to an InnerProductLayer with one-hot vectors as input, but
            for efficiency the input is the 'hot' index of each column itself.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The EmbedLayer constructor
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides EmbedLayer embed_param,
            with EmbedLayer options:
            - num_output. The number of outputs for the Layer.
            
            - input_dim. The input given as an integer to be interpreted as one-hot
            vector indices with dimension num_input.  
            
            - bias_term (/bdefault = true).  Whether or not to use bias.
            
            - weight_filler. The weight filler to use.
            
            - bias_filler.  The bias filler to use.
            </param>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.EmbedLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.EmbedLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (intput) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.EmbedLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: embed
            </summary>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 1)</param>
            <param name="colTop">output blob vector (length 1)</param>
        </member>
        <member name="M:MyCaffe.layers.EmbedLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the input.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="T:MyCaffe.layers.TileLayer`1">
            <summary>
            The TileLayer copies a Blob along specified dimensions.
            This layer is initialized with the MyCaffe.param.TileParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.TileLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The TileLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type TILE with parameter tile_param,
            with options:
              - axis (\b optional, default 1). The index of the axis to tile.
              
              - tiles. The number of copies (tiles) of the Blob to output.
            </param>
        </member>
        <member name="P:MyCaffe.layers.TileLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.TileLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: tile.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.TileLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.TileLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.TileLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# (Set by the <i>axis</i> and <i>tiles</i> parameters) 
                the computed outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.TileLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
            </param>
        </member>
        <member name="T:MyCaffe.layers.ThresholdLayer`1">
            <summary>
            The ThresholdLayer is a neuron layer that tests whether the input exceeds a threshold: outputs 1 for inputs
            above threshold; 0 otherwise.
            This layer is initialized with the MyCaffe.param.ThresholdParameter.
            </summary>
            <remarks>
            @see [Neural Networks with Input Specified Thresholds](http://cs231n.stanford.edu/reports/2016/pdfs/118_Report.pdf) by Fei Liu and Junyang Qian, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ThresholdLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ThresholdLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type THRESHOLD with parameter threshold_param,
            with options:
              - threshold (\b optional, default 0). The threshold value @f$ x @f$ to which the input values are compared.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ThresholdLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ThresholdLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$.</param>
            <param name="colTop">top output blob (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$
                  y = \left\{
                  \begin{array}{lr}
                    0 \: \mathrm{if} \; x \le t \\
                    1 \: \mathrm{if} \; x > t
                  \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ThresholdLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented (non-diferentiable function)
        </member>
        <member name="T:MyCaffe.layers.ReshapeLayer`1">
            <summary>
            The ReshapeLayer reshapes the input Blob into an arbitrary-sized output Blob.
            This layer is initialized with the MyCaffe.param.ReshapeParameter.
            </summary>
            <remarks>
            Note: similarly to FlattenLayer, this layer does not change the input values
            (see FlattenLayer, Blob::ShareData and Blob::ShareDiff).
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.ReshapeLayer`1.m_rgCopyAxes">
            <summary>
            Vector of axes indices whos dimensions we'll copy from the bottom.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReshapeLayer`1.m_nInferredAxis">
            <summary>
            The index of the axis whose dimension we infer, or -1 if none.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReshapeLayer`1.m_nConstantCount">
            <summary>
            The product of the 'constant' output dimensions.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ReshapeLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides ArgMaxParameter argmax_param
            </param>
        </member>
        <member name="P:MyCaffe.layers.ReshapeLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ReshapeLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: reshape
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ReshapeLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReshapeLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReshapeLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - reshape Layers do not perform forward, reshaping is performed in Reshape().
        </member>
        <member name="M:MyCaffe.layers.ReshapeLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - reshape Layers do not perform backward.
        </member>
        <member name="T:MyCaffe.layers.FilterLayer`1">
            <summary>
            The FilterLayer takes two+ Blobs, interprets last Blob as a selector and
            filters remaining Blobs accordingly with selector data (0 means that
            the corresponding item has to be filtered, non-zero means that corresponding
            item needs to stay).
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.FilterLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The FilterLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides FilterParameter filter_param
            </param>
        </member>
        <member name="P:MyCaffe.layers.FilterLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (intput) Blobs: input, selector
            </summary>
        </member>
        <member name="P:MyCaffe.layers.FilterLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: filter
            </summary>
        </member>
        <member name="M:MyCaffe.layers.FilterLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.FilterLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.FilterLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2+)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs to be filtered @f$ x_1 @f$
             -# ...
             -# @f$ (N \times C \times H \times W) @f$
                the inputs to be filtered @f$ x_K @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the selector blob</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (S \times C \times H \times W) @f$ 
                the filtered output @f$ x_1 @f$ where @f$ S @f$ is the number of
                items that haven't been filtered.
             -# @f$ (S \times C \times H \times W) @f$
                the filtered output @f$ x_K @f$ where @f$ S @f$ is the number of 
                items that haven't been filtered
            </param>
        </member>
        <member name="M:MyCaffe.layers.FilterLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the forwarded inputs.
            </summary>
            <param name="colTop">top output Blob vecotr (length 1+), 
            providing the error gradient with respect to the outputs.</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">input Blob vecotor (length 2+), into which the top error
            gradient is copied.</param>
        </member>
        <member name="T:MyCaffe.layers.FlattenLayer`1">
            <summary>
            The FlattenLayer reshapes the input Blob into flat vectors
            This layer is initialized with the MyCaffe.param.FlattenParameter.
            </summary>
            <remarks>
            Note: because this layer does not change the input values -- merely the
            dimensions -- it can simply copy the input.  The copy happens 'virtually'
            (thus taking effectively 0 real time) by setting, in Forward, the data
            pointer of the top Blob to that of the bottom Blob (see Blob::ShareData),
            and in Backward, the diff pointer to the bottom Blob to that of the top Blob
            (see Blob::ShareDiff)
            
            @see [Representation Learning and Pairwise Ranking for Implicit and Explicit Feedback in Recommendation Systems](https://arxiv.org/abs/1705.00105v1) by Mikhail Trofimov, Sumit Sidana, Oleh Horodnitskii, Charlotte Laclau, Yury Maximov, and Massih-Reza Amini, 2017. 
            @see [Deep Neural Networks to Enable Real-time Multimessenger Astrophysics](https://arxiv.org/abs/1701.00008v2) by Daniel George, and E. A. Huerta, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.FlattenLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The FlattenLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides FlattenParameter flatten_param
            </param>
        </member>
        <member name="P:MyCaffe.layers.FlattenLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.FlattenLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: flatten
            </summary>
        </member>
        <member name="M:MyCaffe.layers.FlattenLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.FlattenLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.FlattenLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2+)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.
                the inputs.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times CHW \times 1 \times 1) @f$ the outputs -- i.e., the (virtually) copied, flattened inputs
            </param>
        </member>
        <member name="M:MyCaffe.layers.FlattenLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the concatenate inputs.
            </summary>
            <param name="colTop">top output Blob vecotr (length 1), 
            providing the error gradient with respect to the outputs.</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">input Blob vecotor (length @f$ k @f$), into which the top error
            gradient is (virtually) copied.</param>
        </member>
        <member name="T:MyCaffe.layers.ELULayer`1">
            <summary>
            The ELULayer computes exponential linear unit non-linearity @f$
                y = \left\{
                  \begin{array}{lr}
                    x \: \mbox{if} \; x > 0 \\
                   \alpha (\exp(x)-1) \: \mbox{if} \; x \le 0
                  \end{array} \right. 
                @f$.
            This layer is initialized with the MyCaffe.param.EluParameter.
            </summary>
            <remarks>
            @see [Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/abs/1604.04112) by Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde, and Sandip Shingade, 2016.
            @see [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289) by Djork-Arn Clevert, Thomas Unterthiner, and Sepp Hochreiter, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ELULayer constructor
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides ELUParameter elu_param,
            with ELULayer options:
            - alpha (\b optional, default 1).
              the values @f$ \alpha @f$ by which controls saturation for negative inputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs @f$
                y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$ 
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y}
                      \left(1 - \left[\frac{\exp(2x) - 1}{\exp(2x) + 1} \right]^2 \right)
                    = \frac{\partial E}{\partial y} (1 - y^2)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation using Cuda.
            </summary>
            <remarks>
            Computes if @f$ x > 0   => y = x @f$
                     if @f$ x \leq 0  => y = \alpha (\exp(x)-1) @f$
            </remarks>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$ 
                    y = \left\{ 
                    \begin{array}{lr}
                      x \: \mbox{if} \; x > 0 \\
                      \alpha (\exp(x)-1) \: \mbox{if} \; x \le 0
                    \end{array} \right. 
                @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the ELU value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$ 
                with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$
                    \frac{\partial E}{\partial x} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if} \; x > 0 \\
                      y + \alpha \: \mbox{if} \; x \le 0
                    \end{array} \right. 
                @f$ if propagate_down[0] == true.</param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation using cuDNN.
            </summary>
            <remarks>
            Computes if @f$ x > 0   => y = x @f$
                     if @f$ x \leq 0  => y = \alpha (\exp(x)-1) @f$
            </remarks>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$ 
                    y = \left\{ 
                    \begin{array}{lr}
                      x \: \mbox{if} \; x > 0 \\
                      \alpha (\exp(x)-1) \: \mbox{if} \; x \le 0
                    \end{array} \right. 
                @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ELULayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the ELU value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$ 
                with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$
                    \frac{\partial E}{\partial x} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if} \; x > 0 \\
                      y + \alpha \: \mbox{if} \; x \le 0
                    \end{array} \right. 
                @f$ if propagate_down[0] == true.</param>
        </member>
        <member name="T:MyCaffe.layers.ContrastiveLossLayer`1">
            <summary>
            The ContrastiveLossLayer computes the contrastive loss @f$
                E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 +
                    \left(1-y\right) \max \left(margin-d, 0\right)^2
                    @f$ where @f$
                    d = \left| \left| a_n - b_n \right| \right|_2 @f$. 
            This layer is initialized with the MyCaffe.param.ContrastiveLossParameter.
            </summary>
            <remarks>
            This can be used to train siamese networks.
            
            @see [Object cosegmentation using deep Siamese network](https://arxiv.org/pdf/1803.02555.pdf) by Prerana Mukherjee, Brejesh Lall and Snehith Lattupally, 2018.
            @see [Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval](https://arxiv.org/abs/1711.08490) by Yu-An Chung and Wei-Hung Weng, 2017.
            @see [Fully-Convolutional Siamese Networks for Object Tracking](https://arxiv.org/abs/1606.09549) by Luca Bertinetto, Jack Valmadre, Joo F. Henriques, Andrea Vedaldi, and Philip H. S. Torr, 2016.
            @see [Learning visual similarity for product design with convolutional neural networks](https://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf) by Sean Bell and Kavita Bala, Cornell University, 2015. 
            @see [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) by Raia Hadsel, Sumit Chopra, and Yann LeCun, 2006.
            @see [Similarity Learning with (or without) Convolutional Neural Network](http://slazebni.cs.illinois.edu/spring17/lec09_similarity.pdf) by Moitreya Chatterjee and Yunan Luo, 2017. 
            Centroids:
            @see [Retrieving Similar E-Commerce Images Using Deep Learning](https://arxiv.org/abs/1901.03546) by Rishab Sharma and Anirudha Vishvakarma, arXiv:1901.03546, 2019.
            @see [A New Loss Function for CNN Classifier Based on Pre-defined Evenly-Distributed Class Centroids](https://arxiv.org/abs/1904.06008) by Qiuyu Zhu, Pengju Zhang, and Xin Ye, arXiv:1904.06008, 2019.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ContrastiveLossLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns -1 specifying a variable number of bottoms
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.MinBottomBlobs">
            <summary>
            Returns the minumum number of bottom blobs: featA, featB, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.MaxBottomBlobs">
            <summary>
            Returns the minumum number of bottom blobs: featA, featB, label, centroids
            </summary>
            <remarks>
            The centroids are calculated for each class by the DecodeLayer and are only
            used when 'enable_centroid_learning' = True.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns -1 specifying a variable number of tops.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.MinTopBlobs">
            <summary>
            Specifies the minimum number of required top (output) Blobs: loss
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ContrastiveLossLayer`1.MaxTopBlobs">
            <summary>
            Specifies the maximum number of required top (output) Blobs: loss, matches
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.AllowForceBackward(System.Int32)">
            <summary>
            Unlike most loss layers, in the ContrastiveLossLayer we can backpropagate
            to the first two inputs.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 3)
             -# @f$ (N \times C \times 1 \times 1) @f$
                the features @f$ a \in [-\infty, +\infty]@f$
             -# @f$ (N \times C \times 1 \times 1) @f$
                the features @f$ b \in [-\infty, +\infty]@f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the binary similarity @f$ s \in [0, 1]@f$
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed contrastive loss: @f$ E = 
                  \frac{-1}{N} \sum\limits_{n=1}^N \left(y\right) d^2 +
                  \left(1-y\right) \max \left(margin-d, 0\right)^2
                  @f$ where @f$
                  d = \left| \left| a_n - b_n \right| \right|_2 @f$.
            </param>
            <remarks>
            This can be used to train siamese networks.
            </remarks>
        </member>
        <member name="M:MyCaffe.layers.ContrastiveLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the infogain loss error gradient w.r.t the inputs.
            </summary>
            <remarks>
            Computes the gradients with respect to the two input vectors (bottom[0] and
            bottom[1]), but not the similarity label (bottom[2]).
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * @f$ \lambda @f$, as
                 @f$ \lambda @f$ is the coefficient of this layer's output
                 @f$\ell_i@f$ in the overall Net loss.
                 @f$ E = \lambda_i \ell_i + \mbox{other loss terms}@f$; hence
                     @f$\frac{partial E}{\partial \ell_i} = \lambda_i @f$.
                 (*Assuming that this top blob is not used as a bottom (input) by any
                   other layer of the Net.)
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false as
            we can't compute gradients with respect to the labels (similarly for progagate_down[2] and
            the infogain matrix, if provided as bottom[2]).</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times 1 \times 1) @f$
                the features @f$a@f$; Backward fills their diff with 
                gradients if propagate_down[0] == true.
             -# @f$ (N \times C \times 1 \times 1) @f$
                the features @f$b@f$; Backward fills their diff with gradients if
                propagate_down[1] == true.
            </param>
        </member>
        <member name="T:MyCaffe.layers.MultinomialLogisticLossLayer`1">
            <summary>
            The MultinomialLogicistLossLayer computes the multinomial logistc loss for a one-of-many
            classification task, directly taking a predicted probability
            distribution as input.
            </summary>
            <remarks>
            When predictions are not already a probability distribution, you should
            instead use the SoftmaxWithLossLayer, which maps predictions to a
            distribution using the SoftmaxLayer, before computing the multinomial
            logistic loss.  The SoftmaxWithLossLayer should be preferred over separate
            SoftmaxLayer + MultinomialLogisticLossLayer
            as its gradient computation is more numerically stable.
            
            @see [V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797) by Fausto Milletari, Nassir Navab and Seyed-Ahmad Ahmadi, 2016. 
            @see [Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks](https://arxiv.org/abs/1605.02305) by Yuanzhouhan Cao, Zifeng Wu and Chunhua Shen, 2016. 
            @see [Deep Multitask Architecture for Integrated 2D and 3D Human Sensing](https://arxiv.org/abs/1701.08985) by Alin-lonut Popa, Mihai Zanfir and Cristian Sminchisescu, 2017.
            @see [HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition](https://arxiv.org/abs/1410.0736) by Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jadadeesh, Dennis DeCoste, Wei Di and Yizhou Yu, 2014.
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long, Evan Shelhamer and Trevor Darrell, 2014.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.MultinomialLogisticLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.MultinomialLogisticLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MultinomialLogisticLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ \hat{p} @f$, a Blob with values in
                [0,1] indicating the predicted probability of each of the
                K = CHW classes.  Each prediction vector @f$ \hat{p}_n @f$
                should sum to 1 as in a probability distribution:
                  @f$ \forall n \sum\limits_{k=1}^K \hat{p}_{nk} = 1 @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels @f$ l @f$, an integer-valued Blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed multinomial logistic loss: @f$ E = 
                  \frac{-1}{N} \sum\limits_{n=1}^N \log(\hat{p}_{n,l_n})
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.MultinomialLogisticLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the infogain loss error gradient w.r.t the predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the label inputs (bottom[1]),
            so this method ignores bottom[1] and requires !propagate_down[1], crashing
            if propagate_down[1] == true.
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * @f$ \lambda @f$ as
                 @f$ \lambda @f$ is the coefficient of this layer's output
                 @f$ \ell_i @f$ in the overall Net loss.
                 @f$ E = \lambda_i \ell_i + \mbox{other loss terms} @f$; hence @f$
                   \frac{partial E}{\partial \ell_i} = \lambda_i @f$
                   (*Assuming that this top blob is not used as a bottom (input) by any
                   other layer of the Net.)
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false as
            we can't compute gradients with respect to the labels (similarly for progagate_down[2] and
            the infogain matrix, if provided as bottom[2]).</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ \hat{p} @f$; backward computes diff 
                @f$
                  \frac{\partial E}{\partial \hat{p}} 
                @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
            </param>
        </member>
        <member name="T:MyCaffe.layers.InfogainLossLayer`1">
            <summary>
            The InforgainLossLayer is a generalization of SoftmaxWithLossLayer that takes an
            'information gain' (infogain) matrix specifying the 'value of all label
            pairs.
            This layer is initialized with the MyCaffe.param.InfogainLossParameter.
            </summary>
            <remarks>
            Equivalent to the SoftmaxWithLossLayer if the inforgain matrix is the
            identity.
            
            @see [DeepGaze II: Reading fixations from deep features trained on object recognition](https://arxiv.org/abs/1610.01563) by Matthias Kmmerer, Thomas S. A. Wallis, and Matthias Bethge, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The InfogainLossLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
               
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.ExactNumBottomBlobs">
            <summary>
            InfogainLossLayer takes 2-3 bottom blobs; if there are 3 the third should
            be the infogain matrix. (Otherwise the infogain matrix is loaded from a
            file specified by the LayerParameter.)
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (intput) Blobs: pred, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of required bottom (intput) Blobs: pred, label, matrix
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.ExactNumTopBlobs">
            <summary>
            InfogainLossLayer computes softmax probability internally.
            optional second 'top' outputs the sofmax probability.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: infogain
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InfogainLossLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of requried top (output) Blobs: infogain, softmax
            </summary>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.sum_rows_of_H(MyCaffe.common.Blob{`0})">
            <summary>
            Fill <i>m_blobSumRowsOfH</i> according to matrix H.
            </summary>
            <param name="blobH">Specifies the Sum Rows of H blob.</param>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2-3)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a Blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score for each of the
                @f$ K = CHW @f$ classes.  This layer maps these scores to a
                probability distribution over classes using the softmax function
                @f$ \hat{p}_{nk} = \exp(x_{nk}) /
                \left[\sum_{k'} \exp(x_{nk'})\right] @f$ (see SoftmaxLayer).
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels @f$ l @f$, an integer-valued Blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes
             -# @f$ (1 \times 1 \times K \times K) @f$
                (\b optional) the infogain matrix @f$ H @f$.  This must be provided as
                the third bottom blob input if not provided as the inforgain_mat in the
                InfogainLossParameter.  If @f$ H = I @f$, this layer is equivalent to the
                SoftmaxWithLossLayer.
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed infogain multinomial logistic loss: @f$ E = 
                  \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) =
                  \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k}
                  \log(\hat{p}_{n,k})
                @f$
                where @f$ H_{l_n} @f$ denotes row @f$ l_n @f$ of @f$ H @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.InfogainLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the infogain loss error gradient w.r.t the predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the label inputs (bottom[1]),
            so this method ignores bottom[1] and requires !propagate_down[1], crashing
            if propagate_down[1] == true.  (The same applies to the infogain matrix, if
            provided as bottom[2] rather than in the layer_param.)
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * @f$ \lambda @f$ as
                 @f$ \lambda @f$ is the coefficient of this layer's output
                 @f$ \ell_i @f$ in the overall Net loss.
                 @f$ E = \lambda_i \ell_i + \mbox{other loss terms} @f$; hence
                 @f$ \frac{partial E}{\partial \ell_i} = \lambda_i @f$
                   (*Assuming that this top blob is not used as a bottom (input) by any
                   other layer of the Net.)
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false as
            we can't compute gradients with respect to the labels (similarly for progagate_down[2] and
            the infogain matrix, if provided as bottom[2]).</param>
            <param name="colBottom">bottom input blob vector (length 2-3)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; Backward computes diff
                  @f$ \frac{\partial E}{\partial x} @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
             -# @f$ (1 \times 1 \times K \times K) @f$
                (\b optional) the information gain matrix -- ignored as its error
                gradient computation is not implemented.
            </param>
        </member>
        <member name="T:MyCaffe.layers.MVNLayer`1">
            <summary>
            The "Mean-Variance Normalization" MVNLayer normalizes the input to have 0-mean and/or unit (1) variance.
            This layer is initialized with the MyCaffe.param.MVNParameter.
            </summary>
            <remarks>
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            @see [Learning weakly supervised multimodal phoneme embeddings](https://arxiv.org/abs/1704.06913v1) by Rahma Chaabouni, Ewan Dunbar, Neil Zeghidour, and Emmanuel Dupoux, 2017. 
            @see [Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks](https://arxiv.org/abs/1304.1018) by Dimitri Palaz, Ronan Collobert, and Mathew Magimai-Doss, 2013.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The MVNLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LossParameter mvn_param, with options:
              - normalize_variance (\b optional, default = true). Whether or not to normalize the variance.
              
              - across_channels (\b optional, default = false). Whether or not to normalize across channels.
              
              - eps (\b optional, default = 1e-9). A small value to avoid divide by zero.
            </param>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.MVNLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.MVNLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.MVNLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: mvn
            </summary>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation that computes the normalization.
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                The input data.
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                The normalized output data.
            </param>
        </member>
        <member name="M:MyCaffe.layers.MVNLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the mvn error gradient w.r.t the output.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
             -# @f$ (N \times C \times H \times W) @f$
                the outputs.
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="T:MyCaffe.layers.AbsValLayer`1">
            <summary>
            The AbsValLayer computes the absolute value of the input.
            </summary>
            <remarks>
            Computes @f$ y = |x| @f$
            
            @see [Deep video gesture recognition using illumination invariants](https://arxiv.org/abs/1603.06531v1) by Gupta, Otkrist and Raviv, Dan and Raskar, Ramesh, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.AbsValLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The AbsValLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter.</param>
        </member>
        <member name="M:MyCaffe.layers.AbsValLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes @f$ y = |x| @f$
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs x</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$ y = |x| @f$</param>
        </member>
        <member name="M:MyCaffe.layers.AbsValLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the absolute value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$ with
                respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; Backward fills their diff with gradients @f$
                    \frac{\partial E}{\partial x} = \mathrm{sign}{x} \frac{\partial E}{\partial y}
                @f$
                if propagate_down[0] == true.</param>
        </member>
        <member name="T:MyCaffe.layers.BatchNormLayer`1">
            <summary>
            The BatchNormLayer normalizes the input to have 0-mean and/or unit (1) variance across
            the batch.
            This layer is initialized with the BatchNormParameter.
            </summary>
            <remarks>
            This layer computes Batch Normalization as described in [1]. For each channel
            in the data (i.e. axis 1), it subtracts the mean and divides by the variance, 
            where both statistics are computed across both spatial dimensions and across 
            the different examples in the batch.
            
            By default, during training time, the network its computing global 
            mean/variance statistics via a running average, which is then used at test
            time to allow deterministic outputs for each input.  You can manually
            toggle whether the network is accumulating or using the statistics via the
            use_global_stats option.  For reference, these statistics are kept int the
            layer's three blobs: (0) mean, (1) variance, and (2) moving average factor.
            IMPORTANT: for this feature to work, you MUST set the learning rate to zero
            for all three parameter blobs, i.e., param {lr_mult: 0} three times in the
            layer definition.
            
            Note that the original papaer also included a per-channel learned bias and
            scaling factor.  To implement this in Caffe, define a 'ScaleLayer' configured
            with 'bias_term: true' after each 'BatchNormLayer' to handle both the bias
            and scaling factor.
            
            [1] S. Ioffe and C. Szegedy, [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167). arXiv preprint
                arXiv:1502.03167 (2015).
                
            @see [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737v2) by Alexander Hermans, Lucas Beyer, and Bastian Leibe, 2017. 
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba,  and Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides BatchNormParam batch_norm_param.</param>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.BatchNormLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.BatchNormLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of bottom (input) Blobs required: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BatchNormLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of top (output) Blobs required: batchnorm
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the forward compuation.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the backward computation.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the forward compuation using the native Cuda version.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the backward computation using the native Cuda version.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the forward compuation using cuDNN.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BatchNormLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Perform the backward computation using cuDNN.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.AccuracyLayer`1">
            <summary>
            The AccuracyLayer computes the classification accuracy for a one-of-many
            classification task.
            This layer is initialized with the MyCaffe.param.AccuracyParameter.
            </summary>
            <remarks>
            @see [Convolutional Architecture Exploration for Action Recognition and Image Classification](https://arxiv.org/abs/1512.07502v1) by J. T. Turner, David Aha, Leslie Smith, and Kalyan Moy Gupta, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides AccuracyParameter accuracy_param,
            with AccuracyLayer options:
             - top_k (optional, default 1)
                     Sets the maximumrank k at which prediction is considered
                     correct, For example, if k = 5, a prediction is counted
                     correct if the correct label is among the top 5 predicted labels.</param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.AccuracyLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the number of bottom blobs used: predicted, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.AccuracyLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of top blobs: accuracy
            </summary>
        </member>
        <member name="P:MyCaffe.layers.AccuracyLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of top blobs: accuracy, labels
            </summary>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward compuation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score of each of
                the @f$ K = CHW @f$ classes.  Each @f$ x_n @f$ is mapped to a predicted 
                label @f$ \hat{l}_n @f$ given by its maximal index:
                @f$ \hat{l}_n = \arg\max\limits_k x_{nk} @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer-valued blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed accuracy: @f$
                  \frac{1}{N} \sum\limits_{n=1}^N \delta\{ \hat{l}_n = l_n \}
                @f$
                where @f$ 
                  \delta\{\mathrm{condition}\} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if condition} \\
                      0 \: \mbox{otherwise}
                    \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.forward_gpu(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward compuation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score of each of
                the @f$ K = CHW @f$ classes.  Each @f$ x_n @f$ is mapped to a predicted 
                label @f$ \hat{l}_n @f$ given by its maximal index:
                @f$ \hat{l}_n = \arg\max\limits_k x_{nk} @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer-valued blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed accuracy: @f$
                  \frac{1}{N} \sum\limits_{n=1}^N \delta\{ \hat{l}_n = l_n \}
                @f$
                where @f$ 
                  \delta\{\mathrm{condition}\} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if condition} \\
                      0 \: \mbox{otherwise}
                    \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.forward_cpu(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward compuation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score of each of
                the @f$ K = CHW @f$ classes.  Each @f$ x_n @f$ is mapped to a predicted 
                label @f$ \hat{l}_n @f$ given by its maximal index:
                @f$ \hat{l}_n = \arg\max\limits_k x_{nk} @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer-valued blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed accuracy: @f$
                  \frac{1}{N} \sum\limits_{n=1}^N \delta\{ \hat{l}_n = l_n \}
                @f$
                where @f$ 
                  \delta\{\mathrm{condition}\} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if condition} \\
                      0 \: \mbox{otherwise}
                    \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.forward_cpu_direct(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward compuation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [0, max_label] @f$ indicating the predicted label value.
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer-valued blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed accuracy: @f$
                  \frac{1}{N} \sum\limits_{n=1}^N \delta\{ \hat{l}_n = l_n \}
                @f$
                where @f$ 
                  \delta\{\mathrm{condition}\} = \left\{
                    \begin{array}{lr}
                      1 \: \mbox{if condition} \\
                      0 \: \mbox{otherwise}
                    \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.AccuracyLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented -- AccuracyLayer cannot be used as a loss.
        </member>
        <member name="T:MyCaffe.layers.LSTMUnitLayer`1">
            <summary>
            The LSTMUnitLayer is a helper for LSTMLayer that computes a single timestep of the
            non-linearity of the LSTM, producing the updated cell and hidden
            states.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The LSTMUnitLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LSTM_UNIT.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.LSTMUnitLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.LSTMUnitLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (intput) Blobs: prevtime, gatein, seqcon
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LSTMUnitLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: cellst, hiddenst
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.AllowForceBackward(System.Int32)">
            <summary>
            Returns <i>true</i> for all but the bottom index = 2 for 
            you can't propagate to the sequence comtinuation indicators.
            </summary>
            <param name="nBottomIdx">Specifies the bottom index.</param>
            <returns>Returns whether or not to allow a forced backward.</returns>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 3)
             -# @f$ (1 \times N \times D) @f$
                the previous timestep cell state @f$ c_{t-1} @f$
             -# @f$ (1 \times N \times D) @f$
                the 'gate inputs' @f$ [i_t', f_t', o_t', g_t'] @f$
             -# @f$ (1 \times N) @f$
                the sequence continuation indicators @f$ \delta_t @f$
            </param>
            <param name="colTop">output blob vector (length 2)
             -# @f$ (1 \times N \times d) @f$
                the updated cell state @f$ c_t @f$, computed as 
                <code>
                i_t := sigmoid[i_t']
                f_t := sigmoid[f_t']
                o_t := sigmoid[o_t']
                g_t := tanh[g_t']
                c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)
                </code>
             -# @f$ (1 \times N \times D) @f$
                the updated hidden state @f$ h_t @f$, computed as:
                    @f$ h_t := o_t .* \tanh[c_t] @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.LSTMUnitLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the LSTMUnit inputs.
            </summary>
            <param name="colTop">output Blob vector (length 2), providing the error gradient
            w.r.t. the outputs.
             -# @f$ (1 \times N \times D) @f$
                containing error gradients @f$ \frac{\partial E}{\partial c_t} @f$
                w.r.t. the updated cell state @f$ c_t @f$.
             -# @f$ (1 \times N \times D) @f$
                containing error gradients @f$ \frac{\partial E}{\partial h_t} @f$
                w.r.t. the updated cell state @f$ h_t @f$.</param>
            <param name="rgbPropagateDown">See Layer::Backward.</param>
            <param name="colBottom">input Blob vector (length 3), into which the error gradients
            w.r.t. the LSTMUnit inputs @f$ c_{t-1} @f$, and the gate inputs are computed.  Computation 
            of the error gradients w.r.t. the sequence indicators is not implemented.
             -# @f$ (1 \times N \times D) @f$
                the error gradient w.r.t. the previous timestep cells tate @f$ c_{t-1} @f$
             -# @f$ (1 \times N \times 4D) @f$
                the error gradient w.r.t. the 'gate inputs' @f$
                    [
                        \frac{\partial E}{\partial 'i_t'}
                        \frac{\partial E}{\partial 'f_t'}
                        \frac{\partial E}{\partial 'o_t'}
                        \frac{\partial E}{\partial 'g_t'}
                    ]
                @f$
             -# @f$(1 \times 1 \times N) @f$
                the gradient w.r.t. the sequence continuation indicators @f$ \delta_t @f$ 
                is currently not implemented.
            </param>
        </member>
        <member name="T:MyCaffe.layers.ArgMaxLayer`1">
            <summary>
            The ArgMaxLayer computes the index of the K max values for each datum across
            all dimensions @f$ (C \times H \times W) @f$.
            This layer is initialized with the MyCaffe.param.ArgMaxParameter.
            </summary>
            <remarks>
            Intended for use after a classification layer to produce a prediction.
            If parameter out_max_val is set to true, output is a vector of pairs
            (max_ind, max_val) for each image.  The axis parameter specifies an axis
            along which to maximize.
            
            @see [Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling](https://arxiv.org/abs/1612.06573v1) by Sebastian Ramos, Stefan Gehrig, Peter Pinggera, Uwe Franke, and Carsten Rother, 2016. 
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ArgMaxLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides ArgMaxParameter argmax_param
            with ArgMaxLayer options:
             - top_k (optional uint, default 1).
                     the number K of maximal items to output.
             - out_max_val (optional bool, default false).
                     if set, output a vector of pairs (max_ind, max_val) unless axis is set then
                     output max_val along the specified axis.
             - axis (optional int).
                     if set, maximise along the specified axis else maximise the flattened
                     trailing dimensions for each indes of the first / num dimension.
            </param>
        </member>
        <member name="P:MyCaffe.layers.ArgMaxLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of bottom blobs required: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ArgMaxLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of top blobs required: argmax
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ArgMaxLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ArgMaxLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ArgMaxLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs.
            </param>
            <param name="colTop">output blob vector (length 1)
             -# @f$ (N \times 1 \times K) @f$ or, if out_max_val
                @f$ (N \times 2 \times K) @f$ unless axis set then e.g.
                @f$ (N \times K \times H \times W) @f$ if axis == 1
                the computed outputs @f$
                  y_n = \arg\max\limits_i x_{ni}
                  (for k = 1)
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ArgMaxLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented.
        </member>
        <member name="T:MyCaffe.layers.BaseConvolutionLayer`1">
            <summary>
            The BaseConvolutionLayer is an abstract base class that factors out BLAS code common to
            ConvolutionLayer and DeconvolutionLayer
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_blobKernelShape">
            <summary>
            The spatial dimensions of the filter kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_blobStride">
            <summary>
            The spatial dimensions of the stride.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_blobPad">
            <summary>
            The spatial dimensions of the padding.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_blobDilation">
            <summary>
            The spatial dimentions of the dilation.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_blobConvInputShape">
            <summary>
            The spatial dimensions of the convolution input.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_rgColBufferShape">
            <summary>
            The spatial dimensionss of the col_buffer.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_rgOutputShape">
            <summary>
            The spatial dimensions of the output.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_rgBottomShape">
            <summary>
            The buttom shape.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nNumSpatialAxes">
            <summary>
            The number of spatial axes.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nBottomDim">
            <summary>
            The bottom dimension.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nTopDim">
            <summary>
            The top dimension.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nChannelAxis">
            <summary>
            The channel axis.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nNum">
            <summary>
            The number of items in the batch.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nChannels">
            <summary>
            The number of channels in each item.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nGroup">
            <summary>
            The group.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nOutSpatialDim">
            <summary>
            The output spatial dimension.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nWeightOffset">
            <summary>
            The weight offset used.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_nNumOutput">
            <summary>
            The number of outputs.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_bBiasTerm">
            <summary>
            Whether or not to use bias.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_bIs1x1">
            <summary>
            Whether or not the kernel is 1x1.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseConvolutionLayer`1.m_bForceNDim2col">
            <summary>
            Whether or not to force n-dim 2 column.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The BaseConvolutionLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.getWorkspaceLimitInBytes(System.Boolean)">
            <summary>
            Returns the workspace limit in bytes based on the cudnn_workspace_limit setting.
            </summary>
            <param name="bUseTensorCores">Specifies whether or not tensor cores are to be used and when they are we let cuDNN determine the workspace size.</param>
            <remarks>
            The following cudnn_workspace_limits are used as follows:
            0     = no workspace used.
            -1    = let CUDA choose fastest algorithm and workspace size.
            value = use the value specified * 16.
            </remarks>
            <returns>The workspace limit in bytes is returned.</returns>
        </member>
        <member name="P:MyCaffe.layers.BaseConvolutionLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.getWorkspace">
            <summary>
            Retruns the WorkspaceArgs containing the workspace used by this Layer.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.setWorkspace(System.UInt64)">
            <summary>
            If not already set, allocates the workspace needed in GPU memory.
            </summary>
            <param name="lSize">Specifies the size (in items) of workspace needed.</param>
            <returns>This method always returns <i>true</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="P:MyCaffe.layers.BaseConvolutionLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom Blobs: input 
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BaseConvolutionLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: output
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BaseConvolutionLayer`1.EqualNumBottomTopBlobs">
            <summary>
            Returns that there are an equal number of top and bottom Blobs.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.forward_gemm(System.Int64,System.Int32,System.Int64,System.Int64,System.Int32,System.Boolean)">
            <summary>
            Helper function that abstract away the column buffer and gemm arguments.
            </summary>
            <remarks>
            This function is only used when performing the Caffe version of convolution.
            </remarks>
            <param name="hInput">Specifies a handle to the input data in GPU memory.</param>
            <param name="nInputOffset">Specifies an offset (in items) into the input data.</param>
            <param name="hWeights">Specifies a handle to the weight data in GPU memory.</param>
            <param name="hOutput">Specifies a handle to the output data in GPU memory.</param>
            <param name="nOutputOffset">Specifies an offset (in items) into the output data.</param>
            <param name="bSkipIm2Col">Specifies whether or not to skip the im2coll function call for it was already computed.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.forward_bias(System.Int64,System.Int32,System.Int64)">
            <summary>
            Helper function that abstracts away the column buffer and gemm arguments.
            </summary>
            <remarks>
            This function is only used when performing the Caffe version of convolution.
            </remarks>
            <param name="hOutput">Specifies a handle to the output data in GPU memory.</param>
            <param name="nOutputOffset">Specifies an offset (in items) into the output data.</param>
            <param name="hBias">Specifies a handle to the bias data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.backward_gemm(System.Int64,System.Int32,System.Int64,System.Int64,System.Int32)">
            <summary>
            Helper function that abstract away the column buffer and gemm arguments.
            </summary>
            <remarks>
            This function is only used when performing the Caffe version of convolution.
            </remarks>
            <param name="hOutput">Specifies a handle to the output data in GPU memory.</param>
            <param name="nOutputOffset">Specifies an offset (in items) into the output data.</param>
            <param name="hWeights">Specifies a handle to the weight data in GPU memory.</param>
            <param name="hInput">Specifies a handle to the input data in GPU memory.</param>
            <param name="nInputOffset">Specifies an offset (in items) into the input data.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.weight_gemm(System.Int64,System.Int32,System.Int64,System.Int32,System.Int64)">
            <summary>
            Helper function that abstract away the column buffer and gemm arguments.
            </summary>
            <remarks>
            This function is only used when performing the Caffe version of convolution.
            </remarks>
            <param name="hInput">Specifies a handle to the input data in GPU memory.</param>
            <param name="nInputOffset">Specifies an offset (in items) into the input data.</param>
            <param name="hOutput">Specifies a handle to the output data in GPU memory.</param>
            <param name="nOutputOffset">Specifies an offset (in items) into the output data.</param>
            <param name="hWeights">Specifies a handle to the weight data in GPU memory.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.backward_bias(System.Int64,System.Int64,System.Int32)">
            <summary>
            Helper function that abstracts away the column buffer and gemm arguments.
            </summary>
            <remarks>
            This function is only used when performing the Caffe version of convolution.
            </remarks>
            <param name="hBias">Specifies a handle to the bias data in GPU memory.</param>
            <param name="hInput">Specifies a handle to the input data in GPU memory.</param>
            <param name="nInputOffset">Specifies an offset (in items) into the input data.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.input_shape(System.Int32)">
            <summary>
            Returns the spatial dimensions of the input.
            </summary>
            <param name="i">Specifies the index to add to the channel index.</param>
            <returns>The spatial dimension at the index is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.reverse_dimensions">
            <summary>
            reverse_dimensions should return true iff we are implementing deconv, so
            that conv helpers know which dimensions to use.
            </summary>
            <returns>If the dimensions are to be reversed (e.g. when deconvolving), <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.BaseConvolutionLayer`1.compute_output_shape">
            <summary>
            Compute height_out and width_out from other parameters.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.BaseDataLayer`1">
            <summary>
            The BaseDataLayer is the base class for data Layers that feed Blobs of data into the Net.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.BaseDataLayer`1.m_transformer">
            <summary>
            Specifies the DataTransformer used to transform each data item as it loaded.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseDataLayer`1.m_bOutputLabels">
            <summary>
            Specifies whether or not the Layer should output labels.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseDataLayer`1.m_imgdb">
            <summary>
            Specifies the CaffeImageDatabase.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseDataLayer`1.m_src">
            <summary>
            Specifies the SourceDescriptor of the data source.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BaseDataLayer`1.m_imgMean">
            <summary>
            Specifies the SimpleDatum that optionally contains the image Mean for data centering.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.IXImageDatabaseBase)">
            <summary>
            The BaseDataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter</param>
            <param name="db">Specifies the external database to use.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.setupSourceDescriptor">
            <summary>
            Allows any derivative classes to pre-initialize the m_src which is used in LayerSetup before the DataLayerSetup.
            </summary>
            <returns>When used this method should return <i>true</i>, otherwise <i>false</i> is returned by default.</returns>
        </member>
        <member name="P:MyCaffe.layers.BaseDataLayer`1.ImageMean">
            <summary>
            Get/set the image mean.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Implements common data layer setup functionality, and calls 
            DataLayerSetUp to do special data layer setup for individual layer types.
            </summary>
            <remarks>
            This method may not be overridden except by BasePrefetchingDataLayer.
            </remarks>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.DataLayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Override this method to perform the actual data loading.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Data layers have no bottoms, so reshaping is trivial.
            </summary>
            <param name="colBottom">not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BaseDataLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - data Layers do not perform backward.
        </member>
        <member name="P:MyCaffe.layers.BaseDataLayer`1.Transformer">
            <summary>
            Returns the data transformer used.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.BasePrefetchingDataLayer`1">
            <summary>
            The BasePrefetchingDataLayer is the base class for data Layers that pre-fetch data before feeding the Blobs of data into the Net.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.BasePrefetchingDataLayer`1.m_rgPrefetch">
            <summary>
            Specifies the pre-fetch cache.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.BasePrefetchingDataLayer`1.m_evtCancel">
            <summary>
            Specifies the cancellation event for the internal thread.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.basecode.CancelEvent)">
            <summary>
            The BaseDataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter</param>
            <param name="db">Specifies the external database to use.</param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel any pre-fetching operations.</param>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.preStop">
            <summary>
            The preStop method is called just before stopping the internal thread.  Overriding this
            method gives derivative classes a chance to cancel any waiting objects. 
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.dispose">
            @copydoc BaseDataLayer::dispose 
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            LayerSetUp implements common data layer setup functonality, and calls
            DatLayerSetUp to do special data layer setup for individual layer types.
            </summary>
            <remarks>
            This method should not be overriden.
            </remarks>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="P:MyCaffe.layers.BasePrefetchingDataLayer`1.delayPrefetch">
            <summary>
            Specifies whether or not to delay the prefetch.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.statupPrefetch">
            <summary>
            Starts the prefetch thread.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.final_process(MyCaffe.common.Blob{`0})">
            <summary>
            Provides a final processing step that may be utilized by derivative classes.
            </summary>
            <param name="blobTop">Specifies the top blob just about to be set out the forward operation as the Top[0] blob.</param>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward override implements the functionality to load pre-fetch data and feed it into the 
            top (output) Blobs.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BasePrefetchingDataLayer`1.load_batch(MyCaffe.layers.Batch{`0})">
            <summary>
            The load_batch abstract function should be overriden by each derivative data Layer to 
            load a batch of data.
            </summary>
            <param name="batch">Specifies the Batch of data loaded.</param>
        </member>
        <member name="T:MyCaffe.layers.Batch`1">
            <summary>
            The Batch contains both the data and label Blobs of the batch.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.Batch`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log)">
            <summary>
            The Batch constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
        </member>
        <member name="M:MyCaffe.layers.Batch`1.Dispose">
            <summary>
            Release all GPU and host resources used (if any).
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Batch`1.Data">
            <summary>
            Returns the data Blob of the batch.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Batch`1.Label">
            <summary>
            Returns the label Blob of the batch.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.BiasLayer`1">
            <summary>
            The BiasLayer computes a sum of two input Blobs, with the shape of the latter Blob 
            'broadcast' to match the shape of the former. Equivalent to tiling 
            the latter Blob, then computing the elementwise sum.
            This layer is initialized with the MyCaffe.param.BiasParameter.
            </summary>
            <remarks>
            The second input may be omitted, in which case it's learned as a parameter
            of the layer.  Note: in case bias and scaling are desired, both operations can
            be handled by 'ScaleLayer' configured with 'bias_term: true'.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The BiasLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type BIAS, with bias_param.
            </param>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.BiasLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.BiasLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs: input1
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BiasLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of required bottom (input) Blobs: input1, input2
            </summary>
        </member>
        <member name="P:MyCaffe.layers.BiasLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: bias
            </summary>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 1-2)</param>
            <param name="colTop">output blob vector (length 1)</param>
        </member>
        <member name="M:MyCaffe.layers.BiasLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the input.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1-2).</param>
        </member>
        <member name="T:MyCaffe.layers.BNLLLayer`1">
            <summary>
            The Binomial Normal Log Liklihod Layer.
            </summary>
            <remarks>
            Computes @f$ y = x + \log(1 + \exp(-x)) @f$ if @f$ x > 0 @f$;
                     @f$ y =     \log(1 + \exp(x)) @f$ otherwise.
                     
            @see [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) Wikipedia.
            @see [Imbalance Aware Lithography Hotspot Detection: A Deep Learning Approach](http://www.cse.cuhk.edu.hk/~byu/papers/C55_SPIE2017_CNN.pdf) by Haoyu Yang, Luyang Luo, Jing Su, Chenxi Lin,  and Bei Yu, 2017. 
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.BNLLLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The BNNLLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type BNNL.
            </param>
        </member>
        <member name="M:MyCaffe.layers.BNLLLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward compuation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs @f$
                y = \left\{
                  \begin{array}{ll}
                    x + \log(1 + \exp(-x)) \: \mbox{if } x > 0 \\
                        \log(1 + \exp(x)) \: \mbox{otherwise}
                  \end{array} \right.
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.BNLLLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the BNLL inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; backward fills their diff with gradients
                  @f$ 
                    \frac{\partial E}{\partial x}
                  @f$ if propagate_down[0] == true.
            </param>
        </member>
        <member name="T:MyCaffe.layers.ConcatLayer`1">
            <summary>
            The ConcatLayer takes at least two Blob%s and concatentates them along either the num
            or channel dimension, outputing the result.
            This layer is initialized with the MyCaffe.param.ConcatParameter.
            </summary>
            <remarks>
            @see [Deep Image Aesthetics Classification using Inception Modules and Fine-tuning Connected Layer](https://arxiv.org/abs/1610.02256) by Xin Jin, Jingying Chi, Siwei Peng, Yulu Tian, Chaochen Ye, and Xiaodong Li, 2016.
            @see [Multi-path Convolutional Neural Networks for Complex Image Classification](https://arxiv.org/abs/1506.04701) by Mingming Wang, 2015.
            @see [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567) by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, 2015.
            @see [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261) by Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ConcatLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ConcatLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type CONCAT and concat_param.
            </param>
        </member>
        <member name="P:MyCaffe.layers.ConcatLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ConcatLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: concat
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ConcatLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConcatLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConcatLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">bottom input blob (length 2+)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x_1 @f$
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x_2 @f$
             -# ...
             - K @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x_K @f$</param>
            <param name="colTop">top output blob (length 1)
             -# @f$ (KN \times C \times H \times W) @f$ if axis == 0 or
             -# @f$ (N \times KC \times H \times W) @f$ if axis == 1;
                the concatentation output @f$
                  y = [\begin{array}{cccc} x_1 \: x_2 \: ... \: x_k \end{array}]
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ConcatLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the concatenation inputs.
            </summary>
            <param name="colTop">top output blob (length 1), providing the error gradient with
            respect to the outputs.
             -# @f$ (KN \times C \times H \times W) @f$ if axis == 0, or
                @f$ (N \times KC \times H \times W) @f$ if axis == 1:
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to concatenated outputs y.</param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">input blob vector (length K), into which the top gradient
                @f$ \frac{\partial E}{\partial y} @f$ is deconcatenated back to the
                inputs @f$ 
                \left[ \begin{array}{cccc}
                 \frac{\partial E}{\partial x_1} \:
                 \frac{\partial E}{\partial x_2} \:
                 ... \:
                 \frac{\partial E}{\partial x_K}
                 \end{array} \right] = 
                 \frac{\partial E}{\partial y}
                 @f$
            </param>
        </member>
        <member name="T:MyCaffe.layers.ConvolutionLayer`1">
            <summary>
            The ConvolutionLayer convolves the input image with a bank of learned filters, and (optionally) adds biases.
            This layer is initialized with the MyCaffe.param.ConvolutionParameter.
            </summary>
            <remarks>
            Caffe convolves by reduction to matrix multiplication.  This achieves
            high-throughput and generality of input and filter dimensions but comes at
            the cost of memory for matrices.  This makes use of efficiency in BLAS.
            
            The input is 'im2col' transformed to a channel 'K x H x W' data matrix
            for multiplication with the 'N x K x H x W' filter matrix to yield a
            'N x H x W' output matrix that is then 'col2im' restored.  K is the
            input channel * kernel height * kernel width dimension of the unrolled
            inputs so that the im2col matrix has a column for each input region to
            be filtered.  col2im restores the output spatial structure by unrulling up
            the output channel 'N' columns of the output matrix.
            
            Note: cuDNN accelerates convolution through forward kernels for filtering and bias
            plus backward kernels for gradient w.r.t the filters, biases, and
            inputs.  Caffe + cuDNN further speeds up the computation through forward
            parallelism across groups and backward parallelism across gradients.
            
            The cuDNN engine does not have memory overhead for the matrix buffers.  For many
            input and filter regimes the cuDNN engien is faster than the CAFFE engine,
            but for fully-convolutional models and large inputs the CAFFE engine can be
            faster as long as it fits in memory.
            
            @see [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner, 1998.
            @see [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) by Vincent Dumoulin, and Francesco Visin, 2016.
            @see [Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks](https://arxiv.org/abs/1704.08331) by Nazrul Haque, N. Dinesh Reddy, and K. Madhava Krishna, 2017. 
            @see [A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction](https://arxiv.org/abs/1701.06190v1) by Yoonsik Kim, Insung Hwang, and Nam Ik Cho, 2017.
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long, Evan Shelhamer, and Trevor Darrell, 2014.
            @see [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) by Fisher Yu, and Vladlen Koltun, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ConvolutionLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides ConvolutionParameter convolution_param with ConvolutionLayer options:
             - num_output. The number of filters.
             
             - kernel_size / kernel_h / kernel_w.  The filter dimensions, given by
             kernel_size for square filters or kernel_h and kernel-w for rectangular 
             filters.
             
             - stride / stride_h / stride_w. (\b optional, default 1).  The filter
             stride, given by stride_size for equal dimensions of stride_h and stride_w
             for different strides.  By default the convolution is dense with stride 1.
             
             - pad / pad_h / pad_w. (\b optional, default 0). The zero-padding for
             convolutions, given by pad for equal dimensions or pad_h and pad_w for
             different padding.  Input padding is computed implicitly instead of 
             actual padding.
             
             - dilation (\b optional, default 1).  The filter
             dilation, given by dilation_size for equal dimensions for different
             dilation.  By default the convolution has dilation 1.
             
             - group (\b optional, default 1).  The number of filter groups.  Group
             convolution is a method for reducing parameterization by selectively
             connecting input and output channels.  The input and output channel dimensions
             must be divisible by the number of groups.  For group = 1, the 
             convolutionjf ilters input and output channels are separeated s.t. each
             group takes 1/group of the input channels and makes 1/group of the
             output channels.  Concretely 4 input channels, 8 output channels, and
             2 groups separate input chanels 1-2 and output channels 1-4 into the
             first group and input channels 3-4 and output channels 5-8 into the xecond
             group.
             
             - bias_term (\b optional, default, true). Whether to have a bias.
             
             - engine: convolution has Engine.CAFFE (matrix multiplication) and Engine.CUDNN (library
             kernels + stream parallelism) engines.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer for use with both Engine.CAFFE and Engine.CUDNN modes.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.reverse_dimensions">
            <summary>
            Returns <i>false</i>, for we want convolution, not deconvolution.
            </summary>
            <returns><i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.compute_output_shape">
            <summary>
            Computes the output shape used by the BaseConvolutionLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine.CAFFE mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine.CAFFE mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ConvolutionLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="T:MyCaffe.layers.InputLayer`1">
            <summary>
            The InputLayer provides data to the Net by assigning top Blobs directly.
            This layer is initialized with the MyCaffe.param.InputParameter.
            </summary>
            <remarks>
            This data Layer is a container that merely holds the data assigned to it;
            forward, backward and Reshape are all no-ops.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.InputLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The InputLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LayerParameter input_param, with options:
              - shape. Defines the shape of each top (output).
            </param>
        </member>
        <member name="M:MyCaffe.layers.InputLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.InputLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="P:MyCaffe.layers.InputLayer`1.ExactNumBottomBlobs">
            <summary>
            The InputLayer has no bottom Blobs and therefore returns 0.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InputLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: data
            </summary>
        </member>
        <member name="M:MyCaffe.layers.InputLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation - which is trivial.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">top output blob vector (length 1)
             -# The shape of the data output is specified by the 'shape' parameter value.
            </param>
        </member>
        <member name="M:MyCaffe.layers.InputLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - data Layers do not perform backward.
        </member>
        <member name="T:MyCaffe.layers.LabelMappingLayer`1">
            <summary>
            /b DEPRECIATED (use DataLayer DataLabelMappingParameter instead) The LabelMappingLayer converts original labels to new labels specified by the label mapping.
            This layer is initialized with the MyCaffe.param.LabelMappingParameter.
            </summary>
            <remarks>
            The LabelMappingLayer is a neuron layer attached to the data layer.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.IXImageDatabaseBase)">
            <summary>
            The InnerProductLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LayerParameter labelmapping_param, with options:
              - mapping. Defines the mappings from original label to new label.
              
              - update_database (/b optional, default = false). Whether or not to update the database with the new mapping, otherwise mappings are used online.
              
              - reset_database_labels (/b optional, default = false). Whether or not to reset the database to its original labels or not. This setting requires 'update_database' = true.
              
              - label_boosts (/b optional, default = ""). A string that defines which labels to boost, giving them a higher probability of being selected.
            </param>
            <param name="db">Specifies the CaffeImageDatabase.</param>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.SetNetParameterUsed(MyCaffe.param.NetParameter)">
            <summary>
            Set the parameters needed from the Net, namely the data source used.
            </summary>
            <param name="np">Specifies the NetParameter used.</param>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.GetActualLabelCounts(System.String)">
            <summary>
            Returns a string describing the actual label counts observed during training.
            </summary>
            <param name="strSrc">Specifies the data source to query.</param>
            <returns>The string describing the actual label counts observed is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The LayerSetUp method adjusts the label boost values according to the number
            of mappings made to each label.
            </summary>
            <remarks>
            The basic idea is to even out the image selection between all mapped and all
            non mapped categories.
            <br/>
            So for example, given the following mappings:
            <code>
            mapping                boost
            %-----------------------------------------------
            1 -> 3                 7/3 = 2.33
            2 -> 3                 7/3 = 2.33
            3 (no mapping)         7/3 = 2.33
            4 (no mapping)         7   = 7.00
            5 (no mapping)         7/3 = 2.33
            6 -> 5                 7/3 = 2.33
            7 -> 5                 7/3 = 2.33
            </code>
            <br/>
            This evens out and makes it equally probable to get an image
            from 1,2,3 vs 4 vs 5,6,7 thus treating the images as though
            they actually only had the labels 3, 4 and 5.
            <br/>
            Note, mappings now support mapping conditionally only when
            an image has a specific boost value.  To use conditional
            mapping use the following format:
            <br/>
            "FromLabel->ToLabel?boost=1"        // i.e. only map to the 'ToLabel' if the boost = 1.
            <br/>
            For example:
            <br/>
            "1->2?boost=1"                      // only map from 1 to 2 if boost = 1 on the image.     
            <br/>
            NOTE: spaces are not allowed between the 'ToLabel', the '?', and the 'boost=', etc.
            </remarks>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Converts the input label to the new label specified by the label mapping.
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the inputs x</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the newly mapped labels.</param>
        </member>
        <member name="M:MyCaffe.layers.LabelMappingLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - The LabelMappingLayer does not perform backward.
        </member>
        <member name="T:MyCaffe.layers.DataLayer`1">
            <summary>
            The DataLayer loads data from the IXImageDatabase database.
            This layer is initialized with the MyCaffe.param.DataParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_db">
            <summary>
            Specifies the database.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_cursor">
            <summary>
            Specifies the database used to traverse through the database.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_swTimerBatch">
            <summary>
            Specifies a first timer used to calcualte the batch time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_swTimerTransaction">
            <summary>
            Specfies a second timer used to calculate the transaction time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_dfReadTime">
            <summary>
            Specifies the read time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DataLayer`1.m_dfTransTime">
            <summary>
            Specifies the transaction time.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.DataLayer`1.OnBatchLoad">
            <summary>
            This event fires (only when set) each time a batch is loaded form this dataset.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.basecode.CancelEvent)">
            <summary>
            The DataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter data_param</param>
            <param name="db">Specifies the external database to use.</param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel any pre-fetching operations.</param>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.preStop">
            <summary>
            The preStop override is called just before stopping the internal thread managed by the base class.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.DataLayer`1.delayPrefetch">
            <summary>
            Specifies to delay the prefetch when using a synchronized Data Layer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.Connect(MyCaffe.layers.DataLayer{`0})">
            <summary>
            The Connect method connects one Data Layer to another so that they can synchronize.
            </summary>
            <param name="src">Specifies the source Data Layer whos OnBatchLoad event fires and
            is handled by this Data Layer.
            </param>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.Disconnect">
            <summary>
            Disconnect any previously connected Data Layers.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.DataLayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the DataLayer by starting up the pre-fetching.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="P:MyCaffe.layers.DataLayer`1.ExactNumBottomBlobs">
            <summary>
            No bottom blobs are used by this layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataLayer`1.MinTopBlobs">
            <summary>
            Specifies the minimum number of required top (output) Blobs: data
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DataLayer`1.MaxTopBlobs">
            <summary>
            Specifies the maximum number of required top (output) Blobs: data, label
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.Next">
            <summary>
            Retrieves the next item from the database and rolls the cursor over once the end 
            of the dataset is reached.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.Skip">
            <summary>
            Skip to the next value - used when training in a multi-GPU scenario.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.final_process(MyCaffe.common.Blob{`0})">
            <summary>
            Provides a final processing step that takes place at the end of the base class forward = this is where we apply the mask if one exists and is enabled.
            </summary>
            <param name="blobTop">Specifies the top blob just about to be set out the forward operation as the Top[0] blob.</param>
        </member>
        <member name="M:MyCaffe.layers.DataLayer`1.load_batch(MyCaffe.layers.Batch{`0})">
            <summary>
            Load a batch of data in the background (this is run on an internal thread within the BasePrefetchingDataLayer class).
            </summary>
            <param name="batch">Specifies the Batch of data to load.</param>
        </member>
        <member name="T:MyCaffe.layers.LastBatchLoadedArgs">
            <summary>
            Specifies the arguments sent to the OnBatchLoad event used when synchronizing between Data Layers.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LastBatchLoadedArgs.#ctor(System.Collections.Generic.List{System.Int32})">
            <summary>
            The constructor.
            </summary>
            <param name="rgLabels">Specifies the labels loaded.</param>
        </member>
        <member name="P:MyCaffe.layers.LastBatchLoadedArgs.Labels">
            <summary>
            Returns the labels loaded.
            </summary>
        </member>
        <member name="T:MyCaffe.layers.DeconvolutionLayer`1">
            <summary>
            The DeconvolutionLayer convolves the input with a bank of learned filtered, and (optionally)
            add biases, treating filters and convolution parameters in the
            opposite sense as ConvolutionLayer.
            This layer is initialized with the MyCaffe.param.ConvolutionParameter.
            
            ConvolutionLayer computes each output value by dotting an input window with
            a filter; DeconvolutionLayer multiplies each input value by a filter
            elementwise, and sums over the resulting output windows.  In other words,
            DeconvolutionLayer is ConvolutionLayer with the forward and backward passes
            reversed.  DeconvolutionLayer reuses ConvolutionParameter for its
            parameters, but they take the opposite sense in ConvolutionLayer (so
            padding is removed from the output rather than added to the input, and
            stride results in upsampling rander than downsampling).
            </summary>
            <remarks>
            @see [Learning Deconvolution Network for Semantic Segmentation](https://arxiv.org/abs/1505.04366) by Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han, 2015.
            @see [Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution](https://arxiv.org/abs/1611.06495) by Jiawei Zhang, Wei-Sheng Pan, Rynson Lau, and Ming-Hsuan Yang, 2016. 
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long,  Evan Shelhamer, and Trevor Darrell, 2014.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The DeconvolutionLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides ConvolutionParameter convolution_param with DeconvolutionLayer options:
             - num_output. The number of filters.
             
             - kernel_size / kernel_h / kernel_w.  The filter dimensions, given by
             kernel_size for square filters or kernel_h and kernel-w for rectangular 
             filters.
             
             - stride / stride_h / stride_w. (\b optional, default 1).  The filter
             stride, given by stride_size for equal dimensions of stride_h and stride_w
             for different strides.  By default the convolution is dense with stride 1.
             
             - pad / pad_h / pad_w. (\b optional, default 0). The zero-padding for
             convolutions, given by pad for equal dimensions or pad_h and pad_w for
             different padding.  Input padding is computed implicitly instead of 
             actual padding.
             
             - dilation (\b optional, default 1).  The filter
             dilation, given by dilation_size for equal dimensions for different
             dilation.  By default the convolution has dilation 1.
             
             - group (\b optional, default 1).  The number of filter groups.  Group
             convolution is a method for reducing parameterization by selectively
             connecting input and output channels.  The input and output channel dimensions
             must be divisible by the number of groups.  For group = 1, the 
             convolutionjf ilters input and output channels are separeated s.t. each
             group takes 1/group of the input channels and makes 1/group of the
             output channels.  Concretely 4 input channels, 8 output channels, and
             2 groups separate input chanels 1-2 and output channels 1-4 into the
             first group and input channels 3-4 and output channels 5-8 into the xecond
             group.
             
             - bias_term (\b optional, default, true). Whether to have a bias.
             
             - engine: convolution has Engine.CAFFE (matrix multiplication) and Engine.CUDNN (library
             kernels + stream parallelism) engines.
            </param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer for use with both Engine.CAFFE and Engine.CUDNN modes.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.reverse_dimensions">
            <summary>
            Returns <i>true</i>, for we want deconvolution, not convolution.
            </summary>
            <returns><i>true</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.compute_output_shape">
            <summary>
            Computes the output shape used by the BaseConvolutionLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation with Engine.CAFFE.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using Engine.CAFFE.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation with Engine.CUDNN.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DeconvolutionLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using Engine.CUDNN.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="T:MyCaffe.layers.DropoutLayer`1">
            <summary>
            During training only, sets a random portion of @f$ x @f$ to 0, adjusting
            the rest of the vector magnitude accordingly
            This layer is initialized with the MyCaffe.param.DropoutParameter.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhavsky, and Ruslan R. Salakhutdinov, 2012.
            @see [Information Dropout: Learning Optimal Representations Through Noisy Computation](https://arxiv.org/abs/1611.01353) by Alessandro Achille, and Stevano Soatto, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.DropoutLayer`1.m_blobRand">
            <summary>
            When divided by UINT_MAX, the randomly generated values of u\sim U(0,1)
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DropoutLayer`1.m_dfThreshold">
            <summary>
            The probability p of dropping any input.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.DropoutLayer`1.m_dfScale">
            <summary>
            The scale of undropped inputs at train time 1/(1-p)
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The DeconvolutionLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides DropoutParameter dropout_param with options:
             - dropout_ratio. The dropout ratio.
             
             - seed.  Optionally, specifies a seed for the random number generator used.
            </param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer for use with both Engine.CAFFE and Engine.CUDNN modes.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <remarks>
            Note: during TESTING and RUN, this layer merely acts as a pass-through.
            </remarks>
            <param name="colBottom">blottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs.  
            </param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.forward_caffe(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine CAFFE mode as specified in the LayerParameter.
            </summary>
            <remarks>
                At training time, we have @f$ 
                y_{\mbox{train}} = \left\{
                  \begin{array}{ll}
                    \frac{x}{1 - p} \: \mbox{if } u > p \\
                    0 \: \mbox{otherwise}
                  \end{array} \right.
                @f$, where @f$ u \sim U(0, 1)@f$ is generated independently for each
                input at each iteration.  At test time, we simply have
                @f$ y_{\mbox{test}} = \mathbb{E}[y_{\mbox{train}}] = x @f$.
                
            Note: during TESTING and RUN, this layer merely acts as a pass-through.
            </remarks>
            <param name="colBottom">blottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the computed outputs.  
            </param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.backward_caffe(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine CAFFE mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DropoutLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="T:MyCaffe.layers.DummyDataLayer`1">
            <summary>
            The DummyDataLayer provides data to the Net generated by a Filler.
            This layer is initialized with the MyCaffe.param.DummyDataParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.DummyDataLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The DummyDataLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides DummyDataParameter dummy_data_param with options:
             - data_filler. A list of Fillers to use.
             
             - shape.  A list of shapes to use.
            </param>
        </member>
        <member name="P:MyCaffe.layers.DummyDataLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns 0 for data layers have no bottom (input) Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.DummyDataLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: data
            </summary>
        </member>
        <member name="M:MyCaffe.layers.DummyDataLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DummyDataLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Data layers have no bottoms, so reshaping is trivial.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.DummyDataLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation, which fills the data into the top (output) Blobs.
            </summary>
            <param name="colBottom">Not used.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the data outputs.  
            </param>
        </member>
        <member name="M:MyCaffe.layers.DummyDataLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - data Layers do not perform backward..
        </member>
        <member name="T:MyCaffe.layers.EltwiseLayer`1">
            <summary>
            The EltwiseLayer computes elementwise oeprations, such as product and sum,
            along multiple input blobs.
            This layer is initialized with the MyCaffe.param.EltwiseParameter.
            </summary>
            <remarks>
            @see [DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification](https://arxiv.org/abs/1611.05271v1) by Shu Zhang, Ran He, and Tieniu Tan, 2016. 
            @see [Mixed context networks for semantic segmentation](https://arxiv.org/abs/1610.05854v1) by Haiming Sun, Di Xie, and Shiliang Pu, 2016. 
            @see [Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks](https://arxiv.org/abs/1511.06314v1) by Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The EltwiseLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides EltwiseParameter eltwise_param with options:
             - operation. The eltwise operation (e.g. product, summation, maximum).
             
             - coeff.  A Blob-wise coefficient for summation.
             
             - stable_prod_grad.  Optionally use an asymtotically slower but more stable method for computing the gradient for product operations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.EltwiseLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.EltwiseLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum required number of bottom (input) Blobs: input1, input2
            </summary>
        </member>
        <member name="P:MyCaffe.layers.EltwiseLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: output (result of eltwise operation in input1 and input2)
            </summary>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The Forward computation.
            </summary>
            <param name="colBottom">input blob vector (length 1-2)</param>
            <param name="colTop">output blob vector (length 1)</param>
        </member>
        <member name="M:MyCaffe.layers.EltwiseLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the input.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1-2).</param>
        </member>
        <member name="T:MyCaffe.layers.EuclideanLossLayer`1">
            <summary>
            The EuclideanLossLayer computes the Euclidean (L2) loss @f$
                E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n
                    \right| \right|_2^2 @f$ for real-valued regression tasks.
            </summary>
            <remarks>
            This can be used for least-squares regression tasks.  An InnerProductLayer
            input to a EuclideanLossLayer exactly formulates a linear least squares
            regression problem.  With non-zero weight decay the problem becomes one of
            ridge regression -- see MyCaffe.test/TestGradientBasedSolver.cs for a concrete
            example wherein we check that the gradients computed for a Net with exactly
            this structure match hand-computed gradient formulas for ridge regression.
            
            (Note: Caffe, and SGD in general, is certainly not the best way to solve
            linear least squares problems! We use it only as an instructive example.)
            
            @see [Linking Image and Text with 2-Way Nets](https://arxiv.org/abs/1608.07973) by Aviv Eisenschtat,  and Lior Wolf, 2016.
            @see [Constrained Structured Regression with Convolutional Neural Networks](https://arxiv.org/abs/1511.07497) by Deepak Pathak, Philipp Krhenbhl, Stella X. Yu, and Trevor Darrell, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The EuclideanLossLayer constructor
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type EUCLIDEAN_LOSS.</param>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.AllowForceBackward(System.Int32)">
            <summary>
            Unlike most loss layers, in the EuclideanLossLayer we can backpropagate
            to both inputs -- override to return true and always allow force_backward.
            </summary>
            <param name="nBottomIdx">Specifies the index of the bottom element.</param>
            <returns>Returns <i>true</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ \hat{y} \in [-\infty, +\infty] @f$
             -# @f$ (N \times C \times H \times W) @f$
                the targets @f$ y \in [-infty, +\infty] @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed Euclidean loss: @f$  
                E = \frac{1}{2n} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n
                    \right| \right|_2^2 @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.EuclideanLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the Euclidean error gradient w.r.t. the inputs.
            </summary>
            <remarks>
            Unlike other children of LossLayer, EuclideanLossLayer can compute
            gradients with respect to the label inputs bottom[1] (but still only will
            if propagate_down[1] is set, due to being produced by learnable parameters
            or if force_backward is set). In fact, this layer is 'compmutative' -- the
            result is the same regardless of the order of the two bottoms.
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                This blob's diff will simply contain the loss_weight * @f$ \lambda @f$,
                as @f$ \lambda @f$ is the coefficient of this layer's output
                @f$\ell_i@f$ in the overall Net loss.
                @f$ E = \lambda_i \ell_i + \mbox{other loss terms}@f$; hence
                @f$ \frac{\partial E}{\partial \ell_i} = \lambda_i @f$.
                (*Assuming that this top Blob is not used by a bottom (input) by any
                other layer of the Net.)</param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$\hat{y}@f$; Backward fills their diff with
                gradients @f$
                 \frac{\partial E}{\partial \hat{y}} = 
                   \frac{1}[n} \sum\limits_{n-1}^N (\hat{y}_n - y_n)
                @f$ if propagate_down[0] == true.
             -# @f$ (N \times C \times H \times W) @f$
                the targets @f$ y @f$; Backward fills their diff with gradients
                @f$ \frac{\partial E}{\partial y} = 
                    \frac{1}{n} \sum\limits_{n=1}^N (y_n - \hat{y}_n)
                @f$ if propagate_down[1] == true.
            </param>
        </member>
        <member name="T:MyCaffe.layers.ExpLayer`1">
            <summary>
            The ExpLayer which computes the exponential of the input.
            This layer is initialized with the MyCaffe.param.ExpParameter.
            </summary>
            <remarks>
            Computes @f$ y = \gamma ^ {\alpha x + \beta} @f$,
            as specified by the scale @f$ \alpha @f$, shift @f$ \beta @f$, 
            and base @f$ \gamma @f$.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ExpLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ExpLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type EXP with parameter exp_param,
            with options:
                - scale (\b optional, default 1) the scale @f$ \alpha @f$
                
                - shift (\b optional, default 0) the shift @f$ \beta @f$ 
                
                - base (\b optional, default -1 for a value of @f$ e \approx 2.718 @f$) the base @f$ \gamma @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ExpLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ExpLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ExpLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the EXP value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="T:MyCaffe.layers.Im2colLayer`1">
            <summary>
            The Im2ColLayer is a helper layer for image operations that rearranges image regions into
            column vectors.  
            </summary>
            <remarks>
            Im2col operations are used by the ConvolutionLayer to perform convolution
            by matrix multiplication.
            
            @see [Fast ConvNets Using Group-wise Brain Damage](https://arxiv.org/abs/1506.02515v2) by Vadim Lebedev, and Victor Lempitsky, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.Im2colLayer`1.m_blobKernelShape">
            <summary>
            The spatial dimensions of a filter kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Im2colLayer`1.m_blobStride">
            <summary>
            The spatial dimensions of the stride.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Im2colLayer`1.m_blobPad">
            <summary>
            The spatial dimensions of the padding.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Im2colLayer`1.m_blobDilation">
            <summary>
            The spatial dimensions of the dilation.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The Im2col constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LayerParameter of type IM2COL.</param>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.Im2colLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Im2colLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: im2col.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the input.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (S \times C \times H \times W) @f$
                the im2col output 
            </param>
        </member>
        <member name="M:MyCaffe.layers.Im2colLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the forwarded inputs.
            </summary>
            <param name="colTop">top output Blob vecotr (length 1), 
            providing the error gradient with respect to the outputs.</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">input Blob vecotor (length 1), into which the top error
            gradient is copied.</param>
        </member>
        <member name="T:MyCaffe.layers.InnerProductLayer`1">
            <summary>
            The InnerProductLayer, also know as a 'fully-connected' layer, computes the inner product
            with a set of learned weights, and (optionally) adds biases.
            This layer is initialized with the MyCaffe.param.InnerProductParameter.
            </summary>
            <remarks>
            @see [Product-based Neural Networks for User Response Prediction](https://arxiv.org/abs/1611.00144) by Yanru Qu, Kan Cai, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang, 2016. 
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The InnerProductLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LayerParameter inner_product_param, with options:
              - num_output. The number of outputs.
              
              - bias_term (/b optional, default = true).  Whether or not to include bias.
              
              - weight_filler (/b optional, default = "gaussian").  The filler used to initialize the weights.
              
              - bias_filler (/b optional, default = "constant, 1.0").  The filler used to initialize the bias.
              
              - axis (/b optional, default = 1). The axis to be lumped into a single inner-product computation.
              
              - transpose (/b optional, default = false).  Whether or not to transpose the weight matrix or not.
            </param>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.InnerProductLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.InnerProductLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.InnerProductLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: ip
            </summary>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.ResetNoise">
            <summary>
            Resample the noise for both weights and bias (if used).
            </summary>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (N \times K \times 1 \times 1) @f$
                the computed inner product with the weights, where
                @f$ K @f$ equals <i>num_output</i>.
            </param>
        </member>
        <member name="M:MyCaffe.layers.InnerProductLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the inner product loss error gradient w.r.t the outputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (N \times K \times 1 \times 1) @f$, where @f$ K @f$ is equal to <i>num_output</i>.
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="T:MyCaffe.layers.Layer`1">
            <summary>
            An interface for the units of computation which can be composed into a Net.
            </summary>
            <remarks>
            Layer%s must implement an override to the forward function, in which they take their input (bottom) Blob%s
            (if any) and compute their output Blob%s (if any).  They may also implement aan override to the backward function,
            in which they compute the error gradients with respect to their input Blob's, given the error 
            gradients with their output Blob%s.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_type">
            <summary>
            Specifies the Layer type.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_cuda">
            <summary>
            Specifies the CudaDnn connection to Cuda.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_log">
            <summary>
            Specifies the Log for output.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_param">
            <summary>
            Specifies the LayerParameter describing the Layer.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_phase">
            <summary>
            Specifies the Phase under which the Layer is run.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_colBlobs">
            <summary>
            Specifies the learnable parameter Blobs of the Layer.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_rgbParamPropagateDown">
            <summary>
            Specifies whether or not to compute the learnable diff of each parameter Blob.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_rgLoss">
            <summary>
            Specifies the loss values that indeicate whether each top (output) Blob has a non-zero
            weight in the objective function..
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_tOne">
            <summary>
            Specifies a generic type equal to 1.0.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_tZero">
            <summary>
            Specifies a generic type equal to 0.0.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_bEnablePassthrough">
            <summary>
            Enables/disables the pass-through mode for the layer.  Default = <i>false</i>.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_bUseHalfSize">
            <summary>
            Specifies that the half size of the top (if any) should be converted to the base size.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_bConvertTopOnFwd">
            <summary>
            Specifies whether or not the layer should convert the top on the forward pass when using half sized memory (typically only done with input data).
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_bConvertTopOnBwd">
            <summary>
            Specifies whether or not to convert the top on the backward pass when using half sized memory (typically not done on loss layers).
            </summary>
        </member>
        <member name="F:MyCaffe.layers.Layer`1.m_bConvertBottom">
            <summary>
            Specifies whether or not the layer should convert the bottom when using half sized memory.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.Layer`1.OnGetWorkspace">
            <summary>
            Specifies the OnGetWorkspace event that fires when the getWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.Layer`1.OnSetWorkspace">
            <summary>
            Specifies the OnSetWorkspace event that fires when the setWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.Layer`1.OnGetIteration">
            <summary>
            Specifies the OnGetIteration event that fires when a layer needs to get the current iteration from the solver.
            </summary>
        </member>
        <member name="E:MyCaffe.layers.Layer`1.OnDebug">
            <summary>
            Specifies the OnGetWorkBlob event that is only supported when debugging to get a work
            blob from the primary Net holding this layer.
            </summary>
            <remarks>
            When implemented, this event causes a nan/inf check at the end of each forward and backward pass
            and is only recommended use during debugging.</remarks>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The Layer constructor.
            </summary>
            <remarks>
            Setup code for derivative classes should go into an override of the LayerSetup function where the 
            dimensionsn of the Blob%s are provided to the Layer.
            </remarks>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter that contains the settings of the Layer.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Dispose">
            <summary>
            Releases all GPU and host resources used by the Layer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.dispose">
            <summary>
            Releases all GPU and host resources used by the Layer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.getCurrentIteration">
            <summary>
            Fires the OnGetIteration event to query the current iteration.
            </summary>
            <returns>The GetIterationArgs is returned if the event is connected, otherwise <i>null</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.SetPhase(MyCaffe.basecode.Phase)">
            <summary>
            Changes the layer's Phase to the one specified.
            </summary>
            <param name="phase">Specifies the new Phase for the layer.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Setup(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Implements common Layer setup functionality.
            </summary>
            <remarks>
            Checks that the number of bottom and top blobs are correct.  Calls LayerSetup to do Layer specific
            setup for each layer type, followed by Reshape to setup the sizes of the top Blobs and internal
            buffers.  Shes up the loss weight multiplier blobs for any non-zero loss weights.
            </remarks>
            <param name="colBottom">Specifies the collection of preshaped bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of allocated but unshaped top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Performs Layer specific setup.  Derived layers should override this function as well
            as the Reshape function.
            </summary>
            <remarks>
            This method should perform one-time Layer specific setup.  This may include reading
            and processing relevant parameters from teh <code>layer_param</code>.
            Setting up the shapes of top (output) blobs and internal buffers should be done in the
            <code>Reshape</code> function, which will be called before the Forward pass to 
            adjust the top (input) Blob sizes.
            </remarks>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs to this Layer.</param>
            <param name="colTop">Specifies the collection of allocated but unshaped top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.SetNetParameterUsed(MyCaffe.param.NetParameter)">
            <summary>
            This function allows other layers to gather needed information from the NetParameters if any, and is called when initialzing the Net.
            </summary>
            <param name="np">Specifies the NetParameter.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Adjust the shapes of top blobs and internal buffers to accomodate the shapes
            of the bottom blobs.
            </summary>
            <remarks>
            This method should reshape top blobs as needed according to the shapes of the bottom (input) Blob%s,
            as well as reshaping any internal buffers and making any other necessary adjustments so that the layer
            can accomodate the bottom (input) Blobs.</remarks>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs, with requested input shapes.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs, which should be reshaped as needed by the Layer.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convert_to_full(System.Int32,System.Int64)">
            <summary>
            Convert half memory to full memory.
            </summary>
            <param name="nCount">Specifies the number of items.</param>
            <param name="hMem">Specifies the memory to convert.</param>
            <returns>A handle to the converted memory is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convert(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Convert a collection of blobs from / to half size.
            </summary>
            <param name="col">Specifies the collection to convert.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.ConvertToBase(MyCaffe.common.BlobCollection{`0})">
            <summary>
            ConvertToBase converts any blobs in a collection that are in half size to the base size.
            </summary>
            <param name="col">Specifies the blob collection to convert.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Given the bottom (input) Blobs, this function computes the top (output) Blobs and the loss.
            </summary>
            <remarks>
            The Forward function calls the overriden forward function implemented by each specific Layer derivative 
            to compute the top (output) Blob's values given the bottom (input) Blobs.  If the layer has any non-zero
            <code>loss_weights</code> this function then computes and returns the loss.
            </remarks>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs, whos data fields
            store the input data for this layers' outputs.</param>
            <param name="colTop">Specifies the collection of preshaped top (output) Blobs, whos data fields
            will store this layers' outputs.</param>
            <returns>Returns the total loss from the Layer.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            This forward abstract function must be overriden by each derived Layer class to compute the 
            top (output) Blobs for this layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs, whos data fields
            store the input data for this layers' outputs.</param>
            <param name="colTop">Specifies the collection of preshaped top (output) Blobs, whos data fields
            will store this layers' outputs.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Given the top Blob error gradients, compute the bottom Blob error gradients.
            </summary>
            <remarks>
            The Backward function calls the overriden backward function implemented by each specific Layer derivative,
            to compute the bottom (input) Blob diffs given the top (output) Blob diffs.
            </remarks>
            <param name="colTop">Specifies a collection of top (output) Blobs, whos diff fields store the gradient of the
            error with respect to themselves.</param>
            <param name="rgbPropagateDown">Specifies a List with equal length to the bottom, with each element
            indicating whether or not to propagate the error gradients down to the bottom Blob at the corresponding
            index.</param>
            <param name="colBottom">Specifies a collection of bottom (input) Blobs, whos diff fields are filled with
            the gradient of the error with respect to themselves after the Backward function is run.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            This backward abstract function must be overriden by each derived Layer class to compute the 
            bottom (intput) Blob diffs for this Layer.
            </summary>
            <param name="colTop">Specifies a collection of top (output) Blobs, whos diff fields store the gradient of the
            error with respect to themselves.</param>
            <param name="rgbPropagateDown">Specifies a List with equal length to the bottom, with each element
            indicating whether or not to propagate the error gradients down to the bottom Blob at the corresponding
            index.</param>
            <param name="colBottom">Specifies a collection of bottom (input) Blobs, whos diff fields are filled with
            the gradient of the error with respect to themselves after the Backward function is run.</param>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.blobs">
            <summary>
            Returns the collection of learnable parameter Blobs for the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.internal_blobs">
            <summary>
            Returns the collection of internal Blobs used by the Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.layer_param">
            <summary>
            Returns the LayerParameter for this Layer.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.loss(System.Int32)">
            <summary>
            Returns the scalar loss associated with the top Blob at a given index.
            </summary>
            <param name="nTopIdx">Specifies the index.</param>
            <returns>The loss value is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.set_loss(System.Int32,System.Double)">
            <summary>
            Sets the loss associated with a top Blob at a given index.
            </summary>
            <param name="nTopIdx">Specifies the index.</param>
            <param name="dfLoss">Specifies the loss value.</param>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.type">
            <summary>
            Returns the LayerType of this Layer.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of bottom (input) Blobs required by the Layer, 
            or -1 if no exact number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects an exact number of bottom (input) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of bottom (input) Blobs required by the Layer,
            or -1 if no minimum number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects a minimum number of bottom (input) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of bottom (input) Blobs required by the Layer,
            or -1 if no maximum number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects a maximum number of bottom (input) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of top (output) Blobs required by the Layer, 
            or -1 if no exact number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects an exact number of top (output) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of top (output) Blobs required by the Layer,
            or -1 if no minimum number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects a minimum number of top (output) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of top (output) Blobs required by the Layer,
            or -1 if no maximum number is required.
            </summary>
            <remarks>
            This method should be overriden to return a non-negative value if your Layer
            expects a maximum number of top (output) Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.EqualNumBottomTopBlobs">
            <summary>
            Returns <i>true</i> if the Layer requires and equal number of bottom (input) and
            top (output) Blobs.
            </summary>
            <remarks>
            This method should be overriden to return <i>ture</i> if your Layer expects an
            equal number of bottom and top Blobs.
            </remarks>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.AutoTopBlobs">
            <summary>
            Return whether "anonymous" top (output) Blobs are created automatically by the
            Layer.
            </summary>
            <remarks>
            If this method returns <i>true</i>, Net::Init will create enough "anonymous" top
            Blobs to fulfill the requirement specified by ExactNumTopBlobs() or MinTopBlobs().
            </remarks>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.AllowForceBackward(System.Int32)">
            <summary>
            Return whether to allow <code>force_backward</code> for a given bottom (input) Blob
            index.
            </summary>
            <remarks>
            If AllowForceBackward(i) == <i>false</i>, the <code>force_backward</code> setting will
            be ignored and backpropagate to Blob i only if it needs gradient information. (as is done
            when <code>force_backward == false</code>
            </remarks>
            <param name="nBottomIdx">Specifies the index of the bottom (input) item to force.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.param_propagate_down(System.Int32)">
            <summary>
            Returns whether or not the Layer should compute gradients w.r.t. a
            parameter at a particular index given by a parameter index.
            </summary>
            <param name="nParamIdx">Specifies the parameter index.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.set_param_propagate_down(System.Int32,System.Boolean)">
            <summary>
            Sets whether or not the Layer should compute gradients w.r.t. a
            parameter at a particular index given by a parameter index.
            </summary>
            <param name="nParamIdx">Specifies the index.</param>
            <param name="bPropagate">Specifies whether or not to progagate down the parameter.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.CheckBlobCounts(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Called by the Layer::Setup function to check the number of bottom (input) and top (output) Blobs
            provided match  the expected number of blobs expected via the {EactNum,Min,Max}{Bottom,Top}Blobs
            functions.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.SetLossWeights(MyCaffe.common.BlobCollection{`0})">
            <summary>
            Called by Layer::Setup to initialize the weights associated with any top (output) Blobs
            in the loss function ans store non-zero loss weights in the diff Blob.
            </summary>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.shareParameter(MyCaffe.common.Blob{`0},System.Collections.Generic.List{System.Int32})">
            <summary>
            Attempts to share a parameter Blob if another parameter Blob with the same name and accpetable size is found.
            </summary>
            <param name="b">Specifies the Blob to share.</param>
            <param name="rgMinShape">Specifies the minimum shape requried to share.</param>
            <returns>If the Blob is shared, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.shareLayerBlob(MyCaffe.common.Blob{`0},System.Collections.Generic.List{System.Int32})">
            <summary>
            Attempts to share a Layer Blob if another parameter Blob with the same name and acceptable size is found.
            </summary>
            <param name="b">Specifies the Blob to share.</param>
            <param name="rgMinShape">Specifies the minimum shape requried to share.</param>
            <returns>If the Blob is shared, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.shareLayerBlobs(MyCaffe.layers.Layer{`0})">
            <summary>
            Attempts to share the Layer blobs and internal_blobs with matching names and sizes with those in another matching layer.
            </summary>
            <param name="layer">Specifies the layer who will use the shared blobs and internal blobs from the shared layer.</param>
            <returns>If the layer blobs and internal blobs are shared successfully <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.forward_timing">
            <summary>
            Returns the timing of the last forward pass in milliseconds.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.forward_timing_average">
            <summary>
            Returns the average timing of the forward passes in milliseconds.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.backward_timing">
            <summary>
            Returns the timing of the last backward pass in milliseconds.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.Layer`1.backward_timing_average">
            <summary>
            Returns the average timing of the backward passes in milliseconds.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.SetEnablePassthrough(System.Boolean)">
            <summary>
            Enables/disables the pass-through mode.
            </summary>
            <remarks>
            When enabled, the forward pass merely compies the bottom inputs to the top outputs and returns.
            </remarks>
            <param name="bEnable">Enable/disable the pass-through mode.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.getWorkspace">
            <summary>
            Returns the WorkspaceArgs used to share a workspace between Layers.
            </summary>
            <returns>The WorkspaceArgs are returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.setWorkspace(System.UInt64)">
            <summary>
            Sets the workspace size (in items) and returns <i>true</i> if set, <i>false</i> otherwise.
            </summary>
            <param name="lSize"></param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.check_nan(MyCaffe.common.Blob{`0})">
            <summary>
            Checks a Blob for NaNs and throws an exception if found.
            </summary>
            <param name="b">Specifies the Blob to check.</param>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convert(System.Double)">
            <summary>
            Converts a <i>double</i> to a generic.
            </summary>
            <param name="df">Specifies the <i>double</i> value.</param>
            <returns>Returns the generic value.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convertD(`0)">
            <summary>
            Converts a generic to a <i>double</i> value.
            </summary>
            <param name="df">Specifies the generic value.</param>
            <returns>The <i>double</i> value is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convertF(`0)">
            <summary>
            Converts a generic to a <i>float</i> value.
            </summary>
            <param name="df">Specifies the generic value.</param>
            <returns>The <i>float</i> value is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convertD(`0[])">
            <summary>
            Converts an array of generic values into an array of <i>double</i> values.
            </summary>
            <param name="rg">Specifies the array of generic values.</param>
            <returns>The array of <i>double</i> values is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convert(System.Double[])">
            <summary>
            Converts an array of <i>double</i> values into an array of generic values.
            </summary>
            <param name="rg">Specifies the array of <i>double</i> values.</param>
            <returns>Returns an array of generic values.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convertF(`0[])">
            <summary>
            Converts an array of <i>float</i> values into an array of generic values.
            </summary>
            <param name="rg">Specifies the array of <i>float</i> values.</param>
            <returns>Returns an array of generic values.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.convert(System.Single[])">
            <summary>
            Converts an array of <i>float</i> values into an array of generic values.
            </summary>
            <param name="rg">Specifies the array of <i>float</i> values.</param>
            <returns>Returns an array of generic values.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.val_at(`0[],System.Int32)">
            <summary>
            Returns the integer value at a given index in a generic array.
            </summary>
            <param name="rg">Specifies the generic array.</param>
            <param name="nIdx">Specifies the index.</param>
            <returns>The value at the index is returned as an integer.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.size_at(MyCaffe.common.Blob{`0})">
            <summary>
            Returns the Size of a given two element Blob, such as one that stores Blob size information.
            </summary>
            <param name="b">Specifies the Blob.</param>
            <returns>The height and width are returned in a Size object.</returns>
        </member>
        <member name="M:MyCaffe.layers.Layer`1.Create(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.TransferInput)">
            <summary>
            Create a new Layer based on the LayerParameter.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter that contains the LayerType to create.</param>
            <param name="evtCancel">Specifies the CancelEvent used by some Layers when created.</param>
            <param name="imgDb">Optionally, specifies the CaffeImageDatabase used by data Layers.</param>
            <param name="trxinput">Optionally, specifies the transfer input object used by some of the data Layers.</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.layers.LayerParameterEx`1">
            <summary>
            The LayerParameterEx class is used when sharing another Net to conserve GPU memory and
            extends the LayerParameter with shared Blobs for this purpose.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LayerParameterEx`1.#ctor(MyCaffe.param.LayerParameter,MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0},MyCaffe.layers.Layer{`0})">
            <summary>
            The LayerParameterEx constructor.
            </summary>
            <param name="p">Specifies the original LayerParameter that is wrapped.</param>
            <param name="colBlobs">Specifies the Net parameter Blobs to share.</param>
            <param name="colLayerBlobs">Specifies the Net layer Blobs to share.</param>
            <param name="sharedLayer">Specifies the shared Net layer matching this one that we are creating.</param>
        </member>
        <member name="P:MyCaffe.layers.LayerParameterEx`1.SharedLayer">
            <summary>
            Returns the layer in the shared Net that matches this one.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LayerParameterEx`1.SharedBlobs">
            <summary>
            Returns the shared parameter Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LayerParameterEx`1.SharedLayerBlobs">
            <summary>
            Returns the shared Layer Blobs.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LayerParameterEx`1.Clone(System.Boolean)">
            <summary>
            Creates and returns a new copy of this instance.
            </summary>
            <param name="bCloneBlobs">Specifies whether or not to clone (or just share) the shared Blobs.</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.layers.LogLayer`1">
            <summary>
            The LogLayer computes the log of the input.
            This layer is initialized with the MyCaffe.param.LogParameter.
            </summary>
            <remarks>
            Computes @f$ y = log_{\gamma}(\alpha x + \beta) @f$,
            as specified by the scale @f$ \alpha @f$, shift @f$ \beta @f$,
            and base @f$ \gamma @f$.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LogLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The LogLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LOG with parameter log_param,
            with options:
                - scale (\b optional, default 1) the scale @f$ \alpha @f$
                
                - shift (\b optional, default 0) the shift @f$ \beta @f$ 
                
                - base (\b optional, default -1 for a value of @f$ e \approx 2.718 @f$) the base @f$ \gamma @f$</param>
        </member>
        <member name="M:MyCaffe.layers.LogLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LogLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.LogLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the LOG value inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
        </member>
        <member name="T:MyCaffe.layers.LossLayer`1">
            <summary>
            The LossLayer provides an interface for Layer's that take two blobs as input -- usually
            (1) predictions and (2) ground-truth labels -- and output a 
            singleton blob representing the loss.
            This layer is initialized with the MyCaffe.param.LossParameter.
            </summary>
            <remarks>
            LossLayers are typically only capable of backpropagating to their first input
            -- the predictions.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.LossLayer`1.kLOG_THRESHOLD">
            <summary>
            Specifies the minimum threshold for loss values.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.LossLayer`1.m_bIgnoreLabels">
            <summary>
            Set to <i>true</i> when labels are to be ignored.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.LossLayer`1.m_normalization">
            <summary>
            Specifies the normalization mode used to normalize the loss.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.LossLayer`1.m_nOuterNum">
            <summary>
            Specifies the outer num, such as the batch count (e.g. count(0, axis)).  Each derivative class must set this value appropriately.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.LossLayer`1.m_nInnerNum">
            <summary>
            Specifies the inner num, such as the channel + height + width (e.g. count(axis + 1)).  Each derivative class must set this value appropriately.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The LossLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LOSS with parameter loss_param,
            with options:
                - ignore_label (\b optional, default null). When specified, instances with the label are ignored.
                
                - normalization. The normalization mode to use: FULL, VALID, or BATCH_SIZE. 
            </param>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.get_normalizer(MyCaffe.param.LossParameter.NormalizationMode,System.Int32)">
            <summary>
            Returns the normalizer used to normalize the loss.
            </summary>
            <param name="normalization_mode">Specifies the normalization mode to use.</param>
            <param name="nValidCount">Specifies the number of valid.</param>
            <returns>The normalization value is returned.</returns>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.GetNormalizer(MyCaffe.param.LossParameter.NormalizationMode,System.Int32,System.Int32,System.Int32)">
            <summary>
            Returns the normalizer used to normalize the loss.
            </summary>
            <param name="normalization_mode">Specifies the normalization mode to use.</param>
            <param name="nOuterNum">Specifies the outer number.</param>
            <param name="nInnerNum">Specifies the inner number.</param>
            <param name="nValidCount">Specifies the number of valid.</param>
            <returns>The normalization value is returned.</returns>
        </member>
        <member name="P:MyCaffe.layers.LossLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (intput) Blobs: prediction, label
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: loss
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LossLayer`1.AutoTopBlobs">
            <summary>
            For convenience and backwards compatibility, insturct the Net to 
            automatically allocate a single top Blob for LossLayers, into which
            they output their singleton loss, (even if the user didn't specify
            one in the prototxt, etc.).
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.AllowForceBackward(System.Int32)">
            <summary>
            We usually cannot backpropagate to the labels; ignore force_backward 
            for these inputs.
            </summary>
            <param name="nBottomIdx"></param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="T:MyCaffe.layers.LRNLayer`1">
            <summary>
            The "Local Response Normalization" LRNLayer is used to normalize the input in a local region across or within feature maps.
            This layer is initialized with the MyCaffe.param.LRNParameter.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov, 2012.
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The LRNLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LRN with parameter lrn_param,
            with options:
                - engine (\b optional, default Engine.CUDNN for ACROSS_CHANNELS, Engine.CAFFE for WITHIN_CHANNELS). The engine (Engine.CUDNN or Engine.CAFFE) to use.
                
                - norm (\b optional, default ACROSS_CHANNELS). The region to normalize 
                
                - local_size (\b optional, default 5). The local size of the normalization window.
                
                - alpha (\b optional, default 1e-4).  The alpha value used for variance scaling.  Note: cuDnn uses a default of 1e-4, whereas Caffe uses a default of 1.0.
                
                - beta (\b optional, default 0.75). The beta value used as the power parameter.
                
                - k (\b optional, default 2.0). The k value used during normalization.  Note: cuDnn uses a default of 2.0, whereas Caffe uses a default of 1.0.
            </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.LRNLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.LRNLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.LRNLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: lrn
            </summary>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer for both Engine.CUDNN and Engine.CAFFE modes.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation using either the Engine.CUDNN or Engine.CAFFE mode depending on the 
            <i>engine</i> parameter setting.
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                normalized outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the inputs using either the Engine.CUDNN or Engine.CAFFE mode depending on the 
            <i>engine</i> parameter setting.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation using the Engine.CAFFE mode.
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs.
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                normalized outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the inputs using the Engine.CAFFE mode.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$
                the error gradients.
            </param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation using the Engine.CUDNN mode.
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                normalized outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.LRNLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the inputs using the Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
        </member>
        <member name="T:MyCaffe.layers.LSTMSimpleLayer`1">
            <summary>
            The LSTMSimpleLayer is a simpe version of the long-short term memory layer.
            This layer is initialized with the MyCaffe.param.LSTMSimpleParameter.
            </summary>
            <remarks>
            See original implementation at: https://github.com/junhyukoh/caffe-lstm
            
            @see [A Clockwork RNN](https://arxiv.org/abs/1402.3511) by Jan Koutnik, Klaus Greff, Faustino Gomez, and Jrgen Schmidhuber, 2014.
            @see [Long short-term memory](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.7752) by Sepp Hochreiter and Jrgen Schmidhuber, 1997.
            @see [Learning to execute](https://arxiv.org/abs/1410.4615) by Wojciech Zaremba and Ilya Sutskever, 2014.
            @see [Generating sequences with recurrent neural networks](https://arxiv.org/abs/1308.0850) by Alex Graves, 2013.
            @see [Predictive Business Process Monitoring with LSTM Neural Networks](https://arxiv.org/abs/1612.02130) by Niek Tax, Ilya Verenich, Marcello La Rosa, and Marlon Dumas, 2016. 
            @see [Using LSTM recurrent neural networks for detecting anomalous behavior of LHC superconducting magnets](https://arxiv.org/abs/1611.06241) by Maciej Wielgosz, Andrzej Skocze, and Matej Mertik, 2016.
            @see [Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction](https://arxiv.org/abs/1608.05267v2) by Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, and Ferdous Sohel, 2016.
            </remarks>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The LSTMSimpleLayer constructor.
            </summary>
            <remarks>
            @see [A Clockwork RNN](https://arxiv.org/abs/1402.3511) by Koutnik, et al., 2014
            </remarks>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LSTM with parameter lstm_simple_param,
            with options:
              - num_output.  The dimension of the output -- must be explicitly set to non-zero.
              
              - clipping_threshold (/b optional, default = 0).  The gradient clipping threshold (0 = no clipping).
              
              - weight_filler (/b optional, default = "gaussian"). The weight filler used to initialize the weights.
              
              - bias_filler (/b optional, default = "constant, 1.0"). The bias filler used to initialize the bias values.
              
              - batch_size (/b optional, default = 1).  The batch size.
              
              - enable_clockwork_forget_bias (/b optional, default = false).  Whether or not to set the forget gat bias to 5.0 as recommended by [1] Koutnik, J., et al.
            </param>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.LSTMSimpleLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">inpub Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
            <param name="colTop">top output Blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.LSTMSimpleLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the inputs.
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient
            with respect to outputs
             -# @f$ (N \times C \times H \times W) @f$</param>
            <param name="rgbPropagateDown">propagate_down see Layer::Backward.</param>
            <param name="colBottom">bottom input blob vector (length 1)
             -# @f$ (N \times C \times H \times W) @f$
             </param>
        </member>
        <member name="T:MyCaffe.layers.NeuronLayer`1">
            <summary>
            The NeuronLayer is an interface for layers that take one blob as input (x) 
            and produce only equally-sized blob as output (y), where
            each element of the output depends only on the corresponding input
            element.
            </summary>
            <remarks>
            The NeuralLayer is the base class for all 'neuron' classes that alter the input data, but not its shape.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.NeuronLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The NeuronLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LossParameter for the NeuronLayer.  Parameters are defined by each derivative class.</param>
        </member>
        <member name="P:MyCaffe.layers.NeuronLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.NeuronLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: output.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.NeuronLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.NeuronLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the top (output) Blob to have the same shape as the bottom (input) Blob.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="T:MyCaffe.layers.HingeLossLayer`1">
            <summary>
            The HingeLossLayer computes the hinge loss for a one-of-many classification task.
            This layer is initialized with the MyCaffe.param.HingeLossParameter.
            </summary>
            <remarks>
            @see [CNN-based Patch Matching for Optical Flow with Thresholded Hinge Loss](https://arxiv.org/abs/1607.08064) by Christian Bailer, Kiran Varanasi, and Didier Stricker, 2016.
            @see [Hinge-Loss Markov Random Fields and Probabilistic Soft Logic](https://arxiv.org/abs/1505.04406) by Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.HingeLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The HingeLoss constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.HingeLossLayer`1.dispose">
            @copydoc LossLayer::dispose 
        </member>
        <member name="M:MyCaffe.layers.HingeLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ t @f$, a Blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score for each of
                the @f$ K = CHW @f$ classes.  In an SVM, @f$ t @f$ is the result of
                taking the inner product @f$ X^T W @f$ of the D-dimensional features
                @f$ X \in \mathcal{R}^{D \times N} @f$ and the learned hyperplane
                parameters @f$ W \in \mathcal{R}^{D \times K} @f$, so a Net with just
                an InnerProductLayer (with num_output = @f$ D @f$) providing predictions to a
                HingeLossLayer and no other learnable parameters or losses is
                equivalent to an SVM.
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels @f$ l @f$, an integer-valued Blob with values
                @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the @f$ K @f$ classes
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed hinge loss: @f$ E = 
                \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^K
                [\max(0, 1 - \delta\{l_n = k\} t_{nk})] ^ p @f$,
                for the @f$ L^p @f$ norm (defaults to @f$ p=1 @f$, the L1 norm; L2 norm, as in L2-SVM,
                is also available), and @f$
                \delta\{\mathrm{condition}\} = \left\{
                  \begin{array}{lr}
                    1 \: \mbox{if condition} \\
                   -1 \: \mbox{otherwise}
                  \end{array} \right. 
                @f$
            </param>
            <remarks>
            In an SVM, @f$ t\in \mathcal{R}^{N x K} @f$ is the result of taking
            the inner product @f$ X^T W @f$ of the features
            @f$ X \in \mathcal{R}^{D x N} @f$
            and the learned hyperplane parameters
            @f$ W \in \mathcal{R}^{D x K} @f$.  So, a Net with just an
            InnerProductLayer (with num_output = @f$ k @f$) providing predictions to a
            HingeLossLayer is equivalent to an SVM (assuming it has no other learned
            outside the InnerProductLayer and no other losses outside the
            HingeLossLayer.
            </remarks>  
        </member>
        <member name="M:MyCaffe.layers.HingeLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the hinge loss error gradient w.r.t the predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the label inputs (bottom[1]),
            so this method ignores bottom[1] and requires !propagate_down[1], crashing
            if propagate_down[1] == true.
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * @f$ \lambda @f$ as
                 @f$ \lambda @f$ is the coefficient of this layer's output
                 @f$ \ell_i @f$ in the overall Net loss.
                 @f$ E = \lambda_i \ell_i + \mbox{other loss terms} @f$; hence
                 @f$ \frac{partial E}{\partial \ell_i} = \lambda_i @f$
                   (*Assuming that this top blob is not used as a bottom (input) by any
                   other layer of the Net.)
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false as
            we can't compute gradients with respect to the labels.</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; backward computes diff
                  @f$ \frac{\partial E}{\partial x} @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
            </param>
        </member>
        <member name="T:MyCaffe.layers.PoolingLayer`1">
            <summary>
            The PoolingLayer pools the input image by taking the max, average, etc. within regions.
            This layer is initialized with the MyCaffe.param.PoolingParameter.
            </summary>
            <remarks>
            @see [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) by Vincent Dumoulin and Francesco Visin, 2016.
            @see [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150) by Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba, 2015.
            @see [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner, 1998.
            </remarks>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The PoolingLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">
            Provides PoolingParameter pooling_param with PoolingLayer options:
             - num_output. The number of filters.
             
             - kernel_size / kernel_h / kernel_w.  The pooling dimensions, given by
             kernel_size for square pooling or kernel_h and kernel-w for rectangular 
             pooling.
             
             - stride / stride_h / stride_w. (\b optional, default 1).  The pool
             stride, given by stride_size for equal dimensions of stride_h and stride_w
             for different strides.  By default the pool is dense with stride 1.
             
             - pad / pad_h / pad_w. (\b optional, default 0). The zero-padding for
             pooling, given by pad for equal dimensions or pad_h and pad_w for
             different padding.  Input padding is computed implicitly instead of 
             actual padding.
             
             - global_pooling (\b optional, default, false). Whether to use global
             pooling or not.
             
             - engine: convolution has Engine.CAFFE (matrix multiplication) and Engine.CUDNN (library
             kernels + stream parallelism) engines.
            </param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.PoolingLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.PoolingLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the required number of bottom (input) Blobs: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.PoolingLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the required number of top (output) Blobs: pool, mask (Engine.CAFFE only)
            </summary>
        </member>
        <member name="P:MyCaffe.layers.PoolingLayer`1.MinTopBlobs">
            <summary>
            Currentlym Engine.CUDNN does not support the extra top blob.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.PoolingLayer`1.MaxTopBlobs">
            <summary>
            MAX Pool layers can output an extra top blob for the mask;
            others can only output the pooled inputs.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer for use with both Engine.CAFFE and Engine.CUDNN modes.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using either the Engine.CAFFE or Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine.CAFFE mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
            <param name="colTop">top output Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine.CAFFE mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Forward computation using the Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PoolingLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Run the Backward computation using the Engine.CUDNN mode as specified in the LayerParameter.
            </summary>
            <param name="colTop">top output Blob vector (length 1).</param>
            <param name="rgbPropagateDown">see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (length 1).</param>
        </member>
        <member name="T:MyCaffe.layers.PowerLayer`1">
            <summary>
            The PowerLayer computes the power of the input.
            This layer is initialized with the MyCaffe.param.PowerParameter.
            </summary>
            <remarks>
            Computes @f$ y = (\alpha x + \beta) ^ \gamma @f$
            as specified by the scale @f$ \alpha @f$, shift @f$ \beta @f$, 
            and power @f$ \gamma @f$.
            
            @see [Optimizing a Shallow Multi-Scale Network for Tiny-Imagenet Classification](http://cs231n.stanford.edu/reports/2015/pdfs/dashb_CS231n_Paper.pdf) by Dash Bodington, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.PowerLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The PowerLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LOG with parameter power_param,
            with options:
                - scale (\b optional, default 1) the scale @f$ \alpha @f$
                
                - shift (\b optional, default 0) the shift @f$ \beta @f$ 
                
                - power (\b optional, default 1) the power @f$ \gamma @f$</param>
        </member>
        <member name="M:MyCaffe.layers.PowerLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PowerLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$.
            </param>
            <param name="colTop">top output blob (length 1)
             -# @f$ (n \times C \times H \times W) @f$
                the computed outputs @f$
                  y = (\alpha x + \beta) ^ \gamma
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.PowerLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the power inputs
            </summary>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs
             -# @f$ (N \times C \times H \times W) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.</param>
            <param name="colBottom">the input blob (length 1)
             -# @f$ (N \times C \times H \times W) @f$
                the inputs @f$ x @f$; backward fills their diff with
                gradients @f$
                  \frac{\partial E}{\partial y} =
                     \frac{partial E}{\partial y}
                     \alpha \gamma (\alpha x + \beta) ^ {\gamma - 1} =
                     \frac{\partial E}{\partial y}
                     \frac{\alpha \gamma y}{\alpha x + \beta}
                  @f$ if propagate_down[0] == true.
            </param>
        </member>
        <member name="T:MyCaffe.layers.PReLULayer`1">
            <summary>
            The PReLULayer computes the "Parameterized Rectified Linear Unit" non-linearity.
            This layer is initialized with the MyCaffe.param.PReLUParameter.
            </summary>
            <remarks>
            Computation: @f$ y = max(0,x_i) + a_i min(0, x_i) @f$
            <br/>
            The differences from ReLULayer are 1.) netative slopes are
            learnable through backprop and 2) negative slopes can vary across
            channels.  The number of axes of input blob should be greater than or
            equal to 2.  The 1st axis (0-based) is seen as channels.
            <br/>
            @see [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853) by Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li, 2015.
            @see [Revise Saturated Activation Functions](https://arxiv.org/abs/1602.05980?context=cs) by Bing Xu, Ruitong Huang, and Mu Li, 2016.
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee, 2016.
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary> 
            The PReLULayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type PRELU with parameter prelu_param,
            with options:
                - filler (/b optional, default = "constant", 0.25).  The filler used to fill the learnable parameters.
                
                - channel_shared (\b optional, default false). Whether or not slope parameters are shared across channels. 
            </param>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.PReLULayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward operation
            </summary>
            <param name="colBottom">
            bottom input Blob vector (length 1)
                -# @f$ (N \times C \times ...) @f$
                the inputs @f$ x @f$
            </param>
            <param name="colTop">
            top output Blob vector (length 1)
                -# @f$ (N \times C \times ...) @f$
                the computed outputs for each channel @f$ i @f$
                @f$ y_i = max(0, x_i) + a_i min(0, x_i) @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.PReLULayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the PReLU inputs.
            </summary>
            <param name="colTop">
            top output Blob vector (length 1), providing the error gradient with
            respect to the outputs.
                -# @f$ (N \times C \times ...) @f$
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.
            </param>
            <param name="rgbPropagateDown">
            see Layer::backward.
            </param>
            <param name="colBottom">
            bottom input Blob vector (length 1)
                -# @f$ (N \times C \times ...) @f$
                the inputs @f$ x @f$; For each channel @f$ i @f$ backward fills their
                diff with gradients @f$
                    \frac{\partial E}{\partial x_i} = \left\{
                    \begin{array}{lr}
                        a_i \frac{\partial E}{\partial y_i} \: \mathrm{if} \; x_i \le 0 \\
                        \frac{\partial E}{\partial y_i} \: \mathrm{if} \; x_i > 0
                    \end{array} \right.
                @f$
                If param_propagate_down[0] == true, it fills the diff with gradients
                @f$
                    \frac{\partial E}{\partial a_i} = \left\{
                    \begin{array}{lr}
                        \sum_{x_i} x_i \frac{\partial E}{\partial y_i} \: \mathrm{if} \; x_i \le 0 \\
                        0 \: \mathrm{if} \; x_i > 0
                    \end{array} \right.
                @f$.
            </param>
        </member>
        <member name="T:MyCaffe.layers.ReductionLayer`1">
            <summary>
            The ReductionLayer computes the 'reductions' -- operations that return a scalar output Blob
            for an input Blob of arbitrary size, such as the sum, absolute sum,
            and sum of squares.
            This layer is initialized with the MyCaffe.param.ReductionParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_op">
            <summary>
            the reduction operation performed by the layer.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_dfCoeff">
            <summary>
            a scalar coefficient applied to all outputs.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_nAxis">
            <summary>
            the index of the first input axis to reduce.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_nNum">
            <summary>
            the number of reductions performed.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_nDim">
            <summary>
            the input size of each reduction.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.ReductionLayer`1.m_blobSumMultiplier">
            <summary>
            a helper Blob used for summation (op_ == SUM)
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ReductionLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type REDUCTION with parameter reduction_param, 
            with options:
              - operation. The operation (SUM, ASUM, SUMSQ or MEAN) to run.
            
              - axis (\b optional, default = 0). The first axis to reduce to scalar.
              
              - coeff (\b optional, default = 1).  The coefficient used to scale the output.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.ReductionLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.ReductionLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ReductionLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: reduction
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward operation
            </summary>
            <param name="colBottom">
            bottom input Blob vector (length 1)
                -# @f$ (N \times C \times ...) @f$
                the inputs @f$ x @f$
            </param>
            <param name="colTop">
            top output Blob vector (length 1)
                -# (Shape depends on <i>axis</i> parameter setting)
                the computed outputs.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReductionLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t. the Reduction inputs.
            </summary>
            <param name="colTop">
            top output Blob vector (length 1), providing the error gradient with
            respect to the outputs.
                -# (Shape depends on <i>axis</i> parameter setting) 
            </param>
            <param name="rgbPropagateDown">
            see Layer::backward.
            </param>
            <param name="colBottom">
            bottom input Blob vector (length 1)
                -# @f$ (N \times C \times ...) @f$
                the inputs. 
            </param>
        </member>
        <member name="T:MyCaffe.layers.ReLULayer`1">
            <summary>
            The ReLULayer computes the "Rectifier Linear Unit" ReLULayer non-linearity, a classic for neural networks.
            This layer is initialized with the MyCaffe.param.ReLUParameter.
            </summary>
            <remarks>
            Computation: @f$ y = (1 + \exp(-x))^{-1} @f$
            <br/>
            Note that the gradient vanishes as the values move away from 0.
            The ReLULayer is often a better choice for this reason.
            <br/>
            @see [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853) by Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li, 2015.
            @see [Revise Saturated Activation Functions](https://arxiv.org/abs/1602.05980?context=cs) by Bing Xu, Ruitong Huang, and Mu Li, 2016.
            @see [Rectifier Nonlinearities Improve Neural Network Acoustic Models](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf) by Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng, 2013.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ReLULayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type RELU with parameter relu_param,
            with options:
              - engine. The engine to use, either Engine.CAFFE, or Engine.CUDNN.
              
              - negative_slope (/b optional, default = 0).  The negative slope.  Allow non-zero slope for negative inputs to speed up optimization. 
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                    y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CAFFE mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                    y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using the Engine.CAFFE mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                    y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.ReLULayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using the Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="T:MyCaffe.layers.RecurrentLayer`1">
            <summary>
            The RecurrentLayer is an abstract class for implementing recurrent behavior inside of an
            unrolled newtork.  This layer type cannot be instantiated -- instead,
            you should use one of teh implementations which defines the recurrent
            architecture, such as RNNLayer or LSTMLayer.
            This layer is initialized with the MyCaffe.param.RecurrentParameter.
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_unrolledNet">
            <summary>
            A Net to implement the Recurrent functionality.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_nN">
            <summary>
            The number of independent streams to process simultaneously.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_nT">
            <summary>
            The number of timesteps in the layer's input, and the number of 
            timesteps over which to backpropagate through time.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_bStaticInput">
            <summary>
            Whether the layer has a 'static' input copies across all timesteps.
            </summary>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_nLastLayerIndex">
            <summary>
            The last layer to run in the network.  (Any later layers are losses
            added to force the recurrent net to do backprop.)
            </summary>
        </member>
        <member name="F:MyCaffe.layers.RecurrentLayer`1.m_bExposeHidden">
            <summary>
            Whether the layer's hidden state at the first and last timesteps
            are layer inputs and outputs, respectively.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent)">
            <summary>
            The RecurrentLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LSTM or RNN.
            </param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel training operations.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.Reset">
            <summary>
            Reset the hidden state of the net by zeroing out all recurrent outputs.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.RecurrentLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.RecurrentLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of required bottom (input) Blobs: min+1
            </summary>
        </member>
        <member name="P:MyCaffe.layers.RecurrentLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.AllowForceBackward(System.Int32)">
            <summary>
            Returns <i>true</i> for all but the bottom index = 1, for you can't propagate to the sequence continuation indicators.
            </summary>
            <param name="nBottomIdx">Specifies the bottom index.</param>
            <returns>Returns whether or not to allow forced backward.</returns>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.FillUnrolledNet(MyCaffe.param.NetParameter)">
            <summary>
            Fills net_param with the recurrent network architecture.  Subclasses
            should define this -- see RNNLayer and LSTMLayer for examples.
            </summary>
            <param name="net_param">Specifies the net_param to be filled.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.RecurrentInputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills names with the names of the 0th timestep recurrent input
            Blob's.  Subclasses should define this -- see RNNlayer and LSTMLayer
            for examples.
            </summary>
            <param name="rgNames">Specifies the input names.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.RecurrentInputShapes(System.Collections.Generic.List{MyCaffe.param.BlobShape})">
            <summary>
            Fills shapes with the shapes of the recurrent input Blob's.
            Subclassses should define this -- see RNNLayer and LSTMLayer
            for examples.
            </summary>
            <param name="rgShapes">Specifies the shapes to be filled.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.RecurrentOutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills names with the names of the Tth timestep recurrent output
            Blob's.  Subclassses should define this -- see RNNLayer and LSTMLayer
            for examples.
            </summary>
            <param name="rgNames">Specifies the output names.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.OutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills names with the names of the output blobs, concatenated across
            all timesteps. Should return a name for each top Blob.
            Subclassses should define this -- see RNNLayer and LSTMLayer
            for examples.
            </summary>
            <param name="rgNames">Specifies the output names.</param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Peforms the forward calculation.
            </summary>
            <param name="colBottom">bottom input Blob vector (length 2-3)
            -# @f$ (T \times N \times ...) @f$
                the time-varying input @f$ x @f$. After the first two axes, whose
                dimensions must correspond to the number of timesteps @f$ T @f$ and
                the number of independent streams @f$ N @f$, respectively, its
                dimensions may be arbitrary.  Note that the ordering of dimensions --
                @f$ (T \times N \times ...) @f$, rather than
                @f$ (N \times T \times ...) @f$ -- means that the @f$ N @f$ independent input
                streams must be 'interleaved'.
                
            -# @f$ (T \times N) @f$
                the sequence continuation indicators @f$ \delta @f$. 
                These inputs should be binary (0 or 1) indicators, where
                @f$ \delta{t,n} = 0 @f$ means that timestep @f$ t @f$ of stream
                @f$ n @f$ is the beginning of a new sequence, and hence the previous
                hidden state @f$ h_{t-1} @f$ is mulitplied by @f$ \delta_t = 0 @f$ and
                has no effect on the cell's output at timestep 't', and 
                a value of @f$ \delta_{t,n} = 1 @f$ means that timestep @f$ t @f$ of
                stream @f$ n @f$ is a continuation from the previous timestep
                @f$ t-1 @f$, and the previous hidden state @f$ h_{t-1} @f$ affects the
                updated hidden state and output.
                
            -# @f$ (N \times ...) @f$ (optional)
                the static (non-time-varying) input @f$ x_{static} @f$.
                After the first axis, whose dimensions must be the number of
                independent streams, its dimensions must be the number of
                independent streams, its dimensions may be arbitrary.
                This is mathematically equivalent to using a time-varying input of
                @f$ x'_t = [x_t; x_{static}] @f$ -- i.e., tiling the static input
                across the 'T' timesteps and concatenating with the time-varying
                input.  Note that if this input is used, all timesteps in a single
                batch within a particular one of the @f$ N @f$ streams must share the
                same static input, even if the sequence continuation indicators
                suggest that difference sequences are ending and beginning within a 
                single batch.  This may require padding and/or truncation for uniform
                length.
            </param>
            <param name="colTop">top output Blob (length 1)
            -# @f$ (T \times N \times D) @f$
                the time-varying output @f$ y @f$, where @f$ d @f$ is
                <code>recurrent_param.num_output</code>.
                Refer to documentation for particular RecurrentLayer implementations
                (such as RNNLayer or LSTMLayer) for the definition of @f$ y @f$.
            </param>
        </member>
        <member name="M:MyCaffe.layers.RecurrentLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Backward computation.
            </summary>
            <param name="colTop">See 'foward' documetation.</param>
            <param name="rgbPropagateDown">Specifies whether or not to propagate down.</param>
            <param name="colBottom">See 'forward' documentation.</param>
        </member>
        <member name="T:MyCaffe.layers.LSTMLayer`1">
            <summary>
            The LSTMLayer processes sequential inputs using a 'Long Short-Term Memory' (LSTM)
            [1] style recurrent neural network (RNN). Implemented by unrolling
            the LSTM computation through time.
            This layer is initialized with the MyCaffe.param.RecurrentParameter.
            </summary>
            <remarks>
            The specific architecture used in this implementation is a described
            in "Learning to Execute" [2], reproduced below: 
            <code>
            i_t := sigmoid[ W_{hi} * h_{t-1} + W_{xi} * x_t + b_i ]
            f_t := sigmoid[ W_{hf} * h_{t-1} + W_{xf} * x_t + b_f ]
            o_t := sigmoid[ W_{ho} * h_{t-1} + W_{xo} * x_t + b_o ]
            g_t :=    tanh[ W_{hg} * h_{t-1} + W_{xg} * x_t + b_g ]
            c_t := (f_t .* c_{t-1} + (i_t .* g_t)
            h_t := o_t .* tanh[c_t]
            </code>
            In the implementation, the i, f, o, and g computation are preformed as a
            single inner product.
            
            Notably, this implementation lacks the 'diagonal' gates, as used in the
            LSTM architectures described by Alex Graves [3] and others.
            
            [1] Hochreiter, Sepp, and Schmidhuber, Jurgen. [Long short-term memory](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.7752).
                Neural Computation 9, no. 8 (1997): 1735-1780.
                
            [2] Zaremba, Wojciech, and Sutskever, Ilya. [Learning to execute](https://arxiv.org/abs/1410.4615).
                arXiv preprint arXiv: 1410.4615 (2014).
                
            [3] Graves, Alex. [Generating sequences with recurrent neural networks](https://arxiv.org/abs/1308.0850).
                arXiv preprint arXiv: 1308.0850 (2013).
                
            @see [A Clockwork RNN](https://arxiv.org/abs/1402.3511) by Jan Koutnik, Klaus Greff, Faustino Gomez, and Jrgen Schmidhuber, 2014.
            @see [Predictive Business Process Monitoring with LSTM Neural Networks](https://arxiv.org/abs/1612.02130) by Niek Tax, Ilya Verenich, Marcello La Rosa, and Marlon Dumas, 2016. 
            @see [Using LSTM recurrent neural networks for detecting anomalous behavior of LHC superconducting magnets](https://arxiv.org/abs/1611.06241) by Maciej Wielgosz, Andrzej Skocze, and Matej Mertik, 2016.
            @see [Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction](https://arxiv.org/abs/1608.05267v2) by Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, and Ferdous Sohel, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent)">
            <summary>
            The LSTMLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type LSTM with parameter recurrent_param,
            with options:
              - num_output.  The dimension of the output (and ususally hidden state) representation -- must be explicitly set to non-zero.
              
              - weight_filler (\b optional, default = "gaussian"). The weight filler used to initialize the weights.
              
              - bias_filler (\b optional, default = "constant, 1.0"). The bias filler used to initialize the bias values.
              
              - debug_info (\b optional, default = false). Whether or not to output extra debug information.
              
              - expose_hidden (\b optional, default = false).  Whether @f$ t @f$ add as additional bottom (inputs) the initial hidden state
                Blob%s, and add a additional top (output) the final timestep hidden state Blob%s.  The LSTM architecture adds
                2 additional Blob%s.
            </param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel training operations.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.RecurrentInputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with the names of the 0th timestep recurrent input Blobs.
            </summary>
            <param name="rgNames">Specifies the list of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.RecurrentOutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with names of the Tth timestep recurrent output Blobs.
            </summary>
            <param name="rgNames">Specifies the list of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.RecurrentInputShapes(System.Collections.Generic.List{MyCaffe.param.BlobShape})">
            <summary>
            Fill the <i>rgShapes</i> array with the shapes of the recurrent input Blobs.
            </summary>
            <param name="rgShapes">Specifies the array of BlobShape to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.OutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with  the names of the output
            Blobs, concatenated across all timesteps.
            </summary>
            <param name="rgNames">Specifies the array of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.LSTMLayer`1.FillUnrolledNet(MyCaffe.param.NetParameter)">
            <summary>
            Fills the NetParameter  with the LSTM network architecture.
            </summary>
            <param name="net_param"></param>
        </member>
        <member name="T:MyCaffe.layers.RNNLayer`1">
            <summary>
            The RNNLayer processes time-varying inputs using a simple recurrent neural network (RNN).  Implemented
            as a network unrolling the RNN computation in time.
            This layer is initialized with the MyCaffe.param.RecurrentParameter.
            </summary>
            <remarks>
            Given time-varying inputs @f$ x_t @f$, computes hidden state @f$
                h_t := \tanh[ W_{hh} h_{t_1} + W_{xh} x_t + b_h ]
            @f$, and outputs @f$
                o_t := \tanh[ W_{ho} h_t + b_o ]
            @f$.
            
            @see [Pixel Recurrent Neural Networks](https://arxiv.org/abs/1601.06759) by Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu, , 2016.
            @see [Bayesian Recurrent Neural Networks](https://arxiv.org/abs/1704.02798) by Meire Fotunato, Charles Blundell, and Oriol Vinyals, 2017. 
            @see [Higher Order Recurrent Neural Networks](https://arxiv.org/abs/1605.00064) by Rohollah Soltani and Hui Jiang, 2016.
            @see [Hierarchical Multiscale Recurrent Neural Networks](https://arxiv.org/abs/1609.01704) by Junyoung Chung, Sungjin Ahn, and Yoshua Bengio, 2016. 
            @see [Full Resolution Image Compression with Recurrent Neural Networks](https://arxiv.org/abs/1608.05148) by George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell, 2016.
            @see [ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks](https://arxiv.org/abs/1505.00393v3) by Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, and Yoshua Bengio, 2015.
            @see [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144v2) by Wu, et al., 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter,MyCaffe.basecode.CancelEvent)">
            <summary>
            The RNNLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type RNN with parameter recurrent_param,
            with options:
              - num_output.  The dimension of the output (and ususally hidden state) representation -- must be explicitly set to non-zero.
              
              - weight_filler (/b optional, default = "gaussian"). The weight filler used to initialize the weights.
              
              - bias_filler (/b optional, default = "constant, 1.0"). The bias filler used to initialize the bias values.
              
              - debug_info (/b optional, default = false). Whether or not to output extra debug information.
              
              - expose_hidden (/b optional, default = false).  Whether t add as additional bottom (inputs) the initial hidden state
                Blobs, and add a additional top (output) the final timestep hidden state Blobs.  The RNN architecture adds
                1 additional Blobs.
            </param>
            <param name="evtCancel">Specifies the CancelEvent used to cancel training operations.</param>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.RecurrentInputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with the names of the 0th timestep recurrent input Blobs.
            </summary>
            <param name="rgNames">Specifies the list of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.RecurrentOutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with names of the Tth timestep recurrent output Blobs.
            </summary>
            <param name="rgNames">Specifies the list of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.RecurrentInputShapes(System.Collections.Generic.List{MyCaffe.param.BlobShape})">
            <summary>
            Fill the <i>rgShapes</i> array with the shapes of the recurrent input Blobs.
            </summary>
            <param name="rgShapes">Specifies the array of BlobShape to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.OutputBlobNames(System.Collections.Generic.List{System.String})">
            <summary>
            Fills the <i>rgNames</i> array with  the names of the output
            Blobs, concatenated across all timesteps.
            </summary>
            <param name="rgNames">Specifies the array of names to fill.</param>
        </member>
        <member name="M:MyCaffe.layers.RNNLayer`1.FillUnrolledNet(MyCaffe.param.NetParameter)">
            <summary>
            Fills the NetParameter  with the RNN network architecture.
            </summary>
            <param name="net_param"></param>
        </member>
        <member name="T:MyCaffe.layers.ScaleLayer`1">
            <summary>
            The ScaleLayer computes the elementwise product of two input Blobs, with the shape of
            the latter Blob 'broadcast' to match the shape of the former.
            Equivalent to tiling the later Blob, then computing the elementwise
            product.  Note: for efficiency and convienience this layer can
            additionally perform a 'broadcast' sum too when 'bias_term: true'
            This layer is initialized with the MyCaffe.param.ScaleParameter.
            is set.
            </summary>
            <remarks>
            The latter, scale input may be omitted, in which case it's learned as 
            parameter of the layer (as in the bias, if it is included).
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The ScaleLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SCALE with parameter scale_param,
            with options:
              - bias_term (/b optional, default = false).  TWhether to also learn a bias (equivalent to a ScalarLayer + BiasLayer, but may be more efficient).
              
              - bias_filler (/b optional, default = "constant", 0.1). The filler used to initialize the bias values when <i>bias_term</i> = <i>true</i>.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.ScaleLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.ScaleLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs: firstfactor
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ScaleLayer`1.MaxBottomBlobs">
            <summary>
            Returns the maximum number of required bottom (input) Blobs: firstfactor, secondfactor
            </summary>
        </member>
        <member name="P:MyCaffe.layers.ScaleLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs: scale
            </summary>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.ReInitializeParameters">
            <summary>
            Re-initialize the parameters of the layer.
            </summary>
            <returns>When handled, this method returns <i>true</i>, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <remarks>
            In the below shape specifications, i denotes the value of the
            'axis' field given by 'this.layer_param.scale_param.axis', after
            canonicalization (i.e., conversion from negative to positive index,
            if applicable).
            </remarks>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (d_0 \times ... \times 
                 d_i \times ... d_j ... d_n) @f$
                 the first factor @f$ x @f$.
             -# @f$ (d_i \times d_j) @f$
                 the second factor @f$ y @f$.</param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (d_0 \times ... \times 
                 d_i \times ... \times d_j \times ... \times d_n) @f$
                 the product @f$ z = x y @f$ computed after 'broadcasting' @f$ y @f$.
                 Equivalent to tiling @f$ y @f$ to have the same shape as @f$ x @f$
                 then computing the elementwise product.
            </param>
        </member>
        <member name="M:MyCaffe.layers.ScaleLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 2)</param>
        </member>
        <member name="T:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1">
            <summary>
            The SigmoidCrossEntropyLayer computes the cross-entropy (logisitic) loss and is
            often used for predicting targets interpreted as probabilities.
            </summary>
            <remarks>
            Computation: @f$
                E = \frac{-1}{n} \sum\limits_{n=1}^N \left[
                        p_n \log \hat{p}_n +
                        (1 - p_n) \log(1 - \hat{p}_n)
                    \right]
                @f$
            <br/>
            This layer is implemented rather than separate 
            SigmoidLayer + CrossEntropyLayer as its gradient
            computation is more numerically stable.
            <br/>
            At test time, this layer can be replaced simply by a SigmoidLayer.
            
            @see [Large-Margin Softmax Loss for Convolutional Neural Networks](https://arxiv.org/abs/1612.02295) by Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang, 2016.
            @see [Information Dropout: Learning Optimal Representations Through Noisy Computation](https://arxiv.org/abs/1611.01353) by Alessandro Achille and Stefano Soatto, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SigmoidCrossEntropyLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SIGMOIDCROSSENTROPY_LOSS.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs as variable.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: loss.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of required top (output) Blobs: loss, loss values
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Forward computation.
            </summary>
            <param name="colBottom">bottom input blob (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the scores @f$ x \in [-\infty, +\infty] @f$,
                which this layer maps to probability predictions @f$
                \hat{p}_n = \sigma(x_n) \in [0,1]
                @f$
                using the sigmoid function @f$ \sigma(.) @f$ (see SigmoidLayer).
             -# @f$ (N \times C \times H \times W) @f$
                the targets @f$ y \in [0,1] @f$.
            </param>
            <param name="colTop">top output blob vector (length 1)
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                the computed cross-entropy loss: @f$
                  E = \frac{-1}{n} \sum\limits_{n=1}^N \left[
                          p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n)
                      \right]
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidCrossEntropyLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the sigmoid cross-entropy loss error gradient w.r.t. the 
            predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the target inputs (bottom[1]),
            so this method ignores bottom[1] and requires propagate_down[1] == false, 
            crashing otherwise.
            </remarks>
            <param name="colTop">top output blob (length 1), providing the error gradient with
            respect to the otuputs
             -# @f$ (1 \times 1 \times 1 \times 1) @f$
                This blob's diff will simply contain the loss_weight * @f$ \lambda @f$,
                as @f$ \lambda @f$ is the coefficient of this layer's output
                @f$ \ell_i @f$ in the overall Net loss @f$
                  E = \lambda_i \ell_i + \mbox{other loss terms} @f$; hence @f$
                  \frac{\partial E}{\partial \ell_i} = \lambda_i.
                  @f$
                  (*Assuming that this top blob is not used as a bottom (input) by any
                  other layer of the Net.)</param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false
            as gradient computation with respect to the targets is not implemented.
            </param>
            <param name="colBottom">input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; Backward computes diff @f$
                  \frac{\partial E}{\partial x} = 
                    \frac{1}{n} \sum\limits_{n=1}^N (\hat{p}_n - p_n)
                @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
            </param>
        </member>
        <member name="T:MyCaffe.layers.SigmoidLayer`1">
            <summary>
            The SigmoidLayer is a neuron layer that calculates the sigmoid function,
            a classc choice for neural networks.
            This layer is initialized with the MyCaffe.param.SigmoidParameter.
            </summary>
            <remarks>
            Computation: @f$ y = (1 + \exp(-x))^{-1} @f$
            <br/>
            Note that the gradient vanishes as the values move away from 0.
            The ReLULayer is often a better choice for this reason.
            
            @see [eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys](https://arxiv.org/abs/1702.08568v1) by Joshua Saxe and Konstantin Berlin, 2017. 
            @see [Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904v1) by Fei Wang, Mengquing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang, 2017.
            @see [Attention and Localization based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging](https://arxiv.org/abs/1703.06052v1) by Yong Xu, Qiuqiang Kong, Qiang Huang, Wenwu Wang, and Mark D. Plumbley, 2017.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SigmoidLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SIGMOID with parameter sigmoid_param,
            with options:
              - engine. The engine to use, either Engine.CAFFE, or Engine.CUDNN.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the ganh inputs using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs (y).</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CAFFE.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the ganh inputs using the Engine.CAFFE.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs (y).</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the computed outputs @f$
                y = (1 + \exp(-x))^{-1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.SigmoidLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the ganh inputs using the Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ containing error gradients @f$
                    \frac{\partial E}{\partial y}
                @f$
                with respect to computed outputs (y).</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs @f$ x @f$; 
                Backward fills their diff with gradients @f$
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y} y (1 - y)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="T:MyCaffe.layers.SilenceLayer`1">
            <summary>
            The SilenceLayer ignores bottom blobs while producing no top blobs.  (This is useuful
            to suppress output during testing.)
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SilenceLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SilenceLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SILENCE.</param>
        </member>
        <member name="P:MyCaffe.layers.SilenceLayer`1.MinBottomBlobs">
            <summary>
            Returns the minimum number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SilenceLayer`1.ExactNumTopBlobs">
            <summary>
            Returns 0 as this layer has no output.
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SilenceLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - no setup needed.
        </member>
        <member name="M:MyCaffe.layers.SilenceLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - no reshape needed.
        </member>
        <member name="M:MyCaffe.layers.SilenceLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            @brief Not implemented - no output.
        </member>
        <member name="M:MyCaffe.layers.SilenceLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The backward computation merely sets the bottom diff to zero.
            </summary>
            <param name="colTop">Not used.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
            </param>
        </member>
        <member name="T:MyCaffe.layers.SliceLayer`1">
            <summary>
            The SliceLayer takes a blob and slices it along either the num or channel dimensions
            outputting multiple sliced blob results.
            This layer is initialized with the MyCaffe.param.SliceParameter.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SliceLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SliceLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SLICE with parameter slice_param,
            with options:
              - axis (\b optional, default = 1). The axis along wich to slice.  By default the channel axis (1) is used.
              
              - slice_point (\b optional). The optional slice points.
            </param>
        </member>
        <member name="P:MyCaffe.layers.SliceLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SliceLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: slice
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SliceLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SliceLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SliceLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1+)
             -# @f$ (N \times C \times H \times W) @f$ 
                the first slice.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SliceLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs.
            </summary>
            <param name="colTop">top output Blob vector (Length 1+), providing the error gradient
            with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
            </param>
        </member>
        <member name="T:MyCaffe.layers.SoftmaxLossLayer`1">
            <summary>
            Computes the multinomial logistic loss for a one-of-many
            classification task, passing real-valued predictions through a
            softmax to get a probability distribution over classes.
            </summary>
            <remarks>
            This layer should be preferred over separate
            SofmaxLayer + MultinomialLogisticLossLayer
            as its gradient computation is more numerically stable.
            At test time, this layer can be replaced simply by a SofmaxLayer.
            
            @see [Large-Margin Softmax Loss for Convolutional Neural Networks](https://arxiv.org/abs/1612.02295) by Weiyang Liu, Yandong Wen, Zhiding Yu and Meng Yang, 2016. 
            @see [Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search](https://arxiv.org/abs/1611.05301v1) by Tu Bui, Leonardo Ribeiro, Moacir Ponti, and John Collomosse, 2016.
            </remarks>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            Constructor.
            </summary>
            <param name="cuda">Cuda engine.</param>
            <param name="log">General log.</param>
            <param name="p">provides LossParameter loss_param, with options:
             - ignore_label (optional)
               Specify a label value that whould be ignored when computing the loss.
             - normalize (optional, default true)
               If true, the loss is normalized by the number of (nonignored) labels
               present; otherwise the loss is imply summed over spatial locations.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLossLayer`1.internal_blobs">
            @copydoc Layer::internal_blobs 
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLossLayer`1.ExactNumTopBlobs">
            <summary>
            Returns the exact number of required top (output) Blobs as variable.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLossLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: loss.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SoftmaxLossLayer`1.MaxTopBlobs">
            <summary>
            Returns the maximum number of required top (output) Blobs: loss, labels
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            The forward computation.
            </summary>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$, a blob with values in
                @f$ [-\infty, +\infty] @f$ indicating the predicted score for eachy of
                the K = CHW classes.  This layer maps these scores to a
                probability distribution over classes using the softmax function @f$
                \hat{p}_{nk} = \exp(x_{nk}) /
                \left[\sum_{k'} \exp(x_{nk'})\right] @f$ (see SoftmaxLayer).
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels l, an integer valued blob with values @f$ l_n \in [0, 1, 2, ..., K-1] @f$
                indicating the correct class label among the K classes.</param>
            <param name="colTop">top output blob vector (length 1)
                the computed cross_entropy classification loss: @f$ E = 
                \frac{-1}{N} \sum\limits_{n=1}^N \log(\hat{p}_{n,l_n})
                @f$ for softmax output class probabilities @f$ \hat{p} @f$.</param>
        </member>
        <member name="M:MyCaffe.layers.SoftmaxLossLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the softmax loss error gradient w.r.t the predictions.
            </summary>
            <remarks>
            Gradients cannot be computed with respect to the label inputs (bottom[1]),
            so this method ignores bottom[1] and requires !propagate_down[1], crashing
            if propagate_down[1] == true.
            </remarks>
            <param name="colTop">top output blob vector (length 1), providing the error gradient with
            respect to the outputs.
              -# @f$ (1 \times 1 \times 1 \times 1) @f$
                 This blob's diff will simply contain the loss_weight * @f$ \lambda @f$ as
                 @f$ \lambda @f$ is the coefficient of this layer's output
                 @f$ \ell_i @f$ in the overall Net loss.
                 @f$ E = \lambda_i \ell_i + \mbox{other loss terms}; hence
                   \frac{partial E}{\partial \ell_i} = \lambda_i
                 @f$
                   (*Assuming that this top blob is not used as a bottom (input) by any
                   other layer of the Net.)
            </param>
            <param name="rgbPropagateDown">see Layer::Backward.  propagate_down[1] must be false as
            we can't compute gradients with respect to the labels.</param>
            <param name="colBottom">bottom input blob vector (length 2)
             -# @f$ (N \times C \times H \times W) @f$
                the predictions @f$ x @f$; backward computes diff @f$
                  \frac{\partial E}{\partial x}
                @f$
             -# @f$ (N \times 1 \times 1 \times 1) @f$
                the labels -- ignored as we can't compute their error gradients.
            </param>
        </member>
        <member name="T:MyCaffe.layers.SplitLayer`1">
            <summary>
            The SplitLayer creates a 'split' path in the network by copying the bottom blob
            into multiple top blob's to be used by multiple consuming layers.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.SplitLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The SplitLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type SPLIT.</param>
        </member>
        <member name="P:MyCaffe.layers.SplitLayer`1.ExactNumBottomBlobs">
            <summary>
            Returns the exact number of required bottom (input) Blobs: input.
            </summary>
        </member>
        <member name="P:MyCaffe.layers.SplitLayer`1.MinTopBlobs">
            <summary>
            Returns the minimum number of required top (output) Blobs: split
            </summary>
        </member>
        <member name="M:MyCaffe.layers.SplitLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SplitLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.SplitLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation copying the bottom Blbos with all of the top Blobs.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1+)
             -# @f$ (N \times C \times H \times W) @f$ 
                the output.
            </param>
        </member>
        <member name="M:MyCaffe.layers.SplitLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs.
            </summary>
            <param name="colTop">top output Blob vector (Length 1+), providing the error gradient
            with respect to computed outputs.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
            </param>
        </member>
        <member name="T:MyCaffe.layers.TanhLayer`1">
            <summary>
            The TanhLayer is a neuron layer that calculates the tanh function,
            popular with auto-encoders.
            This layer is initialized with the MyCaffe.param.TanhParameter.
            </summary>
            <remarks>
            Computation: @f$ y = \frac{\exp(2x) - 1}{\exp(2x) + 1} @f$ 
            <br/>
            Note that the gradient vanishes as the values move away from 0.
            The ReLULayer is often a better choice for this reason.
            
            @see [ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks](https://arxiv.org/abs/1505.00393v3) by Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, and Yoshua Bengio, 2015.
            @see [Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction](https://arxiv.org/abs/1608.05267v2) by Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, and Ferdous Sohel, , 2016.
            @see [Applying Deep Learning to Answer Selection: A Study and An Open Task](https://arxiv.org/abs/1508.01585v2) by Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.LayerParameter)">
            <summary>
            The TanhLayer constructor.
            </summary>
            <param name="cuda">Specifies the CudaDnn connection to Cuda.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies the LayerParameter of type TANH with parameter tanh_param,
            with options:
              - engine. The engine to use, either Engine.CAFFE, or Engine.CUDNN.
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.dispose">
            @copydoc Layer::dispose 
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.LayerSetUp(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Setup the layer to run in either Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.Reshape(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Reshape the bottom (input) and top (output) blobs.
            </summary>
            <param name="colBottom">Specifies the collection of bottom (input) Blobs.</param>
            <param name="colTop">Specifies the collection of top (output) Blobs.</param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.forward(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs @f$
                y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.backward(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the inputs using either the Engine.CAFFE or Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$ 
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y}
                      \left(1 - \left[\frac{\exp(2x) - 1}{\exp(2x) + 1} \right]^2 \right)
                    = \frac{\partial E}{\partial y} (1 - y^2)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.forward_cuda(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CAFFE mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs @f$
                y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.backward_cuda(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the ganh inputs using the Engine.CAFFE mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$ 
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y}
                      \left(1 - \left[\frac{\exp(2x) - 1}{\exp(2x) + 1} \right]^2 \right)
                    = \frac{\partial E}{\partial y} (1 - y^2)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.forward_cudnn(MyCaffe.common.BlobCollection{`0},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the forward calculation using the Engine.CUDNN mode.
            </summary>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ the inputs.</param>
            <param name="colTop">top otuput Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the computed outputs @f$
                y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
                @f$
            </param>
        </member>
        <member name="M:MyCaffe.layers.TanhLayer`1.backward_cudnn(MyCaffe.common.BlobCollection{`0},System.Collections.Generic.List{System.Boolean},MyCaffe.common.BlobCollection{`0})">
            <summary>
            Computes the error gradient w.r.t the ganh inputs using the Engine.CUDNN mode.
            </summary>
            <param name="colTop">top output Blob vector (Length 1), providing the error gradient
            with respect to computed outputs.
             -# @f$ (N \times C \times H \times W) @f$ 
                containing error gradients @f$ \frac{\partial E}{\partial y} @f$
                with respect to computed outputs @f$ y @f$.</param>
            <param name="rgbPropagateDown">propagate down see Layer::Backward</param>
            <param name="colBottom">bottom input Blob vector (Length 1)
             -# @f$ (N \times C \times H \times W) @f$ 
                the inputs @f$ x @f$; Backward fills their diff with 
                gradients @f$ 
                \frac{\partial E}{\partial y} 
                    = \frac{\partial E}{\partial y}
                      \left(1 - \left[\frac{\exp(2x) - 1}{\exp(2x) + 1} \right]^2 \right)
                    = \frac{\partial E}{\partial y} (1 - y^2)
                @f$ if propagate_down[0] == true
            </param>
        </member>
        <member name="T:MyCaffe.MyCaffeControl`1">
            <summary>
            The MyCaffeControl is the main object used to manage all training, testing and running of the MyCaffe system.
            </summary>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_settings">
            <summary>
            The settings used to configure the control.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_log">
            <summary>
            The log used for output.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_imgDb">
            <summary>
            The image database.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_bImgDbOwner">
            <summary>
            Whether or not the control owns the image database.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_evtCancel">
            <summary>
            The CancelEvent used to cancel training and testing operations.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_evtForceSnapshot">
            <summary>
            An auto-reset event used to force a snapshot.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_evtForceTest">
            <summary>
            An auto-reset event used to force a test cycle.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_evtPause">
            <summary>
            An auto-reset event used to pause training.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_dataTransformer">
            <summary>
            The data transformer used to transform data.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_project">
            <summary>
            The active project (if any).
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_dataSet">
            <summary>
            The dataset descriptor of the dataset used in the image database.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_strCudaPath">
            <summary>
            The low-level path of the underlying CudaDnn DLL.
            </summary>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.m_rgGpu">
            <summary>
            A list of the Device ID's used for training.
            </summary>
        </member>
        <member name="E:MyCaffe.MyCaffeControl`1.OnSnapshot">
            <summary>
            The OnSnapshot event fires each time a snap-shot is taken.
            </summary>
        </member>
        <member name="E:MyCaffe.MyCaffeControl`1.OnTrainingIteration">
            <summary>
            The OnTrainingIteration event fires at the end of each training iteration.
            </summary>
        </member>
        <member name="E:MyCaffe.MyCaffeControl`1.OnTestingIteration">
            <summary>
            The OnTestingIteration event fires at the end of each testing iteration.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.#ctor(MyCaffe.basecode.SettingsCaffe,MyCaffe.basecode.Log,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,System.Threading.ManualResetEvent,System.Collections.Generic.List{System.Int32},System.String,System.Boolean)">
            <summary>
            The MyCaffeControl constructor.
            </summary>
            <param name="settings">Specifies the setting used to configure the MyCaffeControl.</param>
            <param name="log">Specifies the Log for all output.</param>
            <param name="evtCancel">Specifies the CancelEvent used to abort training and testing operations.</param>
            <param name="evtSnapshot">Optionally, specifies an auto reset event used to force a snap-shot.</param>
            <param name="evtForceTest">Optionally, specifies an auto reset event used to force a test cycle.</param>
            <param name="evtPause">Optionally, specifies an auto reset event used to pause training.</param>
            <param name="rgGpuId">Optionally, specfies a set of GPU ID's that override those specified in the SettingsCaffe object.</param>
            <param name="strCudaPath">Optionally, specifies the path to the low-lever CudaDnnDll.DLL file.  Note, when not set, the system looks in the same directory of the executing assembly for the low-level DLL.</param>
            <param name="bCreateCudaDnn">Optionally, specififies create the connection to CUDA (default = false, causing the creation to occur during Load).</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.dispose">
            <summary>
            Releases all GPU and Host resources used by the CaffeControl.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.CurrentStage">
            <summary>
            Returns the stage under which the project was loaded, if any.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Clone(System.Int32)">
            <summary>
            Clone the current instance of the MyCaffeControl creating a second instance.
            </summary>
            <remarks>
            The second instance has the same project loaded and a copy of the first instance's weights.
            </remarks>
            <param name="nGpuID">Specifies the GPUID on which to load the second instance.</param>
            <returns>The new MyCaffeControl instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CopyGradientsFrom(MyCaffe.MyCaffeControl{`0})">
            <summary>
            Copy the learnable parameter diffs from the source MyCaffeControl into this one.
            </summary>
            <param name="src">Specifies the source MyCaffeControl whos gradients (blob diffs) are to be copied.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CopyWeightsFrom(MyCaffe.MyCaffeControl{`0})">
            <summary>
            Copy the learnable parameter data from the source MyCaffeControl into this one.
            </summary>
            <param name="src">Specifies the source MyCaffeControl whos gradients (blob data) are to be copied.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.ApplyUpdate(System.Int32)">
            <summary>
            Directs the solver to apply the leanred blob diffs to the weights using the solver's learning rate and
            update algorithm.
            </summary>
            <param name="nIteration">Specifies the current iteration.</param>
            <returns>The learning rate used is returned.</returns>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableTesting">
            <summary>
            Enable/disable testing.  For example reinforcement learning does not use testing.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Unload(System.Boolean)">
            <summary>
            Unload the currently loaded project, if any.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.ReInitializeParameters(System.String[])">
            <summary>
            Re-initializes each of the specified layers by re-running the filler (if any) specified by the layer.  
            When the 'rgstr' parameter is <i>null</i> or otherwise empty, the blobs of all layers are re-initialized. 
            </summary>
            <param name="rgstrLayers">Specifies the layers to reinitialize, when <i>null</i> or empty, all layers are re-initialized</param>
            <returns>If a layer is specified and found, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
            <remarks>This method causes the OnTrainingIteration event to fire with the updated values from the re-init.</remarks>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.SetOnTestOverride(System.EventHandler{MyCaffe.common.TestArgs})">
            <summary>
            Sets the root solver's onTest event function.
            </summary>
            <param name="onTest">Specifies the event handler called when testing.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.AddCancelOverrideByName(System.String)">
            <summary>
            Adds a cancel override.
            </summary>
            <param name="strEvtCancel">Specifies the new name of the cancel event to add.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.AddCancelOverride(MyCaffe.basecode.CancelEvent)">
            <summary>
            Adds a cancel override.
            </summary>
            <param name="evtCancel">Specifies the new name of the cancel event to add.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.RemoveCancelOverrideByName(System.String)">
            <summary>
            Remove a cancel override.
            </summary>
            <param name="strEvtCancel">Specifies the new name of the cancel event to remove.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.RemoveCancelOverride(MyCaffe.basecode.CancelEvent)">
            <summary>
            Remove a cancel override.
            </summary>
            <param name="evtCancel">Specifies the new name of the cancel event to remove.</param>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableBlobDebugging">
            <summary>
            Enable/disable blob debugging.
            </summary>
            <remarks>
            Note, when enabled, training will dramatically slow down.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableBreakOnFirstNaN">
            <summary>
            Enable/disable break training after first detecting a NaN.
            </summary>
            <remarks>
            This option requires that EnableBlobDebugging == <i>true</i>.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableDetailedNanDetection">
            <summary>
            When enabled (requires EnableBlobDebugging = <i>true</i>), the detailed Nan (and Infinity) detection is perofmed on each blob when training Net.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableLayerDebugging">
            <summary>
            Enable/disable layer debugging which causes each layer to check for NAN/INF on each forward/backward pass and throw an exception when found.
            </summary>
            <remarks>
            This option dramatically slows down training and is only recommended during debugging.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.EnableSingleStep">
            <summary>
            Enable/disable single step training.
            </summary>
            <remarks>
            This option requires that EnableBlobDebugging == true.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.DataTransformer">
            <summary>
            Returns the DataTransormer used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.Settings">
            <summary>
            Returns the settings used to create the control.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.Cuda">
            <summary>
            Returns the CudaDnn connection used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.Log">
            <summary>
            Returns the Log (for output) used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.Persist">
            <summary>
            Returns the persist used to load and save weights.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.ImageDatabase">
            <summary>
            Returns the CaffeImageDatabase used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.CancelEvent">
            <summary>
            Returns the CancelEvent used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.ActiveGpus">
            <summary>
            Returns a list of Active GPU's used by the control.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.ActiveLabelCounts">
            <summary>
            Returns a string describing the active label counts observed during training.
            </summary>
            <remarks>
            This string can help diagnose label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.LabelQueryHitPercents">
            <summary>
            Returns a string describing the label query hit percentages observed during training.
            </summary>
            <remarks>
            This string can help diagnose label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.LabelQueryEpochs">
            <summary>
            Returns a string describing the label query epochs observed during training.
            </summary>
            <remarks>
            This string can help diagnose label balancing issue.
            </remarks>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.CurrentDevice">
            <summary>
            Returns the name of the current device used.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.CurrentProject">
            <summary>
            Returns the name of the currently loaded project.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.CurrentIteration">
            <summary>
            Returns the current iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.MaximumIteration">
            <summary>
            Returns the maximum iteration.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetDeviceCount">
            <summary>
            Returns the total number of devices installed on this computer.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetDeviceName(System.Int32)">
            <summary>
            Returns the device name of a given device ID.
            </summary>
            <param name="nDeviceID">Specifies the device ID.</param>
            <returns></returns>
        </member>
        <member name="P:MyCaffe.MyCaffeControl`1.LastPhase">
            <summary>
            Returns the last phase run (TRAIN, TEST or RUN).
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.createNetParameterForRunning(MyCaffe.basecode.ProjectEx,MyCaffe.param.TransformationParameter@)">
            <summary>
            Creates a net parameter for the RUN phase.
            </summary>
            <remarks>
            This function transforms the training net parameter into a new net parameter suitable to run in the RUN phase.
            </remarks>
            <param name="p">Specifies a project.</param>
            <param name="transform_param">Specifies the TransformationParameter to use.</param>
            <returns>The new NetParameter suitable for the RUN phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.createNetParameterForRunning(MyCaffe.basecode.descriptors.DatasetDescriptor,System.String,MyCaffe.param.TransformationParameter@,MyCaffe.basecode.Stage)">
            <summary>
            Creates a net parameter for the RUN phase.
            </summary>
            <remarks>
            This function transforms the training net parameter into a new net parameter suitable to run in the RUN phase.
            </remarks>
            <param name="ds">Specifies a DatasetDescriptor for the dataset used.</param>
            <param name="strModel">Specifies the model descriptor.</param>
            <param name="transform_param">Specifies the TransformationParameter to use.</param>
            <param name="stage">Optionally, specifies the stage to create the run network on.</param>
            <returns>The new NetParameter suitable for the RUN phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.createNetParameterForRunning(MyCaffe.param.BlobShape,System.String,MyCaffe.param.TransformationParameter@,MyCaffe.basecode.Stage)">
            <summary>
            Creates a net parameter for the RUN phase.
            </summary>
            <remarks>
            This function transforms the training net parameter into a new net parameter suitable to run in the RUN phase.
            </remarks>
            <param name="shape">Specifies the shape of the images that will be used.</param>
            <param name="strModel">Specifies the model descriptor.</param>
            <param name="transform_param">Specifies the TransformationParameter to use.</param>
            <param name="stage">Optionally, specifies the stage to create the run network on.</param>
            <returns>The new NetParameter suitable for the RUN phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.createNetParameterForRunning(MyCaffe.basecode.SimpleDatum,System.String,MyCaffe.param.TransformationParameter@,System.Int32@,System.Int32@,System.Int32@,MyCaffe.basecode.Stage)">
            <summary>
            Creates a net parameter for the RUN phase.
            </summary>
            <remarks>
            This function transforms the training net parameter into a new net parameter suitable to run in the RUN phase.
            </remarks>
            <param name="sdMean">Specifies the mean image data used to size the network and as the mean image when used in the transformation parameter.</param>
            <param name="strModel">Specifies the model descriptor.</param>
            <param name="transform_param">Returns the TransformationParameter to use.</param>
            <param name="nC">Returns the discovered channel sizing to use.</param>
            <param name="nH">Returns the discovered height sizing to use.</param>
            <param name="nW">Returns the discovered width sizing to use.</param>
            <param name="stage">Optionally, specifies the stage to create the run network on.</param>
            <returns>The new NetParameter suitable for the RUN phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CreateNetParameterForRunning(MyCaffe.param.BlobShape,System.String,MyCaffe.param.TransformationParameter@,MyCaffe.basecode.Stage)">
            <summary>
            Creates a net parameter for the RUN phase.
            </summary>
            <remarks>
            This function transforms the training net parameter into a new net parameter suitable to run in the RUN phase.
            </remarks>
            <param name="shape">Specifies the shape of the images that will be used.</param>
            <param name="strModel">Specifies the model descriptor.</param>
            <param name="transform_param">Specifies the TransformationParameter to use.</param>
            <param name="stage">Optionally, specifies the stage to create the run network on.</param>
            <returns>The new NetParameter suitable for the RUN phase is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.PrepareImageMeans(MyCaffe.basecode.ProjectEx)">
            <summary>
            Prepare the testing image mean by copying the training image mean if the testing image mean is missing.
            </summary>
            <param name="prj">Specifies the project whos image mean is to be prepared.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Load(MyCaffe.basecode.Phase,MyCaffe.basecode.ProjectEx,System.Nullable{MyCaffe.basecode.IMGDB_LABEL_SELECTION_METHOD},System.Nullable{MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD},System.Boolean,MyCaffe.basecode.IXImageDatabaseBase,System.Boolean,System.Boolean,System.String,System.Boolean)">
            <summary>
            Load a project and optionally the MyCaffeImageDatabase.
            </summary>
            <remarks>
            This load function uses the MyCaffeImageDatabase.
            </remarks>
            <param name="phase">Specifies the Phase for which the load should focus.</param>
            <param name="p">Specifies the Project to load.</param>
            <param name="labelSelectionOverride">Optionally, specifies the label selection override (overides the label selection in SettingsCaffe).  The label selection dictates how the label sets are selected.</param>
            <param name="imageSelectionOverride">Optionally, specifies the image selection override (overides the image selection in SettingsCaffe).  The image selection dictates how the images are selected from each label set.</param>
            <param name="bResetFirst">Optionally, resets the device before loading.  IMPORTANT: this functionality is only recommendned during testing, for resetting the device will throw off all other users of the device.</param>
            <param name="imgdb">Optionally, specifies the MyCaffeImageDatabase to use.  When <i>null</i>, an instance if the MyCaffeImageDatabase is created internally.</param>
            <param name="bUseImageDb">Optionally, specifies whehter or not to use the image database (default = true).</param>
            <param name="bCreateRunNet">Optionally, specifies whether or not to create the Run net.</param>
            <param name="strStage">Optionally, specifies a stage under which to load the model.</param>
            <param name="bEnableMemTrace">Optionally, specifies to enable the memory tracing (only available in debug builds).</param>
            <returns>If the project is loaded the function returns <i>true</i>, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Load(MyCaffe.basecode.Phase,System.String,System.String,System.Byte[],System.Nullable{MyCaffe.basecode.IMGDB_LABEL_SELECTION_METHOD},System.Nullable{MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD},System.Boolean,MyCaffe.basecode.IXImageDatabaseBase,System.Boolean,System.Boolean,System.String,System.Boolean)">
            <summary>
            Load a project and optionally the MyCaffeImageDatabase.
            </summary>
            <remarks>
            This load function uses the MyCaffeImageDatabase.
            </remarks>
            <param name="phase">Specifies the Phase for which the load should focus.</param>
            <param name="strSolver">Specifies the solver descriptor.</param>
            <param name="strModel">Specifies the model desciptor.</param>
            <param name="rgWeights">Optionally, specifies the weights to load, or <i>null</i> to ignore.</param>
            <param name="labelSelectionOverride">Optionally, specifies the label selection override (overides the label selection in SettingsCaffe).  The label selection dictates how the label sets are selected.</param>
            <param name="imageSelectionOverride">Optionally, specifies the image selection override (overides the image selection in SettingsCaffe).  The image selection dictates how the images are selected from each label set.</param>
            <param name="bResetFirst">Optionally, resets the device before loading.  IMPORTANT: this functionality is only recommendned during testing, for resetting the device will throw off all other users of the device.</param>
            <param name="imgdb">Optionally, specifies the MyCaffeImageDatabase to use.  When <i>null</i>, an instance if the MyCaffeImageDatabase is created internally.</param>
            <param name="bUseImageDb">Optionally, specifies whehter or not to use the image database (default = true).</param>
            <param name="bCreateRunNet">Optionally, specifies whether or not to create the Run net (default = true).</param>
            <param name="strStage">Optionally, specifies a stage under which to load the model.</param>
            <param name="bEnableMemTrace">Optionally, specifies to enable the memory tracing (only available in debug builds).</param>
            <returns>If the project is loaded the function returns <i>true</i>, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.LoadLite(MyCaffe.basecode.Phase,System.String,System.String,System.Byte[],System.Boolean,System.Boolean,MyCaffe.basecode.SimpleDatum,System.String,System.Boolean)">
            <summary>
            Load a solver and model without using the MyCaffeImageDatabase.
            </summary>
            <remarks>
            This load function is a load lite that does not use the MyCaffeImageDatabase.
            </remarks>
            <param name="phase">Specifies the Phase for which the load should focus.</param>
            <param name="strSolver">Specifies the solver descriptor.</param>
            <param name="strModel">Specifies the model desciptor.</param>
            <param name="rgWeights">Optionally, specifies the weights to load, or <i>null</i> to ignore (default = null).</param>
            <param name="bResetFirst">Optionally, resets the device before loading.  IMPORTANT: this functionality is only recommendned during testing, for resetting the device will throw off all other users of the device.</param>
            <param name="bCreateRunNet">Optionally, specifies whether or not to create the Run net (default = true).</param>
            <param name="sdMean">Optionally, specifies the image mean to use (default = null).</param>
            <param name="strStage">Optionally, specifies a stage under which to load the model.</param>
            <param name="bEnableMemTrace">Optionally, specifies to enable the memory tracing (only available in debug builds).</param>
            <returns>If the project is loaded the function returns <i>true</i>, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.LoadToRun(System.String,System.Byte[],MyCaffe.param.BlobShape,MyCaffe.basecode.SimpleDatum,MyCaffe.param.TransformationParameter,System.Boolean)">
            <summary>
            The LoadToRun method loads the MyCaffeControl for running only (e.g. deployment).
            </summary>
            <remarks>
            This method does not use the MyCaffeImageDatabase.
            </remarks>
            <param name="strModel">Specifies the model description to load.</param>
            <param name="rgWeights">Specifies the trained weights to load.</param>
            <param name="shape">Specifies the expected shape to run on.</param>
            <param name="sdMean">Optionally, specifies the simple datum mean to subtract from input images that are run.</param>
            <param name="transParam">Optionally, specifies the TransformationParameter to use.  When using a 'deployment' model that has no data layers, you should supply a transformation parameter
            that matches the transformation used during training.</param>
            <param name="bForceBackward">Optionally, specifies to force backward propagation in the event that a backward pass is to be run on the Run net - The DeepDraw functionality
            uses this setting so that it can view what the trained weights actually see.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Train(System.Int32,System.Int32,MyCaffe.common.TRAIN_STEP,System.Double,System.Boolean)">
            <summary>
            Train the network a set number of iterations.
            </summary>
            <param name="nIterationOverride">Optionally, specifies number of iterations to run that override the iterations specified in the solver desctiptor.</param>
            <param name="nTrainingTimeLimitInMinutes">Optionally, specifies a maximum number of minutes to train.  When set to 0, this parameter is ignored and no time limit is imposed.</param>
            <param name="step">Optionally, specifies whether or not to single step the training on the forward pass, backward pass or both.  The default is <i>TRAIN_STEP.NONE</i> which runs the training to the maximum number of iterations specified.</param>
            <param name="dfLearningRateOverride">Optionally, specifies a learning rate override (default = 0 which ignores this parameter)</param>
            <param name="bReset">Optionally, reset the iterations to zero.</param>
            <remarks>
            Note when single stepping, no testing cycles are performed.  Currently, the single-step parameter is only suppored when running in single GPU mode.
            </remarks>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Test(System.Int32)">
            <summary>
            Test the network a given number of iterations.
            </summary>
            <param name="nIterationOverride">Optionally, specifies number of iterations to run that override the iterations specified in the solver desctiptor.</param>
            <returns>The accuracy value from the test is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.TestMany(System.Int32,System.Boolean,System.Boolean,MyCaffe.basecode.IMGDB_IMAGE_SELECTION_METHOD,System.Int32)">
            <summary>
            Test on a number of images by selecting random images from the database, running them through the Run network, and then comparing the results with the 
            expected results.
            </summary>
            <param name="nCount">Specifies the number of cycles to run.</param>
            <param name="bOnTrainingSet">Specifies on whether to select images from the training set, or when <i>false</i> the testing set of data.</param>
            <param name="bOnTargetSet">Optionally, specifies to test on the target dataset (if exists) as opposed to the source dataset.  The default is <i>false</i>, which tests on the default (source) dataset.</param>
            <param name="imgSelMethod">Optionally, specifies the image selection method (default = RANDOM).</param>
            <param name="nImageStartIdx">Optionally, specifies the image start index (default = 0).</param>
            <returns>The list of SimpleDatum and their ResultCollections (after running the model on each) is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(System.Int32)">
            <summary>
            Run on a given image in the MyCaffeImageDatabase based on its image index.
            </summary>
            <param name="nImageIdx">Specifies the image index.</param>
            <returns>The result of the run is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(System.Collections.Generic.List{System.Int32})">
            <summary>
            Run on a set of images in the MyCaffeImageDatabase based on their image indexes.
            </summary>
            <param name="rgImageIdx">Specifies a list of image indexes.</param>
            <returns>A list of results from the run is returned - one result per image.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CreateDataBlob(MyCaffe.basecode.SimpleDatum,MyCaffe.common.Blob{`0})">
            <summary>
            Create a data blob from a SimpleDatum by transforming the data and placing the results in the blob returned.
            </summary>
            <param name="d">Specifies the datum to load into the blob.</param>
            <param name="blob">Optionally, specifies a blob to use instead of creating a new one.</param>
            <returns>The data blob containing the transformed data is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(MyCaffe.basecode.SimpleDatum,System.Boolean,System.Boolean)">
            <summary>
            Run on a given Datum. 
            </summary>
            <param name="d">Specifies the Datum to run.</param>
            <param name="bSort">Specifies whether or not to sor the results.</param>
            <param name="bUseSolverNet">Optionally, specifies whether or not to use the training net vs. the run net.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(System.Collections.Generic.List{MyCaffe.basecode.SimpleDatum},System.Boolean,System.Boolean)">
            <summary>
            Run on a given list of Datum. 
            </summary>
            <param name="rgSd">Specifies the list of Datum to run.</param>
            <param name="bSort">Optionally, specifies whether or not to sor the results.</param>
            <param name="bUseSolverNet">Optionally, specifies whether or not to use the training net vs. the run net.</param>
            <returns>A list of results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(System.Drawing.Bitmap,System.Boolean)">
            <summary>
            Run on a given bitmap image.
            </summary>
            <remarks>
            This method does not use the MyCaffeImageDatabase.
            </remarks>
            <param name="img">Specifies the input image.</param>
            <param name="bSort">Specifies whether or not to sort the results.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Run(MyCaffe.basecode.SimpleDatum,System.Boolean)">
            <summary>
            Run on a given Datum. 
            </summary>
            <param name="d">Specifies the Datum to run.</param>
            <param name="bSort">Specifies whether or not to sort the results.</param>
            <returns>The results of the run are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetTestImage(MyCaffe.basecode.Phase,System.Int32@,System.String@)">
            <summary>
            Retrieves a random image from either the training or test set depending on the Phase specified.
            </summary>
            <param name="phase">Specifies whether to select images from the training set or testing set.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <param name="strLabel">Returns the expected label name for the image.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetTestImage(MyCaffe.basecode.Phase,System.Int32)">
            <summary>
            Retrieves a random image from either the training or test set depending on the Phase specified.
            </summary>
            <param name="phase">Specifies whether to select images from the training set or testing set.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetTargetImage(System.Int32,System.Int32,System.Int32@,System.String@,System.Byte[]@,MyCaffe.basecode.SimpleDatum.DATA_FORMAT@)">
            <summary>
            Retrives the image at a given index within the Testing data set.
            </summary>
            <param name="nSrcId">Specifies the Source ID.</param>
            <param name="nIdx">Specifies the image index.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <param name="strLabel">Returns the expected label name for the image.</param>
            <param name="rgCriteria">Returns the data criteria if one exists.</param>
            <param name="fmtCriteria">Returns the format of the data criteria, if one exists.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetTargetImage(System.Int32,System.Int32@,System.String@,System.Byte[]@,MyCaffe.basecode.SimpleDatum.DATA_FORMAT@)">
            <summary>
            Retrives the image with a given ID.
            </summary>
            <param name="nImageID">Specifies the Raw Image ID.</param>
            <param name="nLabel">Returns the expected label for the image.</param>
            <param name="strLabel">Returns the expected label name for the image.</param>
            <param name="rgCriteria">Returns the data criteria if one exists.</param>
            <param name="fmtCriteria">Returns the format of the data criteria, if one exists.</param>
            <returns>The image queried is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetImageMean">
            <summary>
            Returns the image mean used by the solver network used during training.
            </summary>
            <returns>The image mean is returned as a SimpleDatum.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetDataset">
            <summary>
            Returns the current dataset used when training and testing.
            </summary>
            <returns>The DatasetDescriptor is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetWeights">
            <summary>
            Retrieves the weights of the training network.
            </summary>
            <returns>The weights are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.UpdateRunWeights">
            <summary>
            Loads the weights from the training net into the Net used for running.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.UpdateWeights(System.Byte[])">
            <summary>
            Loads the training Net with new weights.
            </summary>
            <param name="rgWeights">Specifies the weights to load.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetNet(System.Byte[],MyCaffe.common.CudaDnn{`0})">
            <summary>
            Creates a new Net, loads the weights specified into it and returns it.
            </summary>
            <param name="rgWeights">Specifies the weights to load.</param>
            <param name="cudaOverride">Optionally, specifies a different cuda instance for the Net to use.</param>
            <returns>The new Net is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetInternalNet(MyCaffe.basecode.Phase)">
            <summary>
            Returns the internal net based on the Phase specified: TRAIN, TEST or RUN.
            </summary>
            <param name="phase">Specifies the Phase used to select the Net.</param>
            <returns>The internal Net is returned.</returns>
            <remarks>
            The following net is returned under the following conditions:
              phase = ALL, return the net from the LastPhase run.  If the LastPhase run = NONE, return the RUN net.
              phase = n/a, return the default RUN net.
              phase = NONE, return the default RUN net.
              phase = TRAIN, return the training net.
              phase = TEST, return the testing net.
            </remarks>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetInternalSolver">
            <summary>
            Get the internal solver.
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Snapshot">
            <summary>
            The Snapshot function forces a snapshot to occur.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.ResetDevice(System.Int32)">
            <summary>
            Reset the device at the given device ID.
            </summary>
            <remarks>
            <b>WARNING!</b> It is recommended that this only be used when testing, for calling this will throw off all other users
            of the device and may cause unpredictable behavior.
            </remarks>
            <param name="nDeviceID">Specifies the device ID of the device to reset.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetLicenseTextEx(System.String)">
            <summary>
            Returns the license text for MyCaffe.
            </summary>
            <param name="strOtherLicenses">Specifies other licenses to append to the license text.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.GetLicenseText(System.String)">
            <summary>
            Returns the license text for MyCaffe.
            </summary>
            <param name="strOtherLicenses">Specifies other licenses to append to the license text.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.VerifyCompute(System.String,System.Int32,System.Boolean)">
            <summary>
            VerifyCompute compares the current compute of the current device (or device specified) against the required compute of the current CudaDnnDLL.dll used.
            </summary>
            <param name="strExtra">Optionally, specifies extra information for the exception if one is thrown.</param>
            <param name="nDeviceID">Optionally, specifies a specific device ID to check, otherwise uses the current device used (default = -1, which uses the current device).</param>
            <param name="bThrowException">Optionally, specifies whether or not to throw an exception on a compute mis-match (default = true).</param>
            <returns>If the device's compute is >= to the required compute fo the CudaDnnDll.dll used, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CreateBlob(System.String)">
            <summary>
            Create an unsized blob and set its name.
            </summary>
            <param name="strName">Specifies the Blob name.</param>
            <returns>The Blob is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.CreateExtension(System.String)">
            <summary>
            Create and load a new extension DLL.
            </summary>
            <param name="strExtensionDLLPath">Specifies the path to the extension DLL.</param>
            <returns>The handle to the extension is returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.FreeExtension(System.Int64)">
            <summary>
            Free an existing extension and unload it.
            </summary>
            <param name="hExtension">Specifies the handle to the extension to free.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.RunExtension(System.Int64,System.Int64,`0[])">
            <summary>
            Run a function on an existing extension.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.RunExtensionD(System.Int64,System.Int64,System.Double[])">
            <summary>
            Run a function on an existing extension using the <i>double</i> base type.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.RunExtensionF(System.Int64,System.Int64,System.Single[])">
            <summary>
            Run a function on an existing extension using the <i>float</i> base type.
            </summary>
            <param name="hExtension">Specifies the extension.</param>
            <param name="lfnIdx">Specifies the function to run on the extension.</param>
            <param name="rgParam">Specifies the parameters.</param>
            <returns>The return values of the function are returned.</returns>
        </member>
        <member name="F:MyCaffe.MyCaffeControl`1.components">
            <summary>
            Required designer variable.
            </summary>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.Dispose(System.Boolean)">
            <summary> 
            Clean up any resources being used.
            </summary>
            <param name="disposing">true if managed resources should be disposed; otherwise, false.</param>
        </member>
        <member name="M:MyCaffe.MyCaffeControl`1.InitializeComponent">
            <summary>
            Required method for Designer support - do not modify
            the contents of this method with the code editor.
            </summary>
        </member>
        <member name="T:MyCaffe.param.beta.DecodeParameter">
            <summary>
            Specifies the parameters for the DecodeLayer and the AccuracyEncodingLayer.
            </summary>
        </member>
        <member name="T:MyCaffe.param.beta.DecodeParameter.TARGET">
            <summary>
            Defines the target type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.beta.DecodeParameter.TARGET.PREGEN">
            <summary>
            Specifies to use pre-generated targets that are evenly spaced.
            </summary>
        </member>
        <member name="F:MyCaffe.param.beta.DecodeParameter.TARGET.CENTROID">
            <summary>
            Specifies to use the centroid as the target.
            </summary>
        </member>
        <member name="F:MyCaffe.param.beta.DecodeParameter.TARGET.KNN">
            <summary>
            Specifies to use the k-nearest neighbor as the target.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.pregen_alpha">
            <summary>
            Specifies the pregen margin.
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.pregen_label_count">
            <summary>
            Specifies the pre-generate target count, only used when 'target' = PREGEN.
            </summary>
            <remarks>
            When using a value > 0, the pre-gen target count must equal the label count for this setting
            directs the loss to first pre-generate equally distanced target encodings for each label and calculate the loss
            (and gradients) that move each labeled item toward those targets.  This is usefule when initially training
            on a very small number of samples.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.enable_centroid_update">
            <summary>
            Specifies whether or not to update the centroids.  For example in some cases a fixed centroid may be desired, such as is the case when initializing centroids with a PREGEN value.
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.centroid_output_iteration">
            <summary>
            Specifies the iteration where calculated centroids are output for each label, before this value, the centroids should not be used for their calculation is not complete (default = 300).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.output_centroids">
            <summary>
            Optionally, specifies to output the centroids in top[1] (default = false).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.ignore_labels">
            <summary>
            Optionally, specifies one or more labels to ignore. (default = none, which then expects all labels).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.target">
            <summary>
            Optionally, specifies the target type to use (default = CENTROID).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.cache_size">
            <summary>
            Specifies the size of the cache (in number of batches) used when calculating the CENTROID and KNN values (default = 300).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DecodeParameter.k">
            <summary>
            Optionally, specifies the K value to use with the KNN target (default = 5).
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.DecodeParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.beta.DataSequenceParameter">
            <summary>
            Specifies the parameters for the DataSequenceLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.cache_size">
            <summary>
            Specifies the cache size used for each labeled image cache, which should be at least 4 x the batch size used (default = 256).
            </summary>
            <remarks>
            The cache size should be higher than the maximum number of items for a given label within a batch, otherwise items within a given batch
            will overrite other items from the batch for a given label.
            
            NOTE: The batch size must also be large enough to contain at least two items from each label.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.k">
            <summary>
            Specifies the 'k' number of negatively matched labled images (default = 0, maximum = 10).  When specifying 0, the output is just the anchor and one alternating negatively/positive matched image.
            </summary>
            <remarks>
            When specifying k>0, the anchor is output with a positive match and 'k' number of negative matches.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.balance_matches">
            <summary>
            Specifies to balance the matching image between negative and positive matches.  This setting only applies when k=0 (default = true).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.output_labels">
            <summary>
            Specifies whether or not to output the labels in an additional top output.  (default = true).
            </summary>
            <remarks>
            Labels areoutput in order per tuplet, so if k = 0 (e.g. output anchor and one negative) the output lables are as follows:
            0 anchor label
            0 negative label
            1 anchor label
            1 negative label
            :
            </remarks>
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.label_count">
            <summary>
            Specifies the number of labels in the data set, or 0 to use dynamic label discovery (requires large enough batch sizes to cover all labels within first batch) - (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.DataSequenceParameter.label_start">
            <summary>
            Specifies the first label in the label set (default = 0).
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.DataSequenceParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.beta.KnnParameter">
            <summary>
            Specifies the parameters for the KnnLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.beta.KnnParameter.num_output">
            <summary>
            Specifies the number of output items (e.g. classes)
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.KnnParameter.k">
            <summary>
            Specifies the 'k' number of nearest neighbors to compare (per class).
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.KnnParameter.max_stored_batches">
            <summary>
            Specifies the maximum number of batches to store before releasing batches.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.KnnParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.beta.UnPoolingParameter">
            <summary>
            Specifies the parameters for the UnPoolingLayer.
            </summary>
            <remarks>
            @see [A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe](https://arxiv.org/abs/1701.04949) by Volodymyr Turchenko, Eric Chalmers, Artur Luczak, 2017.
            @see [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) by Vincent Dumoulin and Francesco Visin, 2016.
            @see [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150) by Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba, 2015.
            @see [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner, 1998.
            @see [Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation](https://arxiv.org/abs/1506.04924) by Seunghoon Hong, Hyeonwoo Noh, and Bohyung Han, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.#ctor">
            @copydoc PoolingParameter 
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.beta.UnPoolingParameter.unpool_size">
            <summary>
            UnPool size is given as a single value for equal dimensions in all 
            spatial dimensions, or once per spatial dimension.
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.UnPoolingParameter.unpool_h">
            <summary>
            The unpooling height override (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.beta.UnPoolingParameter.unpool_w">
            <summary>
            The unpooling width override (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc PoolingParameter::Load 
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc PoolingParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.Clone">
            @copydoc PoolingParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.ToProto(System.String)">
            @copydoc PoolingParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.UnPoolingParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.beta.TripletLossParameter">
            <summary>
            Specifies the parameters for the TripletLossLayer.
            </summary>
            <remarks>
            @see [Deep Metric Learning Using Triplet Network](https://arxiv.org/pdf/1412.6622.pdf) by Hoffer and Ailon, 2018.
            
            @see [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832) by Schroff, 2015.
            
            * Initial Python code for TripletDataLayer/TripletSelectionLayer/TripletLossLayer by luhaofang/tripletloss on github. 
            @see https://github.com/luhaofang/tripletloss - for general architecture
            
            * Initial C++ code for TripletLoss layer by eli-oscherovich in 'Triplet loss #3663' pull request on BVLC/caffe github.
            @see https://github.com/BVLC/caffe/pull/3663/commits/c6518fb5752344e1922eaa1b1eb686bae5cc3964 - for triplet loss layer implementation
            
            For an explanation of the gradient calculations,
            @see http://stackoverflow.com/questions/33330779/whats-the-triplet-loss-back-propagation-gradient-formula/33349475#33349475 - for gradient calculations
            
            @see [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737v2) by Alexander Hermans, Lucas Beyer, and Bastian Leibe, 2017. 
            @see [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832) by Florian Schroff, and Dmitry Kalenichenko, and James Philbin, 2015.
            @see [Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search](https://arxiv.org/abs/1611.05301v1) by Tu Bui, Leonardo Ribeiro, Moacir Ponti, and John Collomosse, 2016.
            </remarks> 
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.beta.TripletLossParameter.alpha">
            <summary>
            Specifies the margin.
            </summary>
        </member>
        <member name="P:MyCaffe.param.beta.TripletLossParameter.pregen_label_start">
            <summary>
            Specifies the starting label for pre-generated targets, only used when 'colBottom.Count' = 5, which contains centroids.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.TripletLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.beta.Normalization1Parameter">
            <summary>
            Specifies the parameters for the Normalization1Layer.
            </summary>
            <remarks>
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.beta.Normalization1Parameter.Norm">
            <summary>
            Defines the normalization type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.beta.Normalization1Parameter.Norm.L1">
            <summary>
            L1 normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.beta.Normalization1Parameter.Norm.L2">
            <summary>
            L2 normalization.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.beta.Normalization1Parameter.norm">
            <summary>
            Specifies the normalization method to use.
            </summary>
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.beta.Normalization1Parameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ImageDataParameter">
            <summary>
            Specifies the parameters for the ImageDataLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.rand_skip">
            <summary>
            Specifies the amount for the image data layer to skip a few points to avoid all asynchronous sgd clients to start at the same point.  The skip point should be set as rand_skip * rand(0,1).  Note that the rand_skip should not be larger than the number of keys in the database.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.shuffle">
            <summary>
            Specifies whether or not the ImageLayer should shuffle the list of files at each epoch.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.new_height">
            <summary>
            When > 0, specifies the new height of the images fed into the network (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.new_width">
            <summary>
            When > 0, specifies the new width of the images fed into the network (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.is_color">
            <summary>
            Specififies whether or not the image is color or gray-scale.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ImageDataParameter.root_folder">
            <summary>
            Specifies the folder containing the image files.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ImageDataParameter.FromProto(MyCaffe.basecode.RawProto,MyCaffe.param.ImageDataParameter)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <param name="p">Optionally, specifies an instance to load.  If <i>null</i>, a new instance is created and loaded.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.DataLabelMappingParameter">
            <summary>
            Specifies the parameters for the DataLabelMappingParameter used to map labels by the DataTransformer.TransformLabel when active.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.DataLabelMappingParameter.mapping">
            <summary>
            Specifies the label mapping where the original label is mapped to the new label specified.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.MapLabel(System.Int32,System.Int32)">
            <summary>
            Queries the mapped label for a given label.
            </summary>
            <param name="nLabel">Specifies the label to query the mapped label from.</param>
            <param name="nBoost">Specifies the boost condition that must be met if specified.</param>
            <returns>The mapped label is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the and return a new DataLabelMappingParameter. 
            </summary>
            <param name="br"></param>
            <param name="bNewInstance"></param>
            <returns>The new object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copies the specified source data label mapping parameter to this one.
            </summary>
            <param name="src">Specifies the source data label mapping parameter.</param>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.ToProto(System.String)">
            <summary>
            Convert the DataLabelMappingParameter into a RawProto.
            </summary>
            <param name="strName">Specifies the RawProto name.</param>
            <returns>The RawProto containing the settings is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataLabelMappingParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.DataDebugParameter">
            <summary>
            The DataDebugParameter is used by the DataParameter when the 'enable_debugging' = True.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataDebugParameter.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataDebugParameter.iterations">
            <summary>
            (/b optional, default = 1) Specifies the number of iterations to output debug information.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataDebugParameter.debug_save_path">
            <summary>
            (/b optional, default = null) Specifies the path where the debug data images are saved, otherwise is ignored when null.  This setting is only used for debugging.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataDebugParameter.Copy(MyCaffe.param.DataDebugParameter)">
            <summary>
            Copies the specified source data noise parameter to this one.
            </summary>
            <param name="src">Specifies the source data noise parameter.</param>
        </member>
        <member name="M:MyCaffe.param.DataDebugParameter.ToProto(System.String)">
            <summary>
            Convert the DataDebugParameter into a RawProto.
            </summary>
            <param name="strName">Specifies the RawProto name.</param>
            <returns>The RawProto containing the settings is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataDebugParameter.FromProto(MyCaffe.basecode.RawProto,MyCaffe.param.DataDebugParameter)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <param name="p">Optionally, specifies an instance to load.  If <i>null</i>, a new instance is created and loaded.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.OptionalParameter">
            <summary>
            The OptionalParameter is the base class for parameters that are optional such as the MaskParameter, DistorationParameter, ExpansionParameter, NoiseParameter, and ResizeParameter.
            </summary>
        </member>
        <member name="M:MyCaffe.param.OptionalParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active (default = false).</param>
        </member>
        <member name="P:MyCaffe.param.OptionalParameter.Active">
            <summary>
            When active, the parameter is used, otherwise it is ignored.
            </summary>
        </member>
        <member name="M:MyCaffe.param.OptionalParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.OptionalParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.OptionalParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.VideoDataParameter">
            <summary>
            Specifies the parameters for the VideoDataLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.VideoDataParameter.VideoType">
            <summary>
            Defines the video type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.VideoDataParameter.VideoType.WEBCAM">
            <summary>
            Specifies to use the web cam if available.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.VideoDataParameter.VideoType.VIDEO">
            <summary>
            Specifies to use a video file.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.video_type">
            <summary>
            Specifies the video type (default = WEBCAM).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.device_id">
            <summary>
            Specifies the device ID (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.video_file">
            <summary>
            Specifies the video file when using the VIDEO type.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.skip_frames">
            <summary>
            Optionally, specifies the number of frames to be skipped before processing a frame (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.video_width">
            <summary>
            Optionally, specifies the video width (default = 400).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.VideoDataParameter.video_height">
            <summary>
            Optionally, specifies the video height (default = 300).
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.VideoDataParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.LabelMap">
            <summary>
            Specifies the LabelMap used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.FindByLabel(System.Int32)">
            <summary>
            Find a label with its label id.
            </summary>
            <param name="nLabel">Specifies the label id.</param>
            <returns>If a label item with a matching id is found, it is returned, otherwise null is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.FindByName(System.String)">
            <summary>
            Find a label with a given name.
            </summary>
            <param name="strName">Specifies the label name.</param>
            <returns>If a label item with a matching name is found, it is returned, otherwise null is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.MapToName(MyCaffe.basecode.Log,System.Boolean,System.Boolean)">
            <summary>
            Map the labels into a dictionary.
            </summary>
            <param name="log">Specifies the output log.</param>
            <param name="bStrict">Specifies whether or not to allow duplicates, when allowed, the duplicate overwrites previous labels with the same ID.</param>
            <param name="bDisplayName">Specifies whether or not to use the display name (true) or name (false)</param>
            <returns>The label to name dictionary is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.MapToLabel(MyCaffe.basecode.Log,System.Boolean)">
            <summary>
            Map the names to their labels.
            </summary>
            <param name="log">Specifies the output log.</param>
            <param name="bStrict">Specifies whether or not to allow duplicates, when allowed, the duplicate overwrites previous labels with the same ID.</param>
            <returns>The name to label mapping is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.ssd.LabelMap.item">
            <summary>
            Specifies the list of label items.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.Copy(MyCaffe.param.ssd.LabelMap)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMap.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.LabelMapItem">
            <summary>
            The LabelMapItem class stores the information for a single label.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMapItem.#ctor(System.Int32,System.String,System.String)">
            <summary>
            The constructor.
            </summary>
            <param name="nLabel">Optionally, specify the label id (default = 0).</param>
            <param name="strName">Optionally, specify the label name (default = null).</param>
            <param name="strDisplay">Optionally, specify the label display name (default = null).</param>
        </member>
        <member name="P:MyCaffe.param.ssd.LabelMapItem.name">
            <summary>
            Get/set the label name.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.LabelMapItem.display">
            <summary>
            Optionally, get/set the display name for the label.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.LabelMapItem.label">
            <summary>
            Get/set the label id.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMapItem.Copy(MyCaffe.param.ssd.LabelMapItem)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMapItem.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMapItem.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.LabelMapItem.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.SaveOutputParameter">
            <summary>
            Specifies the parameters for the SaveOutputLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.SaveOutputParameter.OUTPUT_FORMAT">
            <summary>
            Defines the output format.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.SaveOutputParameter.OUTPUT_FORMAT.VOC">
            <summary>
            Specifies the PASCAL VOC output format.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.SaveOutputParameter.OUTPUT_FORMAT.COCO">
            <summary>
            Specifies the MS COCO output format.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.SaveOutputParameter.OUTPUT_FORMAT.ILSVRC">
            <summary>
            Specifies the ILSVRC output format.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.output_directory">
            <summary>
            Specifies the output directory - if not empty, the results will be saved.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.output_name_prefix">
            <summary>
            Specifies the output name prefix.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.output_format">
            <summary>
            Specifies the output format.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.label_map_file">
            <summary>
            Optionally, specifies the output label map file.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.name_size_file">
            <summary>
            Optionally, specifies the output name size file.
            </summary>
            <remarks>
            The name size file contains a list of names and sizes with the order of the input DB.  The
            file is in the following format:
               name height width
               ...
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.num_test_image">
            <summary>
            Specifies the number of test images.
            </summary>
            <remarks>This setting can be less than the lines specified in the name_size_file.  For example,
            when we only want to evaluate on part of the test images.</remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.SaveOutputParameter.resize_param">
            <summary>
            Specifies the resize parameter used in saving the data.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the and return a new ResizeParameter. 
            </summary>
            <param name="br"></param>
            <param name="bNewInstance"></param>
            <returns>The new object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SaveOutputParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.DetectionOutputParameter">
            <summary>
            Specifies the parameters for the DetectionOutputLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.num_classes">
            <summary>
            Specifies the number of classes that are actually predicted - required!
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.share_location">
            <summary>
            Specifies whether or not to sare the bounding box is shared among different classes (default = true).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.background_label_id">
            <summary>
            Specifies the background class.
            </summary>
            <remarks>
            If there is no background label this should be set to -1.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.nms_param">
            <summary>
            Specifies the parameter used for non maximum suppression.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.save_output_param">
            <summary>
            Specifies the parameter used for saving the detection results.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.code_type">
            <summary>
            Specifies the coding method for the bbox.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.variance_encoded_in_target">
            <summary>
            Specifies whether or not the variance is encoded in the target; otherwise we need to adjust the predicted offset accordingly.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.keep_top_k">
            <summary>
            Specifies the number of total bboxes to be kept per image after nms step, -1 means keeping all bboxes after nms step.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.confidence_threshold">
            <summary>
            Specifies the threshold for deciding which detections to consider - only those which are larger than this threshold.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.visualize">
            <summary>
            Specifies whether or not to visualize the detection results.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.visualize_threshold">
            <summary>
            Specifies the theshold used to visualize detection results.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionOutputParameter.save_file">
            <summary>
            When provided, specifies the outputs to the video file.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionOutputParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.DetectionEvaluateParameter">
            <summary>
            Specifies the parameters for the DetectionEvaluateLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.num_classes">
            <summary>
            Specifies the number of classes that are actually predicted - required!
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.background_label_id">
            <summary>
            Specifies the background class.
            </summary>
            <remarks>
            Needed for sanity check so that background class is neither in the ground truth nor the detections.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.overlap_threshold">
            <summary>
            Specifies the threshold for deciding true/false positive.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.evaulte_difficult_gt">
            <summary>
            Specifies whether or not to consider the ground truth for evaluation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.name_size_file">
            <summary>
            Specifies the file which contains a list of names and sizes in the same order of the input database.  If provided, we scale the prediction and ground truth NormalizedBBox for evaluation.
            </summary>
            <remarks>
            This file is in the following format:
               name height width
               ...
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.DetectionEvaluateParameter.resize_param">
            <summary>
            Specifies the resize parameter used in converting the NormalizedBBox to the original size.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.DetectionEvaluateParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.BatchSampler">
            <summary>
            Specifies a sample of batch of bboxes with provided constraints in SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.#ctor">
            <summary>
            The BatchSampler constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.Save(System.IO.BinaryWriter)">
            <summary>
            Save the BatchSampler to a binary writer.
            </summary>
            <param name="bw">The binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the BatchSampler from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a the BatchSampler is read into a new instance, otherwise it is read into the current instance.</param>
            <returns>The BatchSampler instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.Load(System.IO.BinaryReader)">
            <summary>
            Load the BatchSampler from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <returns>A new BatchSampler instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.ssd.BatchSampler.use_original_image">
            <summary>
            Use the original image as the source for sampling.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.BatchSampler.max_sample">
            <summary>
            If provided (greater than zero), break when found certain number of samples satisfying the sample constraint.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.BatchSampler.max_trials">
            <summary>
            Maximum number of trials for sampling to avoid an infinite loop.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.BatchSampler.sampler">
            <summary>
            Specifies the constraints for sampling the bbox
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.BatchSampler.sample_constraint">
            <summary>
            Get/set the sample constraint.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.Clone">
            <summary>
            Creates a copy of the BatchSampler.
            </summary>
            <returns>A new instance of the BatchSampler is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.System#ICloneable#Clone">
            <summary>
            Creates a copy of the BatchSampler.
            </summary>
            <returns>A new instance of the BatchSampler is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.Compare(MyCaffe.param.ssd.BatchSampler)">
            <summary>
            Compares this BatchSampler to another.
            </summary>
            <param name="bs">Specifies the other BatchSampler to compare this one to.</param>
            <returns>If the two BatchSampler's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.CompareTo(System.Object)">
            <summary>
            Compares this BatchSampler to another.
            </summary>
            <param name="obj">Specifies the other BatchSampler to compare this one to.</param>
            <returns>If the two BatchSampler's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.ToProto(System.String)">
            <summary>
            Converts the BatchSampler to a RawProto.
            </summary>
            <param name="strName">Specifies a name for the RawProto.</param>
            <returns>A new RawProto representing the BatchSampler is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a new BatchSampler from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto containing a representation of the BatchSampler.</param>
            <returns>A new instance of the BatchSampler is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.BatchSampler.ToString">
            <summary>
            Return the string representation of the shape.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.AnnotatedDataParameter">
            <summary>
            Specifies the parameters for the InputLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.AnnotatedDataParameter.anno_type">
            <summary>
            Get/set the annotation type.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.AnnotatedDataParameter.batch_sampler">
            <summary>
            Get/set the batch sampler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.AnnotatedDataParameter.label_map_file">
            <summary>
            Get/set the label map file.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.AnnotatedDataParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.NonMaximumSuppressionParameter">
            <summary>
            Specifies the parameters for the NonMaximumSuppressionParameter used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.NonMaximumSuppressionParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.NonMaximumSuppressionParameter.nms_threshold">
            <summary>
            Get/set the threshold to be used in nms.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NonMaximumSuppressionParameter.top_k">
            <summary>
            Get/set the maximum number of results kept.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NonMaximumSuppressionParameter.eta">
            <summary>
            Get/set the parameter for adaptive nms.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.NonMaximumSuppressionParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the object.
            </summary>
            <param name="src">The copy is placed in this parameter.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.NonMaximumSuppressionParameter.Clone">
            <summary>
            Return a clone of the object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.NonMaximumSuppressionParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.NonMaximumSuppressionParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.MultiBoxLossParameter">
            <summary>
            Specifies the parameters for the MultiBoxLossParameter.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            @see [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540) by Abhinav Shrivastava, Abhinav Gupta, Ross Girshick, 2016.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.MultiBoxLossParameter.LocLossType">
            <summary>
            Defines the localization loss types.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.LocLossType.L2">
            <summary>
            Specifies to use L2 loss.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.LocLossType.SMOOTH_L1">
            <summary>
            Specifies to use smooth L1 loss.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.LocLossTypeFromString(System.String)">
            <summary>
            Convert a string into a LocLossType.
            </summary>
            <param name="str">Specifies the string.</param>
            <returns>The associated LocLossType is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType">
            <summary>
            Defines the confidence loss types.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType.SOFTMAX">
            <summary>
            Specifies to use softmax.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossType.LOGISTIC">
            <summary>
            Specifies to use logistic.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.ConfLossTypeFromString(System.String)">
            <summary>
            Convert a string into a ConfLossType.
            </summary>
            <param name="str">Specifies the string.</param>
            <returns>The associated ConfLossType is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.MultiBoxLossParameter.MatchType">
            <summary>
            Defines the matching method used during training.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.MatchType.BIPARTITE">
            <summary>
            Specifies to use Bi-Partite.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.MatchType.PER_PREDICTION">
            <summary>
            Specifies to use per-prediction matching.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.MatchTypeFromString(System.String)">
            <summary>
            Convert a string into a MatchType.
            </summary>
            <param name="str">Specifies the string.</param>
            <returns>The associated MatchType is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.MultiBoxLossParameter.MiningType">
            <summary>
            Defines the mining type used during training.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.MiningType.NONE">
            <summary>
            Use all negatives.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.MiningType.MAX_NEGATIVE">
            <summary>
            Select negatives based on the score.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.MultiBoxLossParameter.MiningType.HARD_EXAMPLE">
            <summary>
            Select hard examples based on Shrivastava et. al. method.
            </summary>
            <remarks>
            @see [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540) by Abhinav Shrivastava, Abhinav Gupta, Ross Girshick, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.MiningTypeFromString(System.String)">
            <summary>
            Convert a string into a MiningType.
            </summary>
            <param name="str">Specifies the string.</param>
            <returns>The associated MiningType is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.loc_loss_type">
            <summary>
            Get/set the localization loss type (default = SMOOTH_L1).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.conf_loss_type">
            <summary>
            Get/set the confidence loss type (default = SOFTMAX).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.loc_weight">
            <summary>
            Get/set the weight for the localization loss (default = 1.0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.num_classes">
            <summary>
            Get/set the number of classes to be predicted - required!
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.share_location">
            <summary>
            Get/sets whether or not the bounding box is shared among different classes (default = true).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.match_type">
            <summary>
            Get/set the matching method used during training (default = PER_PREDICTION).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.overlap_threshold">
            <summary>
            Get/set the overlap threshold (default = 0.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.background_label_id">
            <summary>
            Get/set the background label id.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.use_difficult_gt">
            <summary>
            Get/set whether or not to consider the difficult ground truth (defalt = true).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.do_neg_mining">
            <summary>
            DEPRECIATED: Get/set whether or not to perform negative mining (default = false).
            </summary>
            <remarks>
            DEPRECIATED: using 'mining_type' instead.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.neg_pos_ratio">
            <summary>
            Get/set the negative/positive ratio (default = 3.0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.neg_overlap">
            <summary>
            Get/set the negative overlap upperbound for the unmatched predictions (default = 0.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.code_type">
            <summary>
            Get/set the coding method for the bounding box.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.encode_variance_in_target">
            <summary>
            Get/set whether or not to encode the variance of the prior box in the loc loss target instead of in the bbox (default = false).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.map_object_to_agnostic">
            <summary>
            Get/set whether or not to map all object classes to an agnostic class (default = false).  This is useful when learning objectness detector.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.ignore_cross_boundary_bbox">
            <summary>
            Get/set whether or not to ignore cross boundary bbox during matching (default = false).  The cross boundary bbox is a bbox who is outside
            of the image region.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.bp_inside">
            <summary>
            Get/set whether or not to only backpropagate on corners which are inside of the image region when encode type is CORNER or CORNER_SIZE (default = false).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.mining_type">
            <summary>
            Get/set the mining type used during training (default = MAX_NEGATIVE).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.nms_param">
            <summary>
            Get/set the parameters used for the non maximum suppression during hard example training.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.sample_size">
            <summary>
            Get/set the number of samples (default = 64).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.use_prior_for_nms">
            <summary>
            Get/set whether or not to use the prior bbox for nms.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.use_prior_for_matching">
            <summary>
            Get/set whether or not to use prior for matching.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.MultiBoxLossParameter.use_gpu">
            <summary>
            Use the GPU version of the algorithm.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.MultiBoxLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.PriorBoxParameter">
            <summary>
            Specifies the parameters for the PriorBoxParameter.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.PriorBoxParameter.CodeType">
            <summary>
            Defines the encode/decode type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.PriorBoxParameter.CodeType.CORNER">
            <summary>
            Encode the corner.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.PriorBoxParameter.CodeType.CENTER_SIZE">
            <summary>
            Encode the center size.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.PriorBoxParameter.CodeType.CORNER_SIZE">
            <summary>
            Encode the corner size.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.CodeTypeFromString(System.String)">
            <summary>
            Convert a string into a CodeType.
            </summary>
            <param name="str">Specifies the string.</param>
            <returns>The associated CodeType is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.min_size">
            <summary>
            Specifies the minimum box size (in pixels) and is required!
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.max_size">
            <summary>
            Specifies the maximum box size (in pixels) and is required!
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.aspect_ratio">
            <summary>
            Specifies various aspect ratios.  Duplicate ratios are ignored.
            If none are provided, a default ratio of 1 is used.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.flip">
            <summary>
            Specifies whether or not to flip each aspect ratio.
            For example, if there is an aspect ratio 'r'
            we will generate aspect ratio '1.0/r' as well.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.clip">
            <summary>
            Specifies whether or not to clip the prior so that it is within [0,1].
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.variance">
            <summary>
            Specifies the variance for adjusting the prior boxes.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.img_size">
            <summary>
            Specifies the image size.  By default we calculate
            the img_height, img_width, step_x and step_y based
            on bottom[0] (feat) and bottom[1] (img).  Unless these
            values are explicitly provided here.
            
            Either the img_h and img_w are used or the img_size,
            but not both.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.img_h">
            <summary>
            Specifies the image height.  By default we calculate
            the img_height, img_width, step_x and step_y based
            on bottom[0] (feat) and bottom[1] (img).  Unless these
            values are explicitly provided here.
            
            Either the img_h and img_w are used or the img_size,
            but not both.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.img_w">
            <summary>
            Specifies the image width.  By default we calculate
            the img_height, img_width, step_x and step_y based
            on bottom[0] (feat) and bottom[1] (img).  Unless these
            values are explicitly provided here.
            
            Either the img_h and img_w are used or the img_size,
            but not both.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.step">
            <summary>
            Specifies the excplicit step size to use.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.step_h">
            <summary>
            Specifies the explicit step size to use along height.
            
            Either the step_h and step_w are used or the step,
            but not both.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.step_w">
            <summary>
            Specifies the explicit step size to use along width.
            
            Either the step_h and step_w are used or the step,
            but not both.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.PriorBoxParameter.offset">
            <summary>
            Specifies the offset to the top left corner of each cell.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.GetAspectRatios(MyCaffe.param.ssd.PriorBoxParameter)">
            <summary>
            Return the list of aspect ratios to use based on the parameters.
            </summary>
            <param name="p">Specifies the prior box parameters.</param>
            <returns>The list of aspect ratios is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.PriorBoxParameter.Reshape(MyCaffe.param.ssd.PriorBoxParameter,System.Int32,System.Int32,System.Nullable{System.Int32})">
            <summary>
            Calculate the reshape size based on the parameters.
            </summary>
            <param name="p">Specifies the PriorBox parameter.</param>
            <param name="nLayerWid">Specifies the image width.</param>
            <param name="nLayerHt">Specifies the image height.</param>
            <param name="nNumPriors">Optionally, specifies a num priors override (default = null).</param>
            <returns>The new shape is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.ExpansionParameter">
            <summary>
            Specifies the parameters for the ExpansionParameter used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.ExpansionParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.ExpansionParameter.prob">
            <summary>
            Get/set probability of using this expansion policy.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ExpansionParameter.max_expand_ratio">
            <summary>
            Get/set the ratio to expand the image.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.ExpansionParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the object.
            </summary>
            <param name="src">The copy is placed in this parameter.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.ExpansionParameter.Clone">
            <summary>
            Return a clone of the object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.ExpansionParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.ExpansionParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.DistortionParameter">
            <summary>
            Specifies the parameters for the DistortionParameter used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.DistortionParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.random_seed">
            <summary>
            Get/set the random seed (default = 0, only used when testing).
            </summary>
            <remarks>The random seed is only used when using the GPU version of distort image.</remarks>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.use_gpu">
            <summary>
            Get/set whether or not to use the GPU for the distortion operations (default = true).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.brightness_prob">
            <summary>
            Get/set probability of adjusting the brightness (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.brightness_delta">
            <summary>
            Get/set amount to add to the pixel values within [-delta,delta] (default = 0)
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.contrast_prob">
            <summary>
            Get/set probability of adjusting the contrast (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.contrast_lower">
            <summary>
            Get/set lower bound for random contrast factor (default = 0.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.contrast_upper">
            <summary>
            Get/set upper bound for random contrast factor (default = 1.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.saturation_prob">
            <summary>
            Get/set probability of adjusting the saturation (default = 0).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.saturation_lower">
            <summary>
            Get/set lower bound for random saturation factor (default = 0.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.saturation_upper">
            <summary>
            Get/set upper bound for random saturation factor (default = 1.5).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.DistortionParameter.random_order_prob">
            <summary>
            Get/set the probability of randomly ordering the image channels (default = 0).
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.DistortionParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the object.
            </summary>
            <param name="src">The copy is placed in this parameter.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.DistortionParameter.Clone">
            <summary>
            Return a clone of the object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.DistortionParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.DistortionParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.NoiseParameter">
            <summary>
            Specifies the parameters for the NoiseParameter used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.NoiseParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.prob">
            <summary>
            Get/set probability of using this resize policy.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.hist_eq">
            <summary>
            Get/set histogram equalized.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.inverse">
            <summary>
            Get/set color inversion.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.decolorize">
            <summary>
            Get/set grayscale.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.gauss_blur">
            <summary>
            Get/set gaussian blur.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.jpeg">
            <summary>
            Get/set jpeg quality.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.posterize">
            <summary>
            Get/set posterization.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.erode">
            <summary>
            Get/set erosion.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.saltpepper">
            <summary>
            Get/set salt-n-pepper noise.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.saltpepper_param">
            <summary>
            Get/set the salt-n-pepper parameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.clahe">
            <summary>
            Get/set the local histogram equalization.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.convert_to_hsv">
            <summary>
            Get/set color space conversion to hsv.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.NoiseParameter.convert_to_lab">
            <summary>
            Get/set color space convertion to lab.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.NoiseParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the object.
            </summary>
            <param name="src">The copy is placed in this parameter.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.NoiseParameter.Clone">
            <summary>
            Return a clone of the object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.NoiseParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.NoiseParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.SaltPepperParameter">
            <summary>
            Specifies the parameters for the SaltPepperParameter used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.SaltPepperParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.SaltPepperParameter.fraction">
            <summary>
            Get/set the percentage of pixels.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SaltPepperParameter.value">
            <summary>
            Get/set the values.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.SaltPepperParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the object.
            </summary>
            <param name="src">The copy is placed in this parameter.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.SaltPepperParameter.Clone">
            <summary>
            Return a clone of the object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SaltPepperParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SaltPepperParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.ResizeParameter">
            <summary>
            Specifies the parameters for the ResizeParameter for use with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.ResizeParameter.ResizeMode">
            <summary>
            Defines the resizing mode.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.ResizeMode.WARP">
            <summary>
            Specifies to warp the sizing.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.ResizeMode.FIT_SMALL_SIZE">
            <summary>
            Specifies to fit into a small size.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.ResizeMode.FIT_LARGE_SIZE_AND_PAD">
            <summary>
            Specifies to fit into a large padded size.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ssd.ResizeParameter.PadMode">
            <summary>
            Defines the padding mode.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.PadMode.CONSTANT">
            <summary>
            Use constant padding.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.PadMode.MIRRORED">
            <summary>
            Use mirrored padding.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.PadMode.REPEAT_NEAREST">
            <summary>
            Repeat the nearest padding.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ssd.ResizeParameter.InterpMode">
            <summary>
            Defines the interpolation mode.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.InterpMode.LINEAR">
            <summary>
            Use linear interpolation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.InterpMode.AREA">
            <summary>
            Use area interpolation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.InterpMode.NEAREST">
            <summary>
            Use nearest neighbor interpolation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.InterpMode.CUBIC">
            <summary>
            Use cubic interpolation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.ResizeParameter.InterpMode.LANCZOS4">
            <summary>
            Use LanCZos4 interpolation.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.prob">
            <summary>
            Get/set probability of using this resize policy.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.resize_mode">
            <summary>
            Get/set the resizing mode.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.height">
            <summary>
            Get/set the resizing height.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.width">
            <summary>
            Get/set the resizing width.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.height_scale">
            <summary>
            Get/set the resizing height scale used with FIT_SMALL_SIZE mode.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.width_scale">
            <summary>
            Get/set the resizing width scale used with FIT_SMALL_SIZE_mode.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.pad_mode">
            <summary>
            Get/set the pad mode for FIT_LARGE_SIZE_AND_PAD mode.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.pad_value">
            <summary>
            Get/set the pad value which is repeated once for all channels, or provided one pad value per channel.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.ResizeParameter.interp_mode">
            <summary>
            Get/set the interp mode which is repeated once for all channels, or provided once per channel.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the and return a new ResizeParameter. 
            </summary>
            <param name="br"></param>
            <param name="bNewInstance"></param>
            <returns>The new object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.ResizeParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.EmitConstraint">
            <summary>
            Specifies the parameters for the EmitConstraint used with SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ssd.EmitConstraint.EmitType">
            <summary>
            Specifies the emit type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.EmitConstraint.EmitType.CENTER">
            <summary>
            Specifies to center the data.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ssd.EmitConstraint.EmitType.MIN_OVERLAP">
            <summary>
            Specifies to overlap the data.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.EmitConstraint.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.ssd.EmitConstraint.emit_type">
            <summary>
            Get/set the emit type.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.EmitConstraint.emit_overlap">
            <summary>
            Get/set the emit overlap used with MIN_OVERLAP.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.EmitConstraint.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.EmitConstraint.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.EmitConstraint.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.EmitConstraint.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.Sampler">
            <summary>
            Specifies the sample of a bbox in the normalized space [0,1] with provided constraints used in SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.#ctor">
            <summary>
            The Sample constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.Save(System.IO.BinaryWriter)">
            <summary>
            Save the Sample to a binary writer.
            </summary>
            <param name="bw">The binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the Sample from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a the Sample is read into a new instance, otherwise it is read into the current instance.</param>
            <returns>The Sample instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.Load(System.IO.BinaryReader)">
            <summary>
            Load the Sample from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <returns>A new Sample instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.ssd.Sampler.min_scale">
            <summary>
            Get/set the minimum scale of the sampled bbox.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Sampler.max_scale">
            <summary>
            Get/set the maximum scale of the sampled bbox.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Sampler.min_aspect_ratio">
            <summary>
            Get/set the minimum aspect ratio of the sampled bbox.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Sampler.max_aspect_ratio">
            <summary>
            Get/set the maximum aspect ratio of the sampled bbox.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.Clone">
            <summary>
            Creates a copy of the Sample.
            </summary>
            <returns>A new instance of the Sample is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.System#ICloneable#Clone">
            <summary>
            Creates a copy of the Sample.
            </summary>
            <returns>A new instance of the Sample is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.Compare(MyCaffe.param.ssd.Sampler)">
            <summary>
            Compares this Sample to another.
            </summary>
            <param name="bs">Specifies the other Sample to compare this one to.</param>
            <returns>If the two Sample's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.CompareTo(System.Object)">
            <summary>
            Compares this Sample to another.
            </summary>
            <param name="obj">Specifies the other Sample to compare this one to.</param>
            <returns>If the two Sample's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.ToProto(System.String)">
            <summary>
            Converts the Sample to a RawProto.
            </summary>
            <param name="strName">Specifies a name for the RawProto.</param>
            <returns>A new RawProto representing the Sample is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a new Sample from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto containing a representation of the Sample.</param>
            <returns>A new instance of the Sample is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.Sampler.ToString">
            <summary>
            Return the string representation of the shape.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.SamplerConstraint">
            <summary>
            Specifies the constratins for selecting sampled bbox used in SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.#ctor">
            <summary>
            The SampleConstraint constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.Save(System.IO.BinaryWriter)">
            <summary>
            Save the SampleConstraint to a binary writer.
            </summary>
            <param name="bw">The binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the SampleConstraint from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a the SampleConstraint is read into a new instance, otherwise it is read into the current instance.</param>
            <returns>The SampleConstraint instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.Load(System.IO.BinaryReader)">
            <summary>
            Load the SampleConstraint from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <returns>A new SampleConstraint instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.min_jaccard_overlap">
            <summary>
            Get/set the minimum Jaccard overlap between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.max_jaccard_overlap">
            <summary>
            Get/set the maximum Jaccard overlap between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.min_sample_coverage">
            <summary>
            Get/set the minimum Sample coverage between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.max_sample_coverage">
            <summary>
            Get/set the maximum Sample coverage between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.min_object_coverage">
            <summary>
            Get/set the minimum Object coverage between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.SamplerConstraint.max_object_coverage">
            <summary>
            Get/set the maximum Object coverage between sampled bbox and all boxes in AnnotationGroup.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.Clone">
            <summary>
            Creates a copy of the SampleConstraint.
            </summary>
            <returns>A new instance of the SampleConstraint is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.System#ICloneable#Clone">
            <summary>
            Creates a copy of the SampleConstraint.
            </summary>
            <returns>A new instance of the SampleConstraint is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.Compare(MyCaffe.param.ssd.SamplerConstraint)">
            <summary>
            Compares this SampleConstraint to another.
            </summary>
            <param name="bs">Specifies the other SampleConstraint to compare this one to.</param>
            <returns>If the two SampleConstraint's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.CompareTo(System.Object)">
            <summary>
            Compares this SampleConstraint to another.
            </summary>
            <param name="obj">Specifies the other SampleConstraint to compare this one to.</param>
            <returns>If the two SampleConstraint's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.ToProto(System.String)">
            <summary>
            Converts the SampleConstraint to a RawProto.
            </summary>
            <param name="strName">Specifies a name for the RawProto.</param>
            <returns>A new RawProto representing the SampleConstraint is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a new SampleConstraint from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto containing a representation of the SampleConstraint.</param>
            <returns>A new instance of the SampleConstraint is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.SamplerConstraint.ToString">
            <summary>
            Return the string representation of the shape.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.PermuteParameter">
            <summary>
            Specifies the parameters for the PermuteLayer.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.PermuteParameter.order">
            <summary>
            Specifies the new orders of the axes of data.
            </summary>
            <remarks>
            Notice that the data should be with in the same range as
            the input data, and that it starts from 0.
            Do not provide a repeated order.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ssd.PermuteParameter.Reshape(System.Collections.Generic.List{System.Int32},System.Collections.Generic.List{System.Int32},System.Int32)">
            <summary>
            Calculates the top shape by running the Reshape calculation.
            </summary>
            <param name="rgOrder">Specifies the ordering to use.</param>
            <param name="rgShape">Specifies the original shape to re-order.</param>
            <param name="nNumAxes">Specifies the number of axes.</param>
            <returns>The new shape based on the ordering is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ssd.Normalization2Parameter">
            <summary>
            Specifies the parameters for the Normalization2Layer used in SSD.
            </summary>
            <remarks>
            @see [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2016.
            @see [GitHub: SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd), by weiliu89/caffe, 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ssd.Normalization2Parameter.across_spatial">
            <summary>
            Specifies to normalize across the spatial dimensions.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Normalization2Parameter.scale_filler">
            <summary>
            Specifies the filler for the initial value of scale, default is 1.0 for all.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Normalization2Parameter.channel_shared">
            <summary>
            Specifies whether or not the scale parameters are shared across channels.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ssd.Normalization2Parameter.eps">
            <summary>
            Specifies the epsilon for not dividing by zero while normalizing variance.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ssd.Normalization2Parameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ui.CsvConverter">
            <summary>
            The CsvConverter is used to display string lists on a single line in a property grid.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.CsvConverter.ConvertTo(System.ComponentModel.ITypeDescriptorContext,System.Globalization.CultureInfo,System.Object,System.Type)">
            <summary>
            Overrides the ConvertTo method of TypeConverter.
            </summary>
            <param name="context">Specifies the context.</param>
            <param name="culture">Specifies the culture.</param>
            <param name="value">Specifies the value.</param>
            <param name="destinationType">Specifies the distination type.</param>
            <returns>The converted type is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ui.FormProperty">
            <summary>
            The FormProperty window is used to edit a given key/value pair.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.FormProperty.#ctor(System.Boolean,System.String,System.String)">
            <summary>
            The constructor.
            </summary>
            <param name="bNew">Specifies that this is a new item.</param>
            <param name="strName">When editing, specifies the existing name.</param>
            <param name="strVal">When editing, specifies the existing value.</param>
        </member>
        <member name="P:MyCaffe.param.ui.FormProperty.Key">
            <summary>
            Returns the Key (e.g. the property name).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ui.FormProperty.Value">
            <summary>
            Returns the property value.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ui.FormProperty.components">
            <summary>
            Required designer variable.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.FormProperty.Dispose(System.Boolean)">
            <summary>
            Clean up any resources being used.
            </summary>
            <param name="disposing">true if managed resources should be disposed; otherwise, false.</param>
        </member>
        <member name="M:MyCaffe.param.ui.FormProperty.InitializeComponent">
            <summary>
            Required method for Designer support - do not modify
            the contents of this method with the code editor.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ui.DictionaryParamEditor">
            <summary>
            The DictionaryParamEditor is used to visually edit dictionary based parameters that are stored as a key/value set packed within a string.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.DictionaryParamEditor.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.DictionaryParamEditor.GetEditStyle(System.ComponentModel.ITypeDescriptorContext)">
            <summary>
            Returns the edit style DROPDOWN
            </summary>
            <param name="context">Specifies the context (not used).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.ui.DictionaryParamEditor.EditValue(System.ComponentModel.ITypeDescriptorContext,System.IServiceProvider,System.Object)">
            <summary>
            The EditValue displays the editing control and returns the new edited value.
            </summary>
            <param name="context">Specifies the context (not used).</param>
            <param name="provider">Specifies the provider implementing the service.</param>
            <param name="value">Specifies the value to edit.</param>
            <returns>The edited value is returned.</returns>
        </member>
        <member name="F:MyCaffe.param.ui.DictionaryParameterEditorControl.components">
            <summary> 
            Required designer variable.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ui.DictionaryParameterEditorControl.Dispose(System.Boolean)">
            <summary> 
            Clean up any resources being used.
            </summary>
            <param name="disposing">true if managed resources should be disposed; otherwise, false.</param>
        </member>
        <member name="M:MyCaffe.param.ui.DictionaryParameterEditorControl.InitializeComponent">
            <summary> 
            Required method for Designer support - do not modify 
            the contents of this method with the code editor.
            </summary>
        </member>
        <member name="T:MyCaffe.param.DataNoiseParameter">
            <summary>
            The DataNoiseParameter is used by the DataParameter when the 'enable_noise_for_nonmatch' = True, which is used when 'images_per_blob' > 1.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataNoiseParameter.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNoiseParameter.use_noisy_mean">
            <summary>
            (\b optional, default = true) When <i>true</i> the noise is applied to the mean and used as the noisy data.  NOTE: When using this setting, the filler should be configured to produce values in the range [0,1].
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNoiseParameter.noise_data_label">
            <summary>
            (\b optional, default = -1) Specifies the label used with each noise filled data used when 'use_noise_for_nonmatch' = <i>true</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNoiseParameter.noise_filler">
            <summary>
            Specifies the noise filler used when 'use_noise_for_nonmatch' = <i>true</i>.  By default the 'noise_filter' is set to CONSTANT(1) which, when used with the 'use_noisy_mean' = True, uses the mean image as the data noise.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNoiseParameter.noisy_save_path">
            <summary>
            (/b optional, default = null) Specifies the path where the noisy data image is saved, otherwise is ignored when null.  This setting is only used for debugging.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataNoiseParameter.Copy(MyCaffe.param.DataNoiseParameter)">
            <summary>
            Copies the specified source data noise parameter to this one.
            </summary>
            <param name="pSrc">Specifies the source data noise parameter.</param>
        </member>
        <member name="M:MyCaffe.param.DataNoiseParameter.ToProto(System.String)">
            <summary>
            Convert the DataNoiseParameter into a RawProto.
            </summary>
            <param name="strName">Specifies the RawProto name.</param>
            <returns>The RawProto containing the settings is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.DataNoiseParameter.FromProto(MyCaffe.basecode.RawProto,MyCaffe.param.DataNoiseParameter)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <param name="p">Optionally, specifies an instance to load.  If <i>null</i>, a new instance is created and loaded.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.DataNormalizerParameter">
            <summary>
            Specifies the parameter for the data normalizer layer.
            </summary>
            <remarks>
            The data normalizer layer provides a detailed normalization that is applied to the data (and label if desired).
            </remarks>
        </member>
        <member name="T:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP">
            <summary>
            Specifies the normalization step to run.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.CENTER">
            <summary>
            Center the data by subtracting the mean.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.STDEV">
            <summary>
            Normalize the data by dividing by the standard deviation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.RANGE">
            <summary>
            Normalize the data by fitting the data into the 'output_min'/'output_max' range.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.ADDITIVE">
            <summary>
            Add each data value to the previous data value.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.RETURNS">
            <summary>
            Create the percentage change of the current data from the previous.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataNormalizerParameter.NORMALIZATION_STEP.LOG">
            <summary>
            Normalize the data by taking the LOG on each item.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.label_data_channel">
            <summary>
            Specifies to data channel used for the label (if any).
            </summary>
            <remarks>
            Some models, such as LSTM, use input data as part of the label.  The label_data_channel specifies which channel within
            the data is used as the label.  When not specified, the label must have the same number of channels as the data.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.across_data_and_label">
            <summary>
            Specifies to normalize across both the data and label data together.
            </summary>
            <remarks>
            When <i>true</i> centering and other normalization takes place across the entire data range within both the data and label together.
            When <i>false</i> centering and other normalization takes place separately for data and separately for the label (if supplied).
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.steps">
            <summary>
            Specifies the normalization steps which are performed in the order for which they are listed.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.ignore_channels">
            <summary>
            Specifies the channels to ignore and just pass through in their original form.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.input_stdev">
            <summary>
            Specifies the input standard deviation, if known.  When not specified input_stdev is determined dynamically from the data input itself.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.input_mean">
            <summary>
            Specifies the input mean, if known.  When not specified the input_mean is determined dynamically from the data input itself.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.input_min">
            <summary>
            Specifies the minimum data range of the intput, if known.  If both input_min and input_max are 0 the input_min/input_max are determined dynamically from the data input itself.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.input_max">
            <summary>
            Specifies the maximum data range of the intput, if known.  If both input_min and input_max are 0 the input_min/input_max are determined dynamically from the data input itself.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.output_min">
            <summary>
            Specifies the minimum data range of the output.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataNormalizerParameter.output_max">
            <summary>
            Specifies the maximum data range of the output.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.DataNormalizerParameter.FromProto(MyCaffe.basecode.RawProto,MyCaffe.param.DataNormalizerParameter)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <param name="p">Optionally, specifies an instance to load.  If <i>null</i>, a new instance is created and loaded.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.nt.OneHotParameter">
            <summary>
            Specifies the parameters used by the OneHotLayer
            </summary>
            <remarks>
            The OneHotLayer converts each single input into 'num_output' vector items each containing a 1 or 0 
            if the input falls within the range of the given vector item.
            
            For example, when using a min/max range of -1,1 spread across 8 vector items (num_output), inputs
            less than or equal to -1 go in the first bucket, inputs greater than or equal to 1 go in the last
            bucket and values in between -1 and 1 go into their repsective buckets (e.g input -0.12 goes into bucket
            index 3 and input 0.12 goes into bucket 4)
            
            8 inputs span across -1 to 1 range creates the following buckets:
            
            index:        0            1            2            3           4           5           6           7 
            bucket: [-1.00,-0.75][-0.75,-0.50][-0.50,-0.25][-0.25, 0.00][0.00, 0.25][0.25, 0.50][0.50, 0.75][0.75, 1.00]
            
            input: -0.75 or less set bucket #0 = 1
            input:  0.75 or greater set bucket #7 = 1
            
            Except for end buckets, inputs are placed in bucket where:  bucket min &lt;= input &lt; bucket max.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.nt.OneHotParameter.axis">
            <summary>
            Specifies the axis over which to apply the one-hot vectoring.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.OneHotParameter.min_axes">
            <summary>
            Specifies the minimum number of axes.  Axes of size 1 are added to the current axis count up to the minimum.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.OneHotParameter.num_output">
            <summary>
            Specifies the number of items within the one-hot vector output.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.OneHotParameter.min">
            <summary>
            Specifies the minimum data range over which to bucketize for the one-hot vector input.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.OneHotParameter.max">
            <summary>
            Specifies the maximum data range over which to bucketize for the one-hot vector input.
            </summary>
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.nt.OneHotParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.nt.ScalarParameter">
            <summary>
            Specifies the parameters for the ScalarLayer
            </summary>
        </member>
        <member name="T:MyCaffe.param.nt.ScalarParameter.ScalarOp">
            <summary>
            Defines the scalar operations that may be performed.
            </summary>
        </member>
        <member name="F:MyCaffe.param.nt.ScalarParameter.ScalarOp.MUL">
            <summary>
            Specifies to run a mul_scalar on the layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.nt.ScalarParameter.ScalarOp.ADD">
            <summary>
            Specifies to run an add_scalar on the layer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.nt.ScalarParameter.value">
            <summary>
            Specifies the scalar value to apply.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.ScalarParameter.operation">
            <summary>
            Specifies the scalar operation to apply (mul, add, etc).
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.ScalarParameter.passthrough_gradient">
            <summary>
            Specifies whether or not to pass-through the gradient without performing the back-prop calculation (default = <i>false</i>).
            </summary>
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.nt.ScalarParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.nt.TVLossParameter">
            <summary>
            Specifies the parameters for the TVLossLayer
            </summary>
            <remarks>
            @see [ftokarev/caffe-neural-style Github](https://github.com/ftokarev/caffe-neural-style) by ftokarev, 2017. 
            @see [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035) by A. Mahendran and A. Vedaldi, CVPR, 2015.
            @see [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, 2015 
            </remarks>
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.nt.TVLossParameter.beta">
            <summary>
            The beta value.
            </summary>
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.nt.TVLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.nt.GramParameter">
            <summary>
            Specifies the parameters for the GramLayer
            </summary>
            <remarks>
            @see [ftokarev/caffe-neural-style Github](https://github.com/ftokarev/caffe-neural-style) by ftokarev, 2017. 
            @see [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, 2015 
            </remarks>
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.nt.GramParameter.axis">
            <summary>
            The first axis to be lumped into a single Gram matrix computation;
            all preceding axes are retained in the output.
            May be negtive to index from the end (e.g. -1 for the last axis)
            For exapmle, if axis == 2 and the input is (N x C x H x W), the output
            will be (N x C x C)
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.GramParameter.alpha">
            <summary>
            Specifies the scaling factor applied before the gram operation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.GramParameter.beta">
            <summary>
            Specifies the scaling factor applied after the gram operation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.nt.GramParameter.disable_scaling_on_gradient">
            <summary>
            Specifies whether or not to apply the un-scaling of the alpha and beta values during the during the backpropagation (default = <i>false</i>).
            </summary>
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.nt.GramParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ParameterParameter">
            <summary>
            Specifies the parameters for the ParameterLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ParameterParameter.shape">
            <summary>
            Specifies the parameter shape.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ParameterParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.CropParameter">
            <summary>
            Specifies the parameters for the MyCaffe.CropLayer.
            </summary>
            <remarks>
            To crop, elements of the first bottom are selected to fit the dimensions
            of the second, reference bottom.  The crop is configured by 
            - the crop 'axis' to pick the diensions for cropping.
            - the crop 'offset' to set the shift for all/each dimension.
            
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long, Evan Shelhamer, and Trevor Darrell, 2014.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.CropParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.CropParameter.axis">
            <summary>
            The axis along which to crop -- may be negative to index from the
            end (e.g., -1 for the last axis). Default is 2 for spatial crop.
            </summary>
            <remarks>
            All dimensions up to but excluding 'axis' are preserved, while the 
            dimensions  including the trailing 'axis' are cropped.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.CropParameter.offset">
            <summary>
            Specifies the offset to set the shift for all/each dimension. 
            </summary>
            <remarks>
            If only one 'offset is set, then all dimensions are offset by this amount.
            Otherwise, the number of offsets must equal the number of cropped axes to
            shift the crop in each dimension accordingly.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.CropParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.CropParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.CropParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.CropParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.CropParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.GradientScaleParameter">
            <summary>
            Specifies the parameters for the GradientScaleLayer.
            </summary>
            <remarks>
            Scaling is performed according to the schedule:
            @f$ y = \frac{2 \cdot height} {1 + \exp(-\alpha \cot progress)} - upper\_bound @f$,
            where @f$ height = upper\_bound - lower\_bound @f$,
            @f$ lower\_bound @f$ is the smallest scaling factor,
            @f$ upper\_bound @f$ is the largest scaling factor,
            @f$ \alpha @f$ controls how fast the transition occurs between the scaling factors,
            @f$ progress = \min(iter / max\_iter, 1) @f$ corresponds to the current transition
            state (the @f$ iter @f$ is the current iteration of the solver).
            
            The GradientScaleLayer can be used to implement gradient reversals.
            
            @see [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) by Ganin et al., 2015, v4 in 2016.
            @see [Github/ddtm/caffe](https://github.com/ddtm/caffe) for original source.
            </remarks> 
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.GradientScaleParameter.lower_bound">
            <summary>
            Specifies the lower bound of the height used for scaling.
            </summary>
        </member>
        <member name="P:MyCaffe.param.GradientScaleParameter.upper_bound">
            <summary>
            Specifies the upper bound of the height used for scaling.
            </summary>
        </member>
        <member name="P:MyCaffe.param.GradientScaleParameter.alpha">
            <summary>
            Specifies the alpha value applied to the current iter/max_iter, used when scaling.
            </summary>
        </member>
        <member name="P:MyCaffe.param.GradientScaleParameter.max_iter">
            <summary>
            Specifies the maximum iteration used when scaling.
            </summary>
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.GradientScaleParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.MemoryDataParameter">
            <summary>
            Specifies the parameters used by the MemoryDataLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.primary_data">
            <summary>
            (\b optional, default = true) Specifies whether or not the data is the primary datset as opposed to a secondary, target dataset.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.label_type">
            <summary>
            (\b optional, default = SINGLE) Specifies the label type: SINGLE - the default which uses the 'Label' field, or MULTIPLE - which uses the 'DataCriteria' field.  
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.batch_size">
            <summary>
            Batch size.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.channels">
            <summary>
            The number of channels in the data.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.height">
            <summary>
            The height of the data.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.width">
            <summary>
            The width of the data.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.label_channels">
            <summary>
            The number of channels in the label.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.label_height">
            <summary>
            The height of the label.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.label_width">
            <summary>
            The width of the label.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.clip_length1">
            <summary>
            Specifies the clip length 1 (default = 0, which means unused).
            </summary>
            <remarks>
            The clip length 1 is only used when a top named 'clip' exists which is used when feeding data
            into an LSTM layer which requires a 'clip'input.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.MemoryDataParameter.clip_length2">
            <summary>
            Specifies the clip length 2 (default = 0, which means unused).
            </summary>
            <remarks>
            The clip length 2 is only used when a top named 'clip' exists which is used when feeding data
            into an LSTM layer which requires a 'clip'input.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.MemoryDataParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.DebugParameter">
            <summary>
            Specifies the parameters used by the DebugLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.DebugParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.DebugParameter.max_stored_batches">
            <summary>
            Specifies the maximum number of batches to store and search for neighbors.  Each batch input is stored until the maximum count is reached at which time, the oldest batch is released.  A larger max value = more GPU memory used.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DebugParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.DebugParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.DebugParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.DebugParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.DebugParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ClipParameter">
            <summary>
            Stores the parameters used by the ClipLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.ClipParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="M:MyCaffe.param.ClipParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>The reason for using Caffe is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ClipParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.ClipParameter.min">
            <summary>
            Specifies the min value for the Clip activation function.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ClipParameter.max">
            <summary>
            Specifies the max value for the Clip activation function.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ClipParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ClipParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ClipParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.ClipParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ClipParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.MaskParameter">
            <summary>
            Specifies the parameters for the MaskParameter used to mask portions of the transformed data when active.
            </summary>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.#ctor(System.Boolean)">
            <summary>
            The constructor.
            </summary>
            <param name="bActive">Specifies whether or not the parameter is active or not.</param>
        </member>
        <member name="P:MyCaffe.param.MaskParameter.boundary_left">
            <summary>
            Get/set the mask boundary left.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MaskParameter.boundary_right">
            <summary>
            Get/set the mask boundary left.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MaskParameter.boundary_top">
            <summary>
            Get/set the mask boundary top.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MaskParameter.boundary_bottom">
            <summary>
            Get/set the mask boundary bottom.
            </summary>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the and return a new MaskParameter. 
            </summary>
            <param name="br"></param>
            <param name="bNewInstance"></param>
            <returns>The new object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.Copy(MyCaffe.param.OptionalParameter)">
            <summary>
            Copy the source object.
            </summary>
            <param name="src">Specifies the source data.</param>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.Clone">
            <summary>
            Return a copy of this object.
            </summary>
            <returns>A new copy of the object is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.ToProto(System.String)">
            <summary>
            Convert this object to a raw proto.
            </summary>
            <param name="strName">Specifies the name of the proto.</param>
            <returns>The new proto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.MaskParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SwishParameter">
            <summary>
            Stores the parameters used by the SwishLayer
            </summary>
            <remarks>
            @see [Activation Functions](https://arxiv.org/abs/1710.05941v2) by Prajit Ramachandran, Barret Zoph, Quoc V. Le., 2017.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.SwishParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="M:MyCaffe.param.SwishParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.SwishParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.SwishParameter.beta">
            <summary>
            Specifies the beta value for the Swish activation function.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SwishParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.SwishParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.SwishParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.SwishParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.SwishParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.AccuracyParameter">
            <summary>
            Specifies the parameters for the AccuracyLayer.
            </summary>
            <remarks>
            @see [Convolutional Architecture Exploration for Action Recognition and Image Classification](https://arxiv.org/abs/1512.07502v1) by J. T. Turner, David Aha, Leslie Smith, and Kalyan Moy Gupta, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.AccuracyParameter.top_k">
            <summary>
            When computing accuracy, count as correct by comparing the true label to
            the top_k scoring classes.  By default, only compare the top scoring
            class (i.e. argmax).
            </summary>
        </member>
        <member name="P:MyCaffe.param.AccuracyParameter.axis">
            <summary>
            The 'label' axis of the prediction blob, whos argmax corresponds to the
            predicted label -- may be negative to index from the end (e.g., -1 for the
            last axis).  For example, if axis == 1 and the predictions are
            @f$ (N \times C \times H \times W) @f$, the label blob is expected to 
            contain N*H*W ground truth labels with integer values in {0, 1, ..., C-1}.
            </summary>
        </member>
        <member name="P:MyCaffe.param.AccuracyParameter.ignore_label">
            <summary>
            If specified, ignore instances with the given label.
            </summary>
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.AccuracyParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ArgMaxParameter">
            <summary>
            Specifies the parameters for the ArgMaxLayer
            </summary>
            <remarks>
            @see [Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling](https://arxiv.org/abs/1612.06573v1) by Sebastian Ramos, Stefan Gehrig, Peter Pinggera, Uwe Franke, and Carsten Rother, 2016. 
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ArgMaxParameter.out_max_val">
            <summary>
            If true produce pairs (argmax, maxval)
            </summary>
        </member>
        <member name="P:MyCaffe.param.ArgMaxParameter.top_k">
            <summary>
            When computing accuracy, count as correct by comparing the true label to
            the top_k scoring classes.  By default, only compare the top scoring
            class (i.e. argmax).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ArgMaxParameter.axis">
            <summary>
            The axis along which to maximize -- may be negative to index from the
            end (e.g., -1 for the last axis).
            By default ArgMaxLayer maximizes over the flattened trailing dimensions
            for each index of the first / num dimension.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ArgMaxParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.BatchNormParameter">
            <summary>
            Specifies the parameters for the BatchNormLayer.
            </summary>
            <remarks>
            @see [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) by Sergey Ioffe and Christian Szegedy, 2015.
            @see [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737v2) by Alexander Hermans, Lucas Beyer, and Bastian Leibe, 2017. 
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba,  and Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.scale_bias">
            <summary>
            Specifies to use the scale and bias terms, otherwise the scale = 1 and bias = 0
            are used to form an identity operation.
            </summary>
            <remarks>
            NOTE: Currently the scale_bias is only used by the CUDNN engine.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.scale_filler">
            <summary>
            Specifies the scale filler used to fill the scale value.  If null, a constant(1) filler is used.
            </summary>
            <remarks>
            NOTE: Currently the scale_bias is only used by the CUDNN engine.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.bias_filler">
            <summary>
            Specifies the bias filler used to file the bias value.  If null, a constant(0) filler is used.
            </summary>
            <remarks>
            NOTE: Currently the scale_bias is only used by the CUDNN engine.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.use_global_stats">
            <summary>
            If <i>false</i>, normalization is performed over the current mini-batch
            and global statistics are accumulated (but not yet used) by a moving
            average.
            If <i>true</i>, those accumulated mean and variance values are used for the 
            normalization.
            By default, this is set to <i>false</i> when the network is in Phase.TRAINING,
            and <i>true</i> when the network is in the Phase.TESTING mode.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.moving_average_fraction">
            <summary>
            Specifies how much the moving average decays each iteration.  Smaller values
            make the moving average decay faster, giving more weight to the recent values.
            </summary>
            <remarks>
            Each iteration updates the moving average @f$_{t-1}@f$ with the current mean
            @f$ Y_t @f$ by @f$ S_t = (1-\beta)Y_t + \beta \cdot S_{t-1} @f$, where @f$ \beta @f$
            is the moving average fraction parameter.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BatchNormParameter.eps">
            <summary>
            Specifies a small value to add to the variance estimate so that we don't divide by
            zero.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.BatchNormParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.BiasParameter">
            <summary>
            Specifies the parameters for the BiasLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.BiasParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.BiasParameter.axis">
            <summary>
            The first axis of bottom[0] (the first input Blob) along which to apply
            bottom[1] (the second input Blob).  May be negative index from end (e.g.,
            -1 for the last axis).
            </summary>
            <remarks>
            For example, if bottom[0] is 4D with shape 100x3x40x60, the output
            top[0] will have the same shape, and bottom[1] may have any of the
            following shapes (for the given value of axis):
            
               (axis == 0 == -4) 100; 100x3; 100x3x40;  100x3x40x60
               (axis == 1 == -3)          3;     3x40;      3x40x60
               (axis == 2 == -2)                   40;        40x60
               (axis == 3 == -1)                                 60
               
            Furthermore, bottom[1] may have the empty shape (regardless of the value of
            'axis') -- a scalar bias.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BiasParameter.num_axes">
            <summary>
            (num_axes is ignored unless just one bottom is given and the bias is
            a learned parameter of the layer.  Otherwise, num_axes is determined by
            the number of axes of the input (bottom[0] covered by the bias
            parameter, or -1 to cover all axes of bottom[0] starting from 'axis'.
            </summary>
            <remarks>
            Set num_axes := 0 to add a zero-axis Blob: a scalar.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.BiasParameter.filler">
            <summary>
            (filler is ignored unless just one bottom is given and the bias is
            a learned parameter of the layer.)
            The initialization for the learned bias parameter.
            </summary>
            <remarks>
            Default is the zero (0) initialization, resulting in teh BiasLayer
            initially performing the identity operation.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.BiasParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.BiasParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.BiasParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.BiasParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.BiasParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.BlobProtoCollection">
            <summary>
            Specifies a collection of BlobProtos.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.#ctor">
            <summary>
            The BlobProtoCollection Constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProtoCollection.Count">
            <summary>
            Specifies the number of items in the collection.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProtoCollection.Item(System.Int32)">
            <summary>
            Get/set a given element in the collection.
            </summary>
            <param name="nIdx">Specifies the index of the element.</param>
            <returns>The element at the given <i>nIdx</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.Add(MyCaffe.param.BlobProto)">
            <summary>
            Add a new BlobProto to the collection.
            </summary>
            <param name="bp">Specifies the BlobProto.</param>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.RemoveAt(System.Int32)">
            <summary>
            Remove a BlobProto at a given index.
            </summary>
            <param name="nIdx">Specifies the index.</param>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.Remove(MyCaffe.param.BlobProto)">
            <summary>
            Remove a given BlobProto if it exists in the collection.
            </summary>
            <param name="bp">Specifies the BlobProto to remove.</param>
            <returns>If the BlobProto is found in the collection and removed, <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.Clear">
            <summary>
            Remove all elements from the collection.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.GetEnumerator">
            <summary>
            Retrive the enumerator for the collection.
            </summary>
            <returns>The enumerator is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProtoCollection.System#Collections#IEnumerable#GetEnumerator">
            <summary>
            Retrieve the enumerator for the collection.
            </summary>
            <returns>The enumerator is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.BlobProto">
            <summary>
            The BlobProto contains the descripion of a blob.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobProto.#ctor">
            <summary>
            Constructor for the BlobProto.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobProto.#ctor(System.Collections.Generic.List{System.Int32})">
            <summary>
            Constructor for the BlobProto
            </summary>
            <param name="rgShape">Specifies the shape of the blob.</param>
        </member>
        <member name="M:MyCaffe.param.BlobProto.Save(System.IO.BinaryWriter)">
            <summary>
            Saves the BlobProto to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer.</param>
        </member>
        <member name="M:MyCaffe.param.BlobProto.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Loads a BlobProto from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader.</param>
            <param name="bNewInstance">When <i>true</i> a new instance is created, otherwise the data is read into the existing BlobProto.</param>
            <returns>The instance of the BlobProto is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.BlobProto.shape">
            <summary>
            Specifies the shape of the Blob.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.num">
            <summary>
            Specifies the number of inputs (such as images) in the Blob.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.channels">
            <summary>
            Specifies the number of images per input.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.height">
            <summary>
            Specifies the height of each input.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.width">
            <summary>
            Specifies the width of each input.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.double_data">
            <summary>
            Get/set the data as a List of <i>double</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.double_diff">
            <summary>
            Get/set the diff as a List of <i>double</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.data">
            <summary>
            Get/set the data as a List of <i>float</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.BlobProto.diff">
            <summary>
            Get/set the diff as a List of <i>float</i>.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobProto.Clone">
            <summary>
            Copies the BlobProto and returns a new instance.
            </summary>
            <returns>The new instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProto.ToProto(System.String)">
            <summary>
            Converts the BlobProto to a RawProto.
            </summary>
            <param name="strName">Specifies a name for the RawProto.</param>
            <returns>The RawProto representing the BlobProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProto.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses a new BlobProto from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the BlobProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProto.CompareTo(System.Object)">
            <summary>
            Compares the BlobProto to another BlobProto.
            </summary>
            <param name="obj">Specifies the other BlobProto to compare to.</param>
            <returns>If the two BlobProto's are the same <i>true</i> is returned, otherwise <i>false</i>.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobProto.Load(System.IO.BinaryReader)">
            <summary>
            Load a new BlobProto from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader.</param>
            <returns>A new instance of the BlobProto is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.BlobShape">
            <summary>
            Specifies the shape of a Blob.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobShape.#ctor">
            <summary>
            The BlobShape constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobShape.#ctor(System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            The BlobShape constructor.
            </summary>
            <param name="nNum">Specifies the number of data items.</param>
            <param name="nC">Specifies the data channels.</param>
            <param name="nH">Specifies the data height.</param>
            <param name="nW">Specifies the data width.</param>
        </member>
        <member name="M:MyCaffe.param.BlobShape.#ctor(System.Collections.Generic.List{System.Int32})">
            <summary>
            The BlobShape constructor.
            </summary>
            <param name="rgShape">Specifies the shape of a blob.</param>
        </member>
        <member name="M:MyCaffe.param.BlobShape.Save(System.IO.BinaryWriter)">
            <summary>
            Save the BlobShape to a binary writer.
            </summary>
            <param name="bw">The binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.BlobShape.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the BlobShape from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a the BlobShape is read into a new instance, otherwise it is read into the current instance.</param>
            <returns>The BlobShape instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.Load(System.IO.BinaryReader)">
            <summary>
            Load the BlobShape from a binary reader.
            </summary>
            <param name="br">The binary reader to use.</param>
            <returns>A new BlobShape instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.BlobShape.dim">
            <summary>
            The blob shape dimensions.
            </summary>
        </member>
        <member name="M:MyCaffe.param.BlobShape.Clone">
            <summary>
            Creates a copy of the BlobShape.
            </summary>
            <returns>A new instance of the BlobShape is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.System#ICloneable#Clone">
            <summary>
            Creates a copy of the BlobShape.
            </summary>
            <returns>A new instance of the BlobShape is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.Compare(MyCaffe.param.BlobShape)">
            <summary>
            Compares this BlobShape to another.
            </summary>
            <param name="bs">Specifies the other BlobShape to compare this one to.</param>
            <returns>If the two BlobShape's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.CompareTo(System.Object)">
            <summary>
            Compares this BlobShape to another.
            </summary>
            <param name="obj">Specifies the other BlobShape to compare this one to.</param>
            <returns>If the two BlobShape's are the same <i>true</i> is returned, otherwise <i>false</i> is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.ToProto(System.String)">
            <summary>
            Converts the BlobShape to a RawProto.
            </summary>
            <param name="strName">Specifies a name for the RawProto.</param>
            <returns>A new RawProto representing the BlobShape is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a new BlobShape from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto containing a representation of the BlobShape.</param>
            <returns>A new instance of the BlobShape is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.BlobShape.ToString">
            <summary>
            Return the string representation of the shape.
            </summary>
            <returns>The string representation is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ConcatParameter">
            <summary>
            Specifies the parameters for the ConcatLayer
            </summary>
            <remarks>
            @see [Deep Image Aesthetics Classification using Inception Modules and Fine-tuning Connected Layer](https://arxiv.org/abs/1610.02256) by Xin Jin, Jingying Chi, Siwei Peng, Yulu Tian, Chaochen Ye, and Xiaodong Li, 2016.
            @see [Multi-path Convolutional Neural Networks for Complex Image Classification](https://arxiv.org/abs/1506.04701) by Mingming Wang, 2015.
            @see [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567) by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, 2015.
            @see [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261) by Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ConcatParameter.axis">
            <summary>
            The axis along which to concatenate -- may be negative to index from the
            end (e.g., -1 for the last axis).
            
            Othe axes must have the same dimension for all the bottom blobs.
            By default, ConcatLayer concatentates blobs along the 'channels' axis 1.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConcatParameter.concat_dim">
            <summary>
            DEPRECIATED: alias for 'axis' -- does not support negative indexing.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ConcatParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ContrastiveLossParameter">
            <summary>
            Specifies the parameters for the ContrastiveLossLayer.
            </summary>
            <remarks>
            @see [Object cosegmentation using deep Siamese network](https://arxiv.org/pdf/1803.02555.pdf) by Prerana Mukherjee, Brejesh Lall and Snehith Lattupally, 2018.
            @see [Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval](https://arxiv.org/abs/1711.08490) by Yu-An Chung and Wei-Hung Weng, 2017.
            @see [Fully-Convolutional Siamese Networks for Object Tracking](https://arxiv.org/abs/1606.09549) by Luca Bertinetto, Jack Valmadre, Joo F. Henriques, Andrea Vedaldi, and Philip H. S. Torr, 2016.
            @see [Learning visual similarity for product design with convolutional neural networks](https://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf) by Sean Bell and Kavita Bala, Cornell University, 2015. 
            @see [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) by Raia Hadsel, Sumit Chopra, and Yann LeCun, 2006.
            @see [Similarity Learning with (or without) Convolutional Neural Network](http://slazebni.cs.illinois.edu/spring17/lec09_similarity.pdf) by Moitreya Chatterjee and Yunan Luo, 2017. 
            Centroids:
            @see [Retrieving Similar E-Commerce Images Using Deep Learning](https://arxiv.org/abs/1901.03546) by Rishab Sharma and Anirudha Vishvakarma, arXiv:1901.03546, 2019.
            @see [A New Loss Function for CNN Classifier Based on Pre-defined Evenly-Distributed Class Centroids](https://arxiv.org/abs/1904.06008) by Qiuyu Zhu, Pengju Zhang, and Xin Ye, arXiv:1904.06008, 2019.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.ContrastiveLossParameter.DISTANCE_CALCULATION">
            <summary>
            Defines the distance calculation to use.
            </summary>
            <remarks>
            @see [Various types of Distance Metrics in Machine Learning](https://medium.com/analytics-vidhya/various-types-of-distance-metrics-machine-learning-cc9d4698c2da) by Sourodip Kundu, Medium, 2019.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.DISTANCE_CALCULATION.EUCLIDEAN">
            <summary>
            Specifies to use the Euclidean Distance (default).
            </summary>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.DISTANCE_CALCULATION.MANHATTAN">
            <summary>
            Specifies to use the Manhattan Distance.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ContrastiveLossParameter.CENTROID_LEARNING">
            <summary>
            Defines the type of centroid learning to use.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.CENTROID_LEARNING.NONE">
            <summary>
            Specifies to not use centroid learning (default).
            </summary>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.CENTROID_LEARNING.MATCHING">
            <summary>
            Specifies to only use centroid learning on matching pairs.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.CENTROID_LEARNING.NONMATCHING">
            <summary>
            Specifies to only use centroid learning on non-matching pairs.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ContrastiveLossParameter.CENTROID_LEARNING.ALL">
            <summary>
            Specifies to use centroid learning on both matching and non-matching pairs.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.margin">
            <summary>
            Margin for dissimilar pair.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.legacy_version">
            <summary>
            The first implementation of this cost did not exactly match the cost of
            Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.
            
            legacy_version = false (the default) uses (margin - d)^2 as proposed in the
            Hadsell paper.  New models should probably use this version.
            
            legacy_version = true uses (margin - d^2).  This is kept to support /
            repoduce existing models and results.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.output_matches">
            <summary>
            Optionally, specifies to output match information (default = false).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.centroid_learning">
            <summary>
            Optionally, specifies to use centroid learning as soon as the centroids (from the DecodeLayer) are ready - e.g. they have an asum > 0 (default = false, meaning no centroid learning occurs).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.distance_calculation">
            <summary>
            Optionally, specifies the distance calculation to use when calculating the distance between encoding pairs (default = EUCLIDEAN).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ContrastiveLossParameter.matching_distance_scale">
            <summary>
            Optionally, specifies the scale applied to the matching distance when calculating the loss (default = 1.0, which ignores the scaling).
            </summary>
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ContrastiveLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ConvolutionParameter">
            <summary>
            Specifies the parameters for the ConvolutionLayer.  The default weight filler is set to the XavierFiller, and the default bias filler is set to ConstantFiller with a value of 0.1.
            </summary>
            <remarks>
            @see [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner, 1998.
            @see [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) by Vincent Dumoulin, and Francesco Visin, 2016.
            @see [Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks](https://arxiv.org/abs/1704.08331) by Nazrul Haque, N. Dinesh Reddy, and K. Madhava Krishna, 2017. 
            @see [A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction](https://arxiv.org/abs/1701.06190v1) by Yoonsik Kim, Insung Hwang, and Nam Ik Cho, 2017.
            @see [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) by Jonathan Long, Evan Shelhamer, and Trevor Darrell, 2014.
            @see [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) by Fisher Yu, and Vladlen Koltun, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.#ctor">
            @copydoc KernelParameter 
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.useCaffeReason(System.Int32)">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.useCudnn(System.Int32)">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <param name="nNumSpatialAxes">Specifies the number of spatial axes used.  For example typically four spatial axes are used: N, C, H, W.</param>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.cudnn_workspace_allow_on_groups">
            <summary>
            When true, allows workspace usage on groups > 1 (default = false).
            </summary>
            <remarks>
            Currently using workspaces on groups > 1 can cause cuDnn memory errors and for this reason
            the default to this setting is false.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.cudnn_workspace_limit">
            <summary>
            Specifies the workspace limit used by cuDnn.  A value of 0 directs cuDNN to use the fastest algorithm possible.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.cudnn_enable_tensor_cores">
            <summary>
            Specifies to enable the CUDA tensor cores when performing the convolution which is faster but not supported by all GPU's.
            </summary>
            <remarks>
            When run on GPU's that do not support Tensor cores, the default math (non-tensor core) is used.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.num_output">
            <summary>
            The number of outputs for the layer.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.bias_term">
            <summary>
            Whether to have bias terms or not.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.group">
            <summary>
            The group size for group convolution.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.weight_filler">
            <summary>
            The filler for the weight.  The default is set to use the 'xavier' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.bias_filler">
            <summary>
            The filler for the bias.  The default is set to use the 'constant = 0.1' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.axis">
            <summary>
            The axis to interpret as 'channels' when performing convolution.
            Preceding dimensions are treated as independent inputs;
            succeeding dimensions are treated as 'spatial'.
            With @f$ (N \times C \times H \times W) @f$ inputs, and axis == 1 (the default), we perform
            N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for
            groups g>1) filters across the spatial axes (H, W) of the input.
            With @f$ (N \times C \times D \times H \times W) @f$ inputs, and axis == 1, we perform
            N independent 3D convolutions, sliding (C/g)-channels
            filters across teh spatial axes @f$ (D \times H \times W) @f$ of the input.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ConvolutionParameter.force_nd_im2col">
            <summary>
            Whether to force use of the general ND convolution, even if a specific
            implementation for blobs of the appropriate number of spatial dimensions
            is available.  (Currently, there is only a 2D-specific convolution
            implementation; for input blobs with num_axes != 2, this option
            is ignored and the ND implementation will be used.)
            </summary>
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc KernelParameter::Load 
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc KernelParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.Clone">
            @copydoc KernelParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.ToProto(System.String)">
            @copydoc KernelParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.ConvolutionParameter.FromProto(MyCaffe.basecode.RawProto)">
            @copydoc KernelParameter::FromProto 
        </member>
        <member name="T:MyCaffe.param.InputParameter">
            <summary>
            Specifies the parameters for the InputLayer.
            </summary>
            <remarks>
            This layer produces N >= 1 top blob(s) to be assigned manually.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.InputParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.InputParameter.shape">
            <summary>
            Define N shapes to set a shape for each top.
            Define 1 shape to set the same shape for every top.
            Define no shape to defer to reshaping manually.
            </summary>
        </member>
        <member name="M:MyCaffe.param.InputParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.InputParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.InputParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.InputParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.InputParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.LabelMappingParameter">
            <summary>
            /b DEPRECIATED (use DataLayer DataLabelMappingParameter instead) Specifies the parameters for the LabelMappingLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.LabelMappingParameter.mapping">
            <summary>
            Specifies the label mapping where the original label is mapped to the new label specified.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LabelMappingParameter.update_database">
            <summary>
            Specifies whether or not to directly update the database with the label mapping for the data source used - when 'false' only the 'in-memory' labels are updated.  WARNING: Updating the database sets the label mapping globally and will impact all other projects using this data source.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LabelMappingParameter.reset_database_labels">
            <summary>
            Specifies whether or not to reset the database labels to the original label values for the data source used.  <b>WARNING:</b> This resets the labels globally to their original setting and will impact all other projects using this data source.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LabelMappingParameter.label_boosts">
            <summary>
            DEPRECIATED: Specifies the labels for which the label boost is to be set.  When set, all labels specified are given a boost such that images are selected with equal probability between all labels specified.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.MapLabel(System.Int32)">
            <summary>
            Queries the mapped label for a given label.
            </summary>
            <param name="nLabel">Specifies the label to query the mapped label from.</param>
            <returns>The mapped label is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.LabelMappingParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.DataParameter">
            <summary>
            Specifies the parameter for the data layer.
            </summary>
            <remarks>
            Note: given the new use of the Transformation Parameter, the
            depreciated elements of the DataParameter have been removed.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.DataParameter.DB">
            <summary>
            Defines the database type to use.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataParameter.DB.NONE">
            <summary>
            Specifies that no backend database is used as is the case with the ImageDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.DataParameter.DB.IMAGEDB">
            <summary>
            Specifies to use the MyCaffeImageDatabase.  Currently this is the only option.
            </summary>
        </member>
        <member name="E:MyCaffe.param.DataParameter.OnVerifyBatchSize">
            <summary>
            This event is, optionally, called to verify the batch size of the DataParameter.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.DataParameter.source">
            <summary>
            When used with the DATA parameter, specifies the data 'source' within the database.  Some sources are used for training whereas others are used for testing.  When used with the IMAGE_DATA parameter, the 'source' specifies the data 'source' file containing the list of image file names.  Each dataset has both a training and testing data source.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.batch_size">
            <summary>
            Specifies the batch size.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.backend">
            <summary>
            Specifies the backend database.
            </summary>
            <remarks>
            NOTE: Currently only the IMAGEDB is supported, which is a separate
            component used to load and manage all images within a given dataset.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DataParameter.prefetch">
            <summary>
            Prefetch queue (Number of batches to prefetch to host memory, increase if
            data access bandwidth varies).
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.enable_random_selection">
            <summary>
            (\b optional, default = null) Specifies whether or not to randomly query images from the data source.  When enabled, images are queried in sequence which can often have poorer training results.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.enable_pair_selection">
            <summary>
            (\b optional, default = null) Specifies whether or not to select images in a pair sequence.  When enabled, the first image queried is queried using the 'random' selection property, and then the second image queried is the image just after the first image queried (even if queried randomly).
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.display_timing">
            <summary>
            (\b optional, default = false) Specifies whether or not to display the timing of each image read.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.label_type">
            <summary>
            (\b optional, default = SINGLE) Specifies the label type: SINGLE - the default which uses the 'Label' field, or MULTIPLE - which uses the 'DataCriteria' field.  
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.primary_data">
            <summary>
            (\b optional, default = true) Specifies whether or not the data is the primary datset as opposed to a secondary, target dataset.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.synchronize_target">
            <summary>
            (\b optional, default = false) Specifies whether or not this is a to be synchronized with another data layer as the target.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.synchronize_with">
            <summary>
            (\b optional, default = null) Specifies a secondary (target) dataset to syncrhonize with.
            </summary>
            <remarks>
            When synchronizing with another dataset the ordering of labels is guaranteed to be the same from both data sets even though
            the images selected are selected at random.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DataParameter.images_per_blob">
            <summary>
            (\b optional, default = 1) Specifies the number of images to load into each blob channel.  For example when set to 2 two 3 channel images are loaded and stacked on the channel dimension,
            thus loading a 6 channel blob (2 images x 3 channels each). 
            </summary>
            <remarks>
            Loading images in pairs (images_per_blob = 2) is used with the siamese network, where the channel of each blob contains the first image followed by the second image.  The total individual
            image channel count equals the blob channel count divided by 2.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DataParameter.output_all_labels">
            <summary>
            (\b optional, default = false) When using images_per_blob > 1, 'output_all_labels' specifies to output all labels for the stacked images instead of just the comparison.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.balance_matches">
            <summary>
            (\b optional, default = true) When using images_per_blob > 1, 'balance_matches' specifies to query images by alternating similar matches followed by dissimilar matches in the next query.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.output_image_information">
            <summary>
            (\b optional, default = false) When <i>true</i> image information such as index and label are output. IMPORTANT: enabling this setting can dramatically slow down training and is only used for debugging.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.forced_primary_label">
            <summary>
            (\b optional, default = -1) When >= 0, this label is used as the primary image label when 'images_per_blob' > 1.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.enable_noise_for_nonmatch">
            <summary>
            (\b optional, default = false) When <i>true</i> an image consisting of noise initialized with noise filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.data_noise_param">
            <summary>
            Specifies the DataNoiseParameter used when 'enable_noise_for_nonmatch' = True.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.enable_debug_output">
            <summary>
            (\b optional, default = false) When <i>true</i> the data sent out through the top are saved as images into the debug directory specified by the data_debug_param.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DataParameter.data_debug_param">
            <summary>
            Specifies the DataDebugParameter used when 'enable_debug_output' = True.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.DataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.DataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.DataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.DataParameter.FromProto(MyCaffe.basecode.RawProto,MyCaffe.param.DataParameter)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <param name="p">Optionally, specifies an instance to load.  If <i>null</i>, a new instance is created and loaded.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.VerifyBatchSizeArgs">
            <summary>
            The VerifyBatchSizeArgs class defines the arguments of the OnVerifyBatchSize event.
            </summary>
        </member>
        <member name="M:MyCaffe.param.VerifyBatchSizeArgs.#ctor(System.UInt32)">
            <summary>
            VerifyBatchSizeArgs constructor.
            </summary>
            <param name="uiBatchSize"></param>
        </member>
        <member name="P:MyCaffe.param.VerifyBatchSizeArgs.Error">
            <summary>
            Get/set the error value.  For example if the receiver of the event determines that the batch size is in error, 
            then the receiver should set the error appropriately.
            </summary>
        </member>
        <member name="P:MyCaffe.param.VerifyBatchSizeArgs.ProposedBatchSize">
            <summary>
            Specifies the proposed batch size that the DataLayer would like to use.
            </summary>
        </member>
        <member name="T:MyCaffe.param.DropoutParameter">
            <summary>
            Specifies the parameters of the DropoutLayer.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhavsky, and Ruslan R. Salakhutdinov, 2012.
            @see [Information Dropout: Learning Optimal Representations Through Noisy Computation](https://arxiv.org/abs/1611.01353) by Alessandro Achille, and Stevano Soatto, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.DropoutParameter.dropout_ratio">
            <summary>
            Specifies the dropout ratio. (e.g. the probability that values will be dropped out and set to zero.  A value of 0.25 = 25% chance that a value is set to 0, and dropped out.)
            </summary>
        </member>
        <member name="P:MyCaffe.param.DropoutParameter.seed">
            <summary>
            Specifies the seed used by cuDnn for random number generation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DropoutParameter.active">
            <summary>
            Specifies whether or not the dropout is active or not.  When inactive and training, the dropout acts the same as it does during testing and is ignored.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.ToProto(System.String)">
            @copydoc EngineParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.DropoutParameter.FromProto(MyCaffe.basecode.RawProto)">
            @copydoc EngineParameter::FromProto 
        </member>
        <member name="T:MyCaffe.param.DummyDataParameter">
            <summary>
            This layer produces N >= 1 top blobs.  DummyDataParameter must specify 1 or 
            shape fields, and 0, 1 or N data fillers.
            This layer is initialized with the ReLUParameter.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.force_refill">
            <summary>
            (\b optional, default = true) Specifies whether to force refill the data (even constant data) as opposed to only refilling once.
            </summary>
            <remarks>
            Given that the training and testing nets share memory, we have changed the default from <i>false</i> to <i>true</i> so that the 
            values are refilled on every forward pass.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.primary_data">
            <summary>
            (\b optional, default = true) Specifies whether or not the data is the primary datset as opposed to a secondary, target dataset.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.data_filler">
            <summary>
            If 0 data fillers are specified, ConstantFiller
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.shape">
            <summary>
            Specifies the shape of the dummy data where:
              shape(0) = num
              shape(1) = channels
              shape(2) = height
              shape(3) = width
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.num">
            <summary>
            <b>DEPRECIATED</b> - 4D dimensions, use 'shape' instead.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.channels">
            <summary>
            <b>DEPRECIATED</b> - 4D dimensions, use 'shape' instead.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.height">
            <summary>
            <b>>DEPRECIATED</b> - 4D dimensions, use 'shape' instead.
            </summary>
        </member>
        <member name="P:MyCaffe.param.DummyDataParameter.width">
            <summary>
            <b>DEPRECIATED</b> - 4D dimensions, use 'shape' instead.
            </summary>
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.DummyDataParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.EltwiseParameter">
            <summary>
            Specifies the parameters for the EltwiseLayer.
            </summary>
            <remarks>
            @see [DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification](https://arxiv.org/abs/1611.05271v1) by Shu Zhang, Ran He, and Tieniu Tan, 2016. 
            @see [Mixed context networks for semantic segmentation](https://arxiv.org/abs/1610.05854v1) by Haiming Sun, Di Xie, and Shiliang Pu, 2016. 
            @see [Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks](https://arxiv.org/abs/1511.06314v1) by Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra, 2015.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.EltwiseParameter.EltwiseOp">
            <summary>
            Defines the operation to perform.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EltwiseParameter.EltwiseOp.PROD">
            <summary>
            Perform an eltwise product.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EltwiseParameter.EltwiseOp.SUM">
            <summary>
            Perform an eltwise summation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EltwiseParameter.EltwiseOp.MAX">
            <summary>
            Find the eltwise maximum.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.EltwiseParameter.operation">
            <summary>
            Specifies the element-wise operation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EltwiseParameter.coeff">
            <summary>
            Specifies the blob-wise coefficient for SUM operation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EltwiseParameter.stable_prod_grad">
            <summary>
            Specifies whether or not to use an asymptotically slower (for > 2 inputs) but stabler method
            of computing the gradient for PROD operation.  (No effect for SUM op.)
            </summary>
        </member>
        <member name="P:MyCaffe.param.EltwiseParameter.coeff_blob">
            <summary>
            If true and the EltwiseOp is SUM, the last bottom blob is a singleton
            coefficient for the first N-1 bottom blobs, with shape @f$ (N-1 \times 1 \times 1 \times 1) @f$.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.EltwiseParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.EluParameter">
            <summary>
            Specifies the parameters for the EluLayer.
            </summary>
            <remarks>
            @see [Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/abs/1604.04112) by Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde, and Sandip Shingade, 2016.
            @see [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289) by Djork-Arn Clevert, Thomas Unterthiner, and Sepp Hochreiter, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.EluParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="M:MyCaffe.param.EluParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.EluParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.EluParameter.alpha">
            <summary>
            Described in [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289) by Clevert, et al., 2015
            </summary>
            <remarks>
            Also see [Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/abs/1604.04112) by Shah, et al., 2016
            </remarks>
        </member>
        <member name="M:MyCaffe.param.EluParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.EluParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.EluParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.EluParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.EluParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.EmbedParameter">
            <summary>
            Specifies the parameters used by the EmbedLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.EmbedParameter.num_output">
            <summary>
             Specifies the number of outputs for the layer.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EmbedParameter.input_dim">
            <summary>
            Specifies the input given as integers to be interpreted as one-hot
            vector indices with dimension num_input.  Hence num_input should be
            1 greater than the maximum possible input value.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EmbedParameter.bias_term">
            <summary>
            Specifies whether to use a bias term or not.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EmbedParameter.weight_filler">
            <summary>
            Specifies the filler for the weights.
            </summary>
        </member>
        <member name="P:MyCaffe.param.EmbedParameter.bias_filler">
            <summary>
            Specifies the filler for the bias.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.EmbedParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.EngineParameter">
            <summary>
            Specifies whether to use the [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) version 
            or Caffe version of a given forward/backward operation.
            </summary>
        </member>
        <member name="T:MyCaffe.param.EngineParameter.Engine">
            <summary>
            Defines the type of engine to use.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EngineParameter.Engine.DEFAULT">
            <summary>
            Use the default engine that best suits the given layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EngineParameter.Engine.CAFFE">
            <summary>
            Use the Caffe version of the layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.EngineParameter.Engine.CUDNN">
            <summary>
            Use the [NVIDIA cuDnn](https://developer.nvidia.com/cudnn) version of the layer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EngineParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.EngineParameter.engine">
            <summary>
            Specifies the Engine in use.
            </summary>
        </member>
        <member name="M:MyCaffe.param.EngineParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.EngineParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.EngineParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.EngineParameter.ToProto(System.String)">
            <summary>
            Convert the parameter into a RawProto.
            </summary>
            <param name="strName">Specifies the name to associate with the RawProto.</param>
            <returns>The new RawProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.EngineParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ExpParameter">
            <summary>
            Specifies the parameters for the ExpLayer.
            </summary>
            <remarks>
            The ExpLayer computes outputs:
            
            @f$ y = base ^ (shift + scale * x) @f$, for base > 0.
            
            Or if base is set to the default (-1), base is set to e.,
            so:
            
            @f$ y = exp(shift + scale * x) @f$
            </remarks> 
        </member>
        <member name="M:MyCaffe.param.ExpParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ExpParameter.base_val">
            <summary>
            Specifies the base to use for the exponent, where @f$ y = base ^ (shift + scale * x) @f$, for base > 0. 
            </summary>
        </member>
        <member name="P:MyCaffe.param.ExpParameter.scale">
            <summary>
            Specifies the scale to use for the exponent, where @f$ y = base ^ (shift + scale * x) @f$, for base > 0.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ExpParameter.shift">
            <summary>
            Specifies the shift to use for the exponent, where @f$ y = base ^ (shift + scale * x) @f$, for base > 0.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ExpParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ExpParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ExpParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ExpParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ExpParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.FillerParameter">
            <summary>
            Specifies the filler parameters used to create each Filler.
            </summary>
        </member>
        <member name="T:MyCaffe.param.FillerParameter.FillerType">
            <summary>
            Defines the type of filler.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.CONSTANT">
            <summary>
            The constant filler fills a blob with constant values.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.UNIFORM">
            <summary>
            The uniform filler fills a blob with values from a uniform distribution.
            </summary>
            <remarks>
            @see [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) Wikipedia.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.GAUSSIAN">
            <summary>
            The gaussian filler fills a blob with values from a gaussian distribution.
            </summary>
            <remarks>
            @see [Guassian Distribution](https://en.wikipedia.org/wiki/Normal_distribution) Wikipedia.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.XAVIER">
            <summary>
            The xavier filler fills a blob with values from a xavier distribution.
            </summary>
            <remarks>
            @see [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by Glorot, Xavier and Bengio, Yoshua, 2010.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.MSRA">
            <summary>
            The msra filler fills a blob with values from a msra distribution.
            </summary>
            <remarks>
            @see [Learning hierarchical categories in deep neural networks](http://web.stanford.edu/class/psych209a/ReadingsByDate/02_15/SaxeMcCGanguli13.pdf) by Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya, 2013.
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) by He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian, 2015.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.POSITIVEUNITBALL">
            <summary>
            The positive unit ball filler fills a blob with values from a positive unit ball distribution.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.FillerType.BILINEAR">
            <summary>
            The bilinear filler fills a blob with values from a bilinear distribution.
            </summary>
        </member>
        <member name="T:MyCaffe.param.FillerParameter.VarianceNorm">
            <summary>
            Defines the variance normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.VarianceNorm.FAN_IN">
            <summary>
            Specifies a fan-in variance normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.VarianceNorm.FAN_OUT">
            <summary>
            Specifies a fan-out variance normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.FillerParameter.VarianceNorm.AVERAGE">
            <summary>
            Specifies an average variance normalization.
            </summary>
        </member>
        <member name="M:MyCaffe.param.FillerParameter.#ctor(System.String,System.Double,System.Double,System.Double)">
            <summary>
            Filler parameter constructor
            </summary>
            <remarks>
            NOTE: Caffe defaults to 'constant', however this causes models that do not specifically specify
            a filler to run with constant 0 filled weights.  Using a 'gaussian' as the default fixes
            this and fills the weights with random numbers.
            </remarks>
            <param name="strType">Optionally, specifies the type of filler to use.  Default = "gaussian"</param>
            <param name="dfVal">Optionally, specifies the value.  Default = 0.0</param>
            <param name="dfMean">Optionally, specifies the mean.  Default = 0.0</param>
            <param name="dfStd">Optionally, specifies the standard deviation.  Default = 1.0</param>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.type">
            <summary>
            Specifies the type of filler to use.
            </summary>
        </member>
        <member name="M:MyCaffe.param.FillerParameter.GetFillerName(MyCaffe.param.FillerParameter.FillerType)">
            <summary>
            Queries the filler text name corresponding to the FillerType.
            </summary>
            <param name="type">Specifies the FillerType.</param>
            <returns>The string associated with the FillerType is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.value">
            <summary>
            Specifies the value used by 'constant' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.min">
            <summary>
            Specifies the minimum value to use with the 'uniform' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.max">
            <summary>
            Specifies the maximum value to use with the 'uniform' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.mean">
            <summary>
            Specifies the mean value to use with the 'gaussian' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.std">
            <summary>
            Specifies the standard deviation value to use with the 'gaussian' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.sparse">
            <summary>
            Specifies the sparcity value to use with the 'guassian' filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.FillerParameter.variance_norm">
            <summary>
            Specifies the variance normalization method to use with the 'xavier' and 'mrsa' fillers.
            </summary>
        </member>
        <member name="M:MyCaffe.param.FillerParameter.Clone">
            <summary>
            Creates a new copy of this instance of the parameter.
            </summary>
            <returns>A new instance of this parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.FillerParameter.ToProto(System.String)">
            <summary>
            Convert the parameter into a RawProto.
            </summary>
            <param name="strName">Specifies the name to associate with the RawProto.</param>
            <returns>The new RawProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.FillerParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.FlattenParameter">
            <summary>
            Specifies the parameters for the FlattenLayer.
            </summary>
            <remarks>
            @see [Representation Learning and Pairwise Ranking for Implicit and Explicit Feedback in Recommendation Systems](https://arxiv.org/abs/1705.00105v1) by Mikhail Trofimov, Sumit Sidana, Oleh Horodnitskii, Charlotte Laclau, Yury Maximov, and Massih-Reza Amini, 2017. 
            @see [Deep Neural Networks to Enable Real-time Multimessenger Astrophysics](https://arxiv.org/abs/1701.00008v2) by Daniel George, and E. A. Huerta, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.FlattenParameter.axis">
            <summary>
            Specifies the first axis to flatten: all preceding axes are retained in the output.
            May be negative to index from the end (e.g., -1 for the last axis).
            </summary>
        </member>
        <member name="P:MyCaffe.param.FlattenParameter.end_axis">
            <summary>
            Specifies the last axis to flatten: all following axes are retained in the output.
            May be negative to index from the end (e.g., -1 for the last axis).
            </summary>
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.FlattenParameter.Reshape(System.Int32,System.Int32,System.Collections.Generic.List{System.Int32},System.Int32,System.Int32)">
            <summary>
            Calculate the reshape array given the parameters.
            </summary>
            <param name="nParamAxis">Specifies the parameter start axis.</param>
            <param name="nParamEndAxis">Specifies the parameter end axis.</param>
            <param name="rgShape">Specifies the shape parameter.</param>
            <param name="nStartAxis">Specifies the already initialized canonical start axis, or -1 if not initialized (default = -1).</param>
            <param name="nEndAxis">Specifies the already initialized canonical end axis or -1 if not initialized (default = -1).</param>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.param.HingeLossParameter">
            <summary>
            Specifies the parameters for the HingLossLayer.
            </summary>
            <remarks>
            @see [CNN-based Patch Matching for Optical Flow with Thresholded Hinge Loss](https://arxiv.org/abs/1607.08064) by Christian Bailer, Kiran Varanasi, and Didier Stricker, 2016.
            @see [Hinge-Loss Markov Random Fields and Probabilistic Soft Logic](https://arxiv.org/abs/1505.04406) by Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor, 2015.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.HingeLossParameter.Norm">
            <summary>
            Defines the type of normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.HingeLossParameter.Norm.L1">
            <summary>
            L1 normalization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.HingeLossParameter.Norm.L2">
            <summary>
            L2 normalization.
            </summary>
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.HingeLossParameter.norm">
            <summary>
            Specify the Norm to use L1 or L2
            </summary>
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.HingeLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.InfogainLossParameter">
            <summary>
            Specifies the parameters for the InfogainLossLayer.
            </summary>
            <remarks>
            @see [DeepGaze II: Reading fixations from deep features trained on object recognition](https://arxiv.org/abs/1610.01563) by Matthias Kmmerer, Thomas S. A. Wallis, and Matthias Bethge, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.InfogainLossParameter.source">
            <summary>
            Specifies the infogain matrix source.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InfogainLossParameter.axis">
            <summary>
            [\b optional, default = 1] Specifies the axis of the probability.
            </summary>
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.InfogainLossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.InnerProductParameter">
            <summary>
            Specifies the parameters for the InnerProductLayer.
            </summary>
            <remarks>
            @see [Product-based Neural Networks for User Response Prediction](https://arxiv.org/abs/1611.00144) by Yanru Qu, Kan Cai, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang, 2016. 
            </remarks>
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.enable_noise">
            <summary>
            Enable/disable noise in the inner-product layer (default = false).
            </summary>
            <remarks>
            When enabled, noise is only used during the training phase.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.sigma_init">
            <summary>
            Specifies the initialization value for the sigma weight and sigma bias used when 'enable_noise' = <i>true</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.num_output">
            <summary>
            The number of outputs for the layer.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.min_top_axes">
            <summary>
            Optionally, specifies the minimum top axes (default = -1, which ignores this setting).
            </summary>
            <remarks>
            NOTE: The Deconvolution Layer requires 'min_top_axes' = 4.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.bias_term">
            <summary>
            Whether to have bias terms or not.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.weight_filler">
            <summary>
            The filler for the weights.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.bias_filler">
            <summary>
            The filler for the bias.
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.axis">
            <summary>
            Specifies the first axis to be lumped into a single inner product computation;
            all preceding axes are retained in the output.
            May be negative to index from the end (e.g., -1 for the last axis)
            </summary>
        </member>
        <member name="P:MyCaffe.param.InnerProductParameter.transpose">
            <summary>
            Specifies whether to transpose the weight matrix or not.
            If transpose == true, any operations will be performed on the transpose
            of the weight matrix.  The weight matrix itself is not going to be transposed
            but rather the transfer flag of operations will be toggled accordingly.
            </summary>
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.InnerProductParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.KernelParameter">
            <summary>
            Specifies the basic kernel parameters (used by convolution and pooling)
            </summary>
        </member>
        <member name="M:MyCaffe.param.KernelParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="P:MyCaffe.param.KernelParameter.pad">
            <summary>
            Pad is given as a single value for equal dimensions in all 
            spatial dimensions, or once per spatial dimension.
            </summary>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.stride">
            <summary>
            Stride is given as a single value for equal dimensions in all 
            spatial dimensions, or once per spatial dimension.
            </summary>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.kernel_size">
            <summary>
            Kernel size is given as a single value for equal dimensions in all 
            spatial dimensions, or once per spatial dimension.
            </summary>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.dilation">
            <summary>
            Factor used to dilate the kernel, (implicitly) zero-filling the resulting
            holes.  (Kernel dilation is sometimes referred to by its use in the
            algorithm 'a trous from Holschneider et al. 1987.)
            </summary>
            <remarks>
            Dilation is used by the MyCaffe.ConvolutionLayer and MyCaffe.Im2colLayer.
            
            @see [A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform](https://link.springer.com/chapter/10.1007/978-3-642-75988-8_28) by Holschneider, et al., 1990.
            @see [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) by Yu, et al., 2015.
            @see [Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks](https://arxiv.org/abs/1704.08331) by Haque, et al., 2017. 
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.pad_h">
            <summary>
            The padding height (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.pad_w">
            <summary>
            The padding width (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.stride_h">
            <summary>
            The stride height (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.stride_w">
            <summary>
            The stride width (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.kernel_h">
            <summary>
            The kernel height (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.KernelParameter.kernel_w">
            <summary>
            The kernel width (2D only)
            </summary>
            <remarks>
            For 2D only, the H and W versions may also be used to
            specify both spatial dimensions.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.KernelParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.KernelParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.KernelParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.KernelParameter.ToProto(System.String)">
            <summary>
            Convert the parameter into a RawProto.
            </summary>
            <param name="strName">Specifies the base name for the raw proto.</param>
            <returns>The RawProto is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.KernelParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a RawProto into a new instance of the parameter.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.LayerParameter">
            <summary>
            Specifies the base parameter for all layers.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.m_rgBlobs">
            <summary>
            The blobs containing the numeric parameters of the layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.m_rgbPropagateDown">
            <summary>
            Specifies whether to backpropagate to each bottom.  If specified,
            Caffe will automatically infer whether each input needs backpropagation
            to compute parameter gradients.  If set to true for some inputs,
            backpropagation to those inputs is forced; if set to false for some inputs,
            backpropagation to those inputs is skipped.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.m_rgInclude">
            <summary>
            Rules controlling whether and when a layer is included in the network,
            based on the current NetState.  You may specify a non-zero number of rules
            to include OR exclude, but not both.  If no include or exclude rules are
            specified, the layer is always included.  If the current NetState meets
            ANY (i.e,, one or more) of the specified rules, the layer is
            included/excluded.
            </summary>
        </member>
        <member name="T:MyCaffe.param.LayerParameter.LayerType">
            <summary>
            Specifies the layer type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ABSVAL">
            <summary>
            Initializes a parameter for the AbsValLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ACCURACY">
            <summary>
            Initializes a parameter for the AccuracyLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ACCURACY_DECODE">
            <summary>
            Initializes a parameter for the AccuracyDecodeLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ACCURACY_ENCODING">
            <summary>
            Initializes a parameter for the AccuracyEncodingLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ANNOTATED_DATA">
            <summary>
            Initializes a parameter for the AnnotatedDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ARGMAX">
            <summary>
            Initializes a parameter for the ArgMaxLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.BIAS">
            <summary>
            Initializes a parameter for the BiasLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.BATCHNORM">
            <summary>
            Initializes a parameter for the BatchNormLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.BATCHREINDEX">
            <summary>
            Initializes a parameter for the BatchReindexLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.BNLL">
            <summary>
            Initializes a parameter for the BNLLLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.CLIP">
            <summary>
            Initializes a parameter for the ClipLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.CONCAT">
            <summary>
            Initializes a parameter for the ConcatLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.CONTRASTIVE_LOSS">
            <summary>
            Initializes a parameter for the ContrastiveLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.CONVOLUTION">
            <summary>
            Initializes a parameter for the ConvolutionLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.CROP">
            <summary>
            Initializes a parameter for the CropLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DECODE">
            <summary>
            Initializes a parameter for the DecodeLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DECONVOLUTION">
            <summary>
            Initializes a parameter for the DeconvolutionLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DETECTION_EVALUATE">
            <summary>
            Initializes a parameter for the DetectionEvaluateLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DETECTION_OUTPUT">
            <summary>
            Initializes a parameter for the DetectionOutputLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DATA">
            <summary>
            Initializes a parameter for the DataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DATA_NORMALIZER">
            <summary>
            Initializes a parameter for the DataNormalizerLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DATA_SEQUENCE">
            <summary>
            Initializes a parameter for the DataSequenceLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DROPOUT">
            <summary>
            Initializes a parameter for the DropoutLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DUMMYDATA">
            <summary>
            Initializes a parameter for the DummyDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ELTWISE">
            <summary>
            Initializes a parameter for the EltwiseLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ELU">
            <summary>
            Initializes a parameter for the ELULayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.EMBED">
            <summary>
            Initializes a parameter for the EmbedLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.EUCLIDEAN_LOSS">
            <summary>
            Initializes a parameter for the EuclideanLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.EVENT">
            <summary>
            Initializes a parameter for the EventLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.EXP">
            <summary>
            Initializes a parameter for the ExpLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.FILTER">
            <summary>
            Initializes a parameter for the FilterLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.FLATTEN">
            <summary>
            Initializes a parameter for the FlattenLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.GRADIENTSCALER">
            <summary>
            Initializes a parameter for the GradScaleLayer (used for gradient reversal)
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.GRAM">
            <summary>
            Initializes a parameter for the GramLayer (used with Neural Style)
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.GRN">
            <summary>
            Initializes a parameter for the GRNLayer (global response normalization L2)
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.HINGE_LOSS">
            <summary>
            Initializes a parameter for the HingeLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.IMAGE_DATA">
            <summary>
            Initializes a parameter for the ImageDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.IM2COL">
            <summary>
            Initializes a parameter for the Im2ColLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.INFOGAIN_LOSS">
            <summary>
            Initializes a parameter for the InfogainLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.INNERPRODUCT">
            <summary>
            Initializes a parameter for the InnerProductLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.INPUT">
            <summary>
            Initializes a parameter for the InputLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LABELMAPPING">
            <summary>
            Initializes a parameter for the LabelMappingLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LOG">
            <summary>
            Initializes a parameter for the LogLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LOSS">
            <summary>
            Initializes a parameter for the LossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LRN">
            <summary>
            Initializes a parameter for the LRNLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.MEMORYDATA">
            <summary>
            Initializes a parameter for the MemoryDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.MEMORY_LOSS">
            <summary>
            Initializes a parameter for the MemoryLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.MULTIBOX_LOSS">
            <summary>
            Initialize a parameter for the MultiBoxLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.MULTINOMIALLOGISTIC_LOSS">
            <summary>
            Initializes a parameter for the MultinomialLogisticLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.MVN">
            <summary>
            Initializes a parameter for the MVNLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.ONEHOT">
            <summary>
            Initializes a parameter for the OneHotLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.PARAMETER">
            <summary>
            Initializes a parameter for the ParameterLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.PERMUTE">
            <summary>
            Initializes a parameter for the PermuteLayer used with SSD.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.POOLING">
            <summary>
            Initializes a parameter for the PoolingLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.POWER">
            <summary>
            Initializes a parameter for the PowerLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.PRELU">
            <summary>
            Initializes a parameter for the PReLULayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.PRIORBOX">
            <summary>
            Initializes a parameter for the PriorBoxLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.REDUCTION">
            <summary>
            Initializes a parameter for the ReductionLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.RELU">
            <summary>
            Initializes a parameter for the ReLULayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.RESHAPE">
            <summary>
            Initializes a parameter for the ReshapeLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SCALAR">
            <summary>
            Initializes a parameter for the ScalarLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SCALE">
            <summary>
            Initializes a parameter for the ScaleLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SIGMOID">
            <summary>
            Initializes a parameter for the SigmoidLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SIGMOIDCROSSENTROPY_LOSS">
            <summary>
            Initializes a parameter for the SigmoidCrossEntropyLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SOFTMAXCROSSENTROPY_LOSS">
            <summary>
            Initializes a parameter for the SoftmaxCrossEntropyLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SOFTMAX">
            <summary>
            Initializes a parameter for the SoftmaxLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SOFTMAXWITH_LOSS">
            <summary>
            Initializes a parameter for the SoftmaxLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SMOOTHL1_LOSS">
            <summary>
            Intiializes a parameter for the SmoothL1LossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SPP">
            <summary>
            Initializes a parameter for the SPPLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SILENCE">
            <summary>
            Initializes a parameter for the SilenceLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SLICE">
            <summary>
            Initializes a parameter for the SliceLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SPLIT">
            <summary>
            Initializes a parameter for the SplitLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.SWISH">
            <summary>
            Initializes a parameter for the SwishLayer
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TV_LOSS">
            <summary>
            Initializes a parameter for the TVLossLayer (used with Neural Style).
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TANH">
            <summary>
            Initializes a parameter for the TanhLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.THRESHOLD">
            <summary>
            Initializes a parameter for the ThresholdLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TILE">
            <summary>
            Initializes a parameter for the TileLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TRANSFORM">
            <summary>
            Initializes a parameter for the DataTransformer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LSTM_SIMPLE">
            <summary>
            Initializes a parameter for the LSTMSimpleLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.RECURRENT">
            <summary>
            Initializes a parameter for the RecurrentLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.RNN">
            <summary>
            Initializes a parameter for the RNNLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LSTM">
            <summary>
            Initializes a parameter for the LSTMLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.LSTM_UNIT">
            <summary>
            Initializes a parameter for the LSTMUnitLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.UNPOOLING1">
            <summary>
            DEPRECIATED - Initializes a parameter for the UnpoolingLayer1 which uses a CPU based implementation (slower).
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.UNPOOLING">
            <summary>
            Initializes a parameter for the UnpoolingLayer which uses a GPU based implementation (faster).
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.NORMALIZATION1">
            <summary>
            Initializes a parameter for the Normalization1Layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.NORMALIZATION2">
            <summary>
            Initializes a parameter for the Normalization2Layer used with SSD.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TRIPLET_LOSS_SIMPLE">
            <summary>
            Initializes a parameter for the TripletLossSimpleLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TRIPLET_LOSS">
            <summary>
            Initializes a parameter for the TripletLossLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TRIPLET_SELECT">
            <summary>
            Initializes a parameter for the TripletSelectLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.TRIPLET_DATA">
            <summary>
            Initializes a parameter for the TripletDataLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.KNN">
            <summary>
            Initializes a parameter for the KNNLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.DEBUG">
            <summary>
            Initializes a parameter for the DebugLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.BINARYHASH">
            <summary>
            Initializes a parameter for the BinaryHashLayer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameter.LayerType.VIDEO_DATA">
            <summary>
            Initializes a parameter for the VideoDataLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.#ctor">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.LayerParameter.#ctor(MyCaffe.param.LayerParameter.LayerType,System.String)">
            <summary>
            The LayerParameter constructor.
            </summary>
            <param name="lt">Assignes this LayerType to the layer.</param>
            <param name="strName">Assigns this name to the layer.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.#ctor(MyCaffe.param.LayerParameter)">
            <summary>
            The LayerParameter constructor.
            </summary>
            <param name="p">Used to initialize the new LayerParameter.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.GetParameterCount">
            <summary>
            Returns the number of ParamSpec parameters used by the layer.
            </summary>
            <returns>The ParamSpec count is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.CopyDefaults(MyCaffe.param.LayerParameter)">
            <summary>
            Copies the defaults from another LayerParameter.
            </summary>
            <param name="p">Specifies the LayerParameter to copy.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.MeetsPhase(MyCaffe.basecode.Phase)">
            <summary>
            Determines whether or not this LayerParameter meets a given Phase.
            </summary>
            <param name="phase">Specifies the Phase.</param>
            <returns>Returns <i>true</i> if this LayerParameter meets the Phase, <i>false</i> otherwise.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.Save(System.IO.BinaryWriter)">
            <summary>
            Save this parameter to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the parameter from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader.</param>
            <param name="bNewInstance">When <i>true</i> a new instance is created (the default), otherwise the existing instance is loaded from the binary reader.</param>
            <returns>Returns an instance of the parameter.</returns>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.name">
            <summary>
            Specifies the name of this LayerParameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.type">
            <summary>
            Specifies the type of this LayerParameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.use_halfsize">
            <summary>
            Specifies whether or not to use half sized memory or not.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.SetType(MyCaffe.param.LayerParameter.LayerType,System.Boolean)">
            <summary>
            Set the layer type.
            </summary>
            <param name="type">Specifies the new layer type.</param>
            <param name="bNewParam">Optionally, specifies to create new params (default = true).</param>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.bottom">
            <summary>
            Specifies the active bottom connections (in the bottom, out the top).
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.top">
            <summary>
            Specifies the active top connections (in the bottom, out the top)
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.phase">
            <summary>
            Specifies the Phase for which this LayerParameter is run.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.freeze_learning">
            <summary>
            Get/set whether or not to freeze the learning for this layer globally.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.loss_weight">
            <summary>
            Specifies the loss weight.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.parameters">
            <summary>
            Specifies the ParamSpec parameters of the LayerParameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.blobs">
            <summary>
            Specifies the blobs of the LayerParameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.propagate_down">
            <summary>
            Specifies whether or not the LayerParameter (or protions of) should be backpropagated.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.include">
            <summary>
            Specifies the NetStateRule's for which this LayerParameter should be included.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.exclude">
            <summary>
            Specifies the NetStateRule's for which this LayerParameter should be excluded.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.group_start">
            <summary>
            Specifies whether or not this node is the start of a new group - this is only used when rendering models.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.transform_param">
            <summary>
            Returns the parameter set when initialized with LayerType.TRANSFORM
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.accuracy_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ACCURACY
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.argmax_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ARGMAX
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.batch_norm_param">
            <summary>
            Returns the parameter set when initialized with LayerType.BATCHNORM
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.bias_param">
            <summary>
            Returns the parameter set when initialized with LayerType.BIAS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.clip_param">
            <summary>
            Returns the parameter set when initialized with LayerType.CLIP
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.concat_param">
            <summary>
            Returns the parameter set when initialized with LayerType.CONCAT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.contrastive_loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.CONTRASTIVE_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.convolution_param">
            <summary>
            Returns the parameter set when initialized with LayerType.CONVOLUTION
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.crop_param">
            <summary>
            Returns the parameter set when initialized with LayerType.CROP
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.decode_param">
            <summary>
            Returns the parameter set when initializing with LayerType.DECODE or LayerType.ACCURACY_ENCODING;
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.annotated_data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ANNOTATED_DATA
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.detection_evaluate_param">
            <summary>
            Returns the parmeter set when initialized with LayerType.DETECTION_EVALUATE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.detection_output_param">
            <summary>
            Returns the parmeter set when initialized with LayerType.DETECTION_OUTPUT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DATA
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.data_normalizer_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DATA_NORMALIZER
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.data_sequence_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DATA_SEQUENCE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.debug_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DEBUG
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.dropout_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DROPOUT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.dummy_data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.DUMMYDATA
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.eltwise_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ELTWISE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.elu_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ELU
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.embed_param">
            <summary>
            Returns the parameter set when initialized with LayerType.EMBED
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.exp_param">
            <summary>
            Returns the parameter set when initialized with LayerType.EXP
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.flatten_param">
            <summary>
            Returns the parameter set when initialized with LayerType.FLATTEN
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.gradient_scale_param">
            <summary>
            Returns the parameter set when initialized with LayerType.GSL
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.gram_param">
            <summary>
            Returns the parameter set when initialized with LayerType.GRAM
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.tv_loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.TV_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.hinge_loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.HINGE_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.image_data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.IMAGE_DATA
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.infogain_loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.INFOGAIN_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.inner_product_param">
            <summary>
            Returns the parameter set when initialized with LayerType.INNERPRODUCT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.knn_param">
            <summary>
            Returns the parameter set when initialized with LayerType.KNN
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.labelmapping_param">
            <summary>
            Returns the parameter set when initialized with LayerType.LABELMAPPING
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.log_param">
            <summary>
            Returns the parameter set when initialized with LayerType.LOG
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.lrn_param">
            <summary>
            Returns the parameter set when initialized with LayerType.LRN
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.memory_data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.MEMORY_DATA
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.multiboxloss_param">
            <summary>
            Returns the parameter set when initializing with LayerType.MULTIBOX_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.mvn_param">
            <summary>
            Returns the parameter set when initialized with LayerType.MVN
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.onehot_param">
            <summary>
            Returns the parameter set when initialized with LayerType.ONEHOT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.normalization1_param">
            <summary>
            Returns the parameter set when initialized with LayerType.NORMALIZATION1
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.normalization2_param">
            <summary>
            Returns the parameter set when initialized with LayerType.NORMALIZATION2
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.pooling_param">
            <summary>
            Returns the parameter set when initialized with LayerType.POOLING
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.unpooling_param">
            <summary>
            Returns the parameter set when initialized with LayerType.UNPOOLING
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.parameter_param">
            <summary>
            Returns the parameter set when initialized with LayerType.PARAMETER
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.permute_param">
            <summary>
            Returns the parameter set when initialized with LayerType.PERMUTE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.power_param">
            <summary>
            Returns the parameter set when initialized with LayerType.POWER
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.prelu_param">
            <summary>
            Returns the parameter set when initialized with LayerType.PRELU
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.prior_box_param">
            <summary>
            Returns the parameter set when initialized with LayerType.PRIORBOX
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.reduction_param">
            <summary>
            Returns the parameter set when initialized with LayerType.REDUCTION
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.relu_param">
            <summary>
            Returns the parameter set when initialized with LayerType.RELU
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.reshape_param">
            <summary>
            Returns the parameter set when initialized with LayerType.RESHAPE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.scalar_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SCALAR
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.scale_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SCALE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.sigmoid_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SIGMOID
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.softmax_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SOFTMAX
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.spp_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SPP
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.slice_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SLICE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.swish_param">
            <summary>
            Returns the parameter set when initialized with LayerType.SWISH
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.tanh_param">
            <summary>
            Returns the parameter set when initialized with LayerType.TANH
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.threshold_param">
            <summary>
            Returns the parameter set when initialized with LayerType.THRESHOLD
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.tile_param">
            <summary>
            Returns the parameter set when initialized with LayerType.TILE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.triplet_loss_param">
            <summary>
            Returns the parameter set when initialized with LayerType.TRIPLET_LOSS
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.lstm_simple_param">
            <summary>
            Returns the parameter set when initialized with LayerType.LSTM_SIMPLE
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.recurrent_param">
            <summary>
            Returns the parameter set when initialized with LayerType.RECURRENT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.input_param">
            <summary>
            Returns the parameter set when initialized with LayerType.INPUT
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.video_data_param">
            <summary>
            Returns the parameter set when initialized with LayerType.VIDEO_DATA
            </summary>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.clear_blobs">
            <summary>
            Clears the collection of Blobs used by this layer.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.solver_count">
            <summary>
            Returns the number of Solvers participating in a multi-GPU session for which the Solver using this LayerParameter is associated.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.solver_rank">
            <summary>
            Returns the SolverRank of the Solver using this LayerParameter (if any).
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.expected_top">
            <summary>
            Returns a list of <i>expected</i> top connections (in the bottom, out the top).
            </summary>
        </member>
        <member name="P:MyCaffe.param.LayerParameter.expected_bottom">
            <summary>
            Returns a list of <i>expected</i> bottom connections (in the bottom, out the top).
            </summary>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.CopyParameters(MyCaffe.param.LayerParameter)">
            <summary>
            Copy just the layer specific parameters to this layer parameter.
            </summary>
            <param name="src">Specifies the source who's specific layer parameters are to be compied.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.Clone(System.Boolean)">
            <summary>
            Creates a new copy of this instance of the parameter.
            </summary>
            <returns>A new instance of this parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.System#ICloneable#Clone">
            <summary>
            Creates a new copy of this instance of the parameter.
            </summary>
            <returns>A new instance of this parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.CompareTo(System.Object)">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.LayerParameter.ToProto(System.String)">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.LayerParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.GetType(System.String)">
            <summary>
            Converts the string type into a LayerType, or <i>null</i> if no match is found.
            </summary>
            <param name="strType">Specifies the layer type.</param>
            <returns>The LayerType is returned, or <i>null</i> if not found.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameter.ToString">
            <summary>
            Returns a string representation of the LayerParameter.
            </summary>
            <returns></returns>
        </member>
        <member name="T:MyCaffe.param.LayerParameterBase">
            <summary>
            The LayerParameterBase is the base class for all other layer specific parameters.
            </summary>
        </member>
        <member name="T:MyCaffe.param.LayerParameterBase.LABEL_TYPE">
            <summary>
            Defines the label type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameterBase.LABEL_TYPE.NONE">
            <summary>
            Specifies that no label is used.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameterBase.LABEL_TYPE.SINGLE">
            <summary>
            Specifies a single label value per batch item so if the batch size = 8 there should be 8 labels, which is the default.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameterBase.LABEL_TYPE.MULTIPLE">
            <summary>
            Specifies multiple values for the label such as are used in segmentation problems, 
            the label's are stored in each Data Item's <i>DataCriteria</i> field.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LayerParameterBase.LABEL_TYPE.ONLY_ONE">
            <summary>
            Specifies to force the label to a single value regardless of the batch size, so if the batch size is 8, the label count remains only 1.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LayerParameterBase.#ctor">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.LayerParameterBase.Clone">
            <summary>
            Creates a new copy of this instance of the parameter.
            </summary>
            <returns>A new instance of this parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.LayerParameterBase.Copy(MyCaffe.param.LayerParameterBase)">
            <summary>
            Copy on parameter to another.
            </summary>
            <param name="src">Specifies the parameter to copy.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameterBase.Save(System.IO.BinaryWriter)">
            <summary>
            Save this parameter to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.LayerParameterBase.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Load the parameter from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader.</param>
            <param name="bNewInstance">When <i>true</i> a new instance is created (the default), otherwise the existing instance is loaded from the binary reader.</param>
            <returns>Returns an instance of the parameter.</returns>
        </member>
        <member name="T:MyCaffe.param.LogParameter">
            <summary>
            Specifies the parameters for the LogLayer.
            </summary>
            <remarks>
            The LogLayer computes outputs:
            
            @f$ y = log_base(shift + scale * x) @f$, for base > 0.
            
            Or if base is set to the default (-1), base is set to e.,
            so:
            
            @f$ y = ln(shift + scale * x) @f$
            </remarks>
        </member>
        <member name="M:MyCaffe.param.LogParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.LogParameter.base_val">
            <summary>
            Specifies the base to use for the log, where @f$ y = log_base(shift + scale * x) @f$, for base > 0.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LogParameter.scale">
            <summary>
            Specifies the scale to use for the log, where @f$ y = log_base(shift + scale * x) @f$, for base > 0.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LogParameter.shift">
            <summary>
            Specifies the shift to use for the log, where @f$ y = log_base(shift + scale * x) @f$, for base > 0.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LogParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.LogParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.LogParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.LogParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.LogParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.LossParameter">
            <summary>
            Stores the parameters used by loss layers.
            </summary>
        </member>
        <member name="T:MyCaffe.param.LossParameter.NormalizationMode">
            <summary>
            How to normalize the loss for loss layers that aggregate across batches,
            spatial dimensions, or other dimensions.  Currenly only implemented in
            SoftmaxWithLoss layer.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LossParameter.NormalizationMode.FULL">
            <summary>
            Divide by the number of examples in the batch times spatial dimensions.
            Outputs that receive the ignore label will NOT be ignored in computing
            the normalization factor.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LossParameter.NormalizationMode.VALID">
            <summary>
            Divide by the total number of output locations that do not take the
            ignore label.  If ignore label is not set, this behaves like FULL.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LossParameter.NormalizationMode.BATCH_SIZE">
            <summary>
            Divide by the batch size.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LossParameter.NormalizationMode.NONE">
            <summary>
            Do not normalize the loss.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LossParameter.#ctor(MyCaffe.param.LossParameter.NormalizationMode)">
            <summary>
            The constructor for the LossParameter.
            </summary>
            <remarks>
            The default VALID normalization mode is used for all loss layers, except
            for the SigmoidCrossEntropyLoss layer which uses BATCH_SIZE as the default
            for historical reasons.
            </remarks>
            <param name="norm">Specifies the default normalization mode.</param>
        </member>
        <member name="P:MyCaffe.param.LossParameter.ignore_label">
            <summary>
            If specified, the ignore instances with the given label.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LossParameter.normalization">
            <summary>
            Specifies the normalization mode (default = VALID).
            </summary>
        </member>
        <member name="P:MyCaffe.param.LossParameter.normalize">
            <summary>
            <b>DEPRECIATED</b>.  Ignore if normalization is specified.  If normalization
            is not specified, then setting this to false will be equivalent to 
            normalization = BATCH_SIZE to be consistent with previous behavior.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LossParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.LossParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.LossParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.LossParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.LossParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.LRNParameter">
            <summary>
            Specifies the parameter for the LRNLayer.
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov, 2012.
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.LRNParameter.NormRegion">
            <summary>
            Defines the normalization region.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LRNParameter.NormRegion.ACROSS_CHANNELS">
            <summary>
            Normalize across channels.
            </summary>
        </member>
        <member name="F:MyCaffe.param.LRNParameter.NormRegion.WITHIN_CHANNEL">
            <summary>
            Normalize within channels.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LRNParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.LRNParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.LRNParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.LRNParameter.local_size">
            <summary>
            Specifies the local size of the normalization window width.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LRNParameter.alpha">
            <summary>
            Specifies the alpha value used for variance scaling in the normalization formula.  NOTE: cuDNN uses a default of alpha = 1e-4, whereas Caffe uses a default of alpha = 1.0
            </summary>
        </member>
        <member name="P:MyCaffe.param.LRNParameter.beta">
            <summary>
            Specifies the beta value used as the power parameter in the normalization formula.  NOTE: both cuDNN and Caffe use a default of beta = 0.75
            </summary>
        </member>
        <member name="P:MyCaffe.param.LRNParameter.norm_region">
            <summary>
            Specifies the region over which to normalize.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LRNParameter.k">
            <summary>
            Specifies the k value used by the normalization parameter.  NOTE: cuDNN uses a default of k = 2.0, whereas Caffe uses a default of k = 1.0.
            </summary>
        </member>
        <member name="M:MyCaffe.param.LRNParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.LRNParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.LRNParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.LRNParameter.ToProto(System.String)">
            @copydoc EngineParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.LRNParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.LSTMSimpleParameter">
            <summary>
            Specifies the parameters for the LSTMSimpleLayer.
            </summary>
            <remarks>
            @see [A Clockwork RNN](https://arxiv.org/abs/1402.3511) by Jan Koutnik, Klaus Greff, Faustino Gomez, and Jrgen Schmidhuber, 2014.
            @see [Long short-term memory](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.7752) by Sepp Hochreiter and Jrgen Schmidhuber, 1997.
            @see [Learning to execute](https://arxiv.org/abs/1410.4615) by Wojciech Zaremba and Ilya Sutskever, 2014.
            @see [Generating sequences with recurrent neural networks](https://arxiv.org/abs/1308.0850) by Alex Graves, 2013.
            @see [Predictive Business Process Monitoring with LSTM Neural Networks](https://arxiv.org/abs/1612.02130) by Niek Tax, Ilya Verenich, Marcello La Rosa, and Marlon Dumas, 2016. 
            @see [Using LSTM recurrent neural networks for detecting anomalous behavior of LHC superconducting magnets](https://arxiv.org/abs/1611.06241) by Maciej Wielgosz, Andrzej Skocze, and Matej Mertik, 2016.
            @see [Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction](https://arxiv.org/abs/1608.05267v2) by Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, and Ferdous Sohel, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.num_output">
            <summary>
            Specifies the number of outputs for the layer.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.clipping_threshold">
            <summary>
            Specifies the gradient clipping threshold, default = 0.0 (i.e. no clipping).
            </summary>
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.weight_filler">
            <summary>
            Specifies the filler parameters for the weight filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.bias_filler">
            <summary>
            Specifies the filler parameters for the bias filler.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.batch_size">
            <summary>
            Specifies the batch size, default = 1.
            </summary>
        </member>
        <member name="P:MyCaffe.param.LSTMSimpleParameter.enable_clockwork_forgetgate_bias">
            <summary>
            When enabled, the forget gate bias is set to 5.0.
            </summary>
            <remarks>
            @see [A Clockwork RNN](https://arxiv.org/abs/1402.3511) by Koutnik, et al., 2014
            </remarks>
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.LSTMSimpleParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.MVNParameter">
            <summary>
            Specifies the parameters for the MVNLayer.
            </summary>
            <remarks>
            @see [Layer Normalization](https://arxiv.org/abs/1607.06450) by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.
            @see [Learning weakly supervised multimodal phoneme embeddings](https://arxiv.org/abs/1704.06913v1) by Rahma Chaabouni, Ewan Dunbar, Neil Zeghidour, and Emmanuel Dupoux, 2017. 
            @see [Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks](https://arxiv.org/abs/1304.1018) by Dimitri Palaz, Ronan Collobert, and Mathew Magimai-Doss, 2013.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.MVNParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.MVNParameter.normalize_variance">
            <summary>
            Specifies whether or not to normalize the variance.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MVNParameter.across_channels">
            <summary>
            Specifies whether or not to normalize accross channels.
            </summary>
        </member>
        <member name="P:MyCaffe.param.MVNParameter.eps">
            <summary>
            Specifies a small value to avoid divide by zero.
            </summary>
        </member>
        <member name="M:MyCaffe.param.MVNParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.MVNParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.MVNParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.MVNParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.MVNParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.NetParameter">
            <summary>
            Specifies the parameters use to create a Net
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetParameter.#ctor">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.NetParameter.Save(System.IO.BinaryWriter)">
            <summary>
            Save the parameter to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.NetParameter.Load(System.IO.BinaryReader)">
            <summary>
            Load a new instance of the parameter from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader.</param>
            <returns>The new instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.NetParameter.ProjectID">
            <summary>
            Specifies the ID of the project that created this net param (if any).
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.name">
            <summary>
            The name of the network.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.input">
            <summary>
            The input blobs to the network.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.input_shape">
            <summary>
            The shape of the input blobs.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.input_dim">
            <summary>
            <b>DEPRECIATED</b> - 4D input dimensions - use 'input_shape' instead.
            If specified, for each input blob there should be four
            values specifying the num, channels, height and width of the input blob.
            Thus, there should be a total of (4 * #input) numbers.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.force_backward">
            <summary>
            Whether the network will force every layer to carry out backward operation.
            If set False, then whether to carry out backward is determined
            automatically according to the net structure and learning rates.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.state">
            <summary>
            The current 'state' of the network, including the phase, level and stage.
            Some layers may be included/excluded depending on this state and the states
            specified in the layers' include and exclude fields.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.debug_info">
            <summary>
            Print debugging information about results while running Net::Forward,
            Net::Backward and Net::Update.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.layer">
            <summary>
            The layers that make up the net.  Each of their configurations, including
            connectivity and behavior, is specified as a LayerParameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.solver_count">
            <summary>
            Specifies the number of solvers used in a multi-gpu training session.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetParameter.solver_rank">
            <summary>
            Specifies the rank of the solver using this network.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetParameter.ToProto(System.String)">
            @copydoc BaseParameter 
        </member>
        <member name="M:MyCaffe.param.NetParameter.ToProto(System.String,System.Boolean)">
            <summary>
            Save the parameter settings to a RawProto.
            </summary>
            <param name="strName">Specifies the name, typically 'root'.</param>
            <param name="bIncludeState">Specifies whether or not to also include the state when saving.</param>
            <returns>The RawProto representing the settings is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parse a RawProto into a new instance of the parameter.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetParameter.Clone(System.Boolean,System.Nullable{System.Int32},System.Nullable{System.Int32})">
            <summary>
            Creates a new copy of this instance of the parameter.
            </summary>
            <param name="bCloneLayers">When <i>true</i>, each layer is cloned as well.</param>
            <param name="nSolverCount">Optionally, specifies a solver count for the clone.</param>
            <param name="nSolverRank">Optionally, specifies a solver rank for the clone.</param>
            <returns>A new instance of this parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetParameter.DebugString">
            <summary>
            Returns a debug string for the network.
            </summary>
            <returns>The debug string is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetParameter.FindLayer(MyCaffe.param.LayerParameter.LayerType,MyCaffe.basecode.Phase)">
            <summary>
            Locates a layer based on a layer type and phase.
            </summary>
            <param name="type">Specifies the LayerParameter.LayerType to look for.</param>
            <param name="phase">Optionally, specifies a phase to look for.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.NetParameter.FindLayerIndex(System.String)">
            <summary>
            Locates the index of a layer based on a given layer name.
            </summary>
            <param name="strName">Specifies the layer name to look for.</param>
            <returns>The index of the layer is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.NetState">
            <summary>
            Specifies the NetState which includes the phase, level and stage
            for which a given Net is to run under.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetState.#ctor">
            <summary>
            The NetState constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetState.Save(System.IO.BinaryWriter)">
            <summary>
            Saves the NetState to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.NetState.Load(System.IO.BinaryReader)">
            <summary>
            Loads a new NetState instance from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader to use.</param>
            <returns>The new NetState instance is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.NetState.phase">
            <summary>
            Specifies the Phase of the NetState.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetState.level">
            <summary>
            Specifies the level of the NetState.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetState.stage">
            <summary>
            Specifies the stages of the NetState.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetState.MergeFrom(MyCaffe.param.NetState)">
            <summary>
            Merges another NetState with this instance.
            </summary>
            <param name="ns"></param>
        </member>
        <member name="M:MyCaffe.param.NetState.Clone">
            <summary>
            Creates a new copy of this NetState instance.
            </summary>
            <returns>The new NetState instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetState.System#ICloneable#Clone">
            <summary>
            Creates a new copy of this NetState instance.
            </summary>
            <returns>The new NetState instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetState.CompareTo(System.Object)">
            <summary>
            Compares this NetState to another one.
            </summary>
            <param name="obj">Specifies the other NetState to compare with this one.</param>
            <returns>If the NetStates are the same 0 is returned, otherwise 1 is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetState.ToProto(System.String)">
            <summary>
            Converts this NetState to a RawProto.
            </summary>
            <param name="strName">Specifies a name given to the RawProto.</param>
            <returns>The new RawProto representing the NetState is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetState.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses a RawProto representing a NetState into a NetState instance.
            </summary>
            <param name="rp">Specifies the RawProto representing the NetState.</param>
            <returns>The new instance of the NetState is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.NetStateRule">
            <summary>
            Specifies a NetStateRule used to determine whether a Net falls within a given <i>include</i> or <i>exclude</i> pattern.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.#ctor">
            <summary>
            Specifies the NetStateRule constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.#ctor(MyCaffe.basecode.Phase)">
            <summary>
            Specifies the NetStateRule constructor.
            </summary>
            <param name="p">Specifies the phase to assign to the NetStateRule</param>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.Save(System.IO.BinaryWriter)">
            <summary>
            Saves the NetStateRule to a given binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Loads a NetStateRule from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a new NetStateRule instance is created and loaded, otherwise this instance is loaded.</param>
            <returns>The instance of the NetStateRule is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.NetStateRule.phase">
            <summary>
            Set phase to require the NetState to have a particular phase (TRAIN or TEST)
            to meet this rule.
            </summary>
            <remarks>
            Note when the phase is set to NONE, the rule applies to all phases.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.NetStateRule.min_level">
            <summary>
            Set the minimum levels in which the layer should be used.
            Leave undefined to meet the rule regardless of level.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetStateRule.max_level">
            <summary>
            Set the maximum levels in which the layer should be used.
            Leave undefined to meet the rule regardless of level.
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetStateRule.stage">
            <summary>
            Customizable sets of stages to include.
            The net must have ALL of the specified stages and NONE of the specified
            'not_stage's to meet the rule.
            (Use mutiple NetStateRules to specify conjunctions of stages.)
            </summary>
        </member>
        <member name="P:MyCaffe.param.NetStateRule.not_stage">
            <summary>
            Customizable sets of stages to exclude.
            The net must have ALL of the specified stages and NONE of the specified
            'not_stage's to meet the rule.
            (Use mutiple NetStateRules to specify conjunctions of stages.)
            </summary>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.Clone">
            <summary>
            Creates a new copy of a NetStateRule instance.
            </summary>
            <returns>The new instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.ToProto(System.String)">
            <summary>
            Converts a NetStateRule into a RawProto.
            </summary>
            <param name="strName">Specifies a name given to the RawProto.</param>
            <returns>The new RawProto representing the NetStateRule is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses a RawProto representing a NetStateRule and creates a new instance of a NetStateRule from it.
            </summary>
            <param name="rp">Specifies the RawProto used.</param>
            <returns>The new NeteStateRule instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.NetStateRule.CompareTo(System.Object)">
            <summary>
            Compares this NetStateRule to another one.
            </summary>
            <param name="obj">Specifies the other NetStateRule to compare.</param>
            <returns>0 is returned if the NetStateRule instances match, otherwise 1 is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ParamSpec">
            <summary>
            Specifies training parameters (multipliers on global learning constants,
            and the name of other settings used for weight sharing).
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.m_strName">
            <summary>
            The names of the parameter blobs -- useful for sharing parameters among
            layers, but never requires otherwise.  To share a parameter between two
            layers, give it a (non-empty) name.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.m_shareMode">
            <summary>
            Whether to require shared weights to have the same shape, or just the same
            count -- defaults to STRICT if unspecified.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ParamSpec.DimCheckMode">
            <summary>
            Defines the dimension check mode.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.DimCheckMode.STRICT">
            <summary>
            STRICT (default) requires that num, channels, height, width each match.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.DimCheckMode.PERMISSIVE">
            <summary>
            PERMISSIVE requires only the count (num*channels*height*width) to match.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.m_dfLrMult">
            <summary>
            The multiplier on the global learning rate for this parameter.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ParamSpec.m_dfDecayMult">
            <summary>
            The multiplier on the global weight decay for this parameter.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.#ctor">
            <summary>
            The ParamSpec constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.#ctor(System.String)">
            <summary>
            The ParamSpec constructor.
            </summary>
            <param name="strName">Specifies a name given to the ParamSpec.</param>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.#ctor(System.Double,System.Double,System.String)">
            <summary>
            The ParamSpec constructor.
            </summary>
            <param name="dfLrMult">Specifies the learning rate multiplier given to the ParamSpec.</param>
            <param name="dfDecayMult">Specifies the decay rate multiplier given to the ParamSpec.</param>
            <param name="strName">Specifies the name given to the ParamSpec.</param>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.Save(System.IO.BinaryWriter)">
            <summary>
            Saves the ParamSpec to a binary writer.
            </summary>
            <param name="bw">Specifies the binary writer to use.</param>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.Load(System.IO.BinaryReader,System.Boolean)">
            <summary>
            Loads a ParamSpec from a binary reader.
            </summary>
            <param name="br">Specifies the binary reader to use.</param>
            <param name="bNewInstance">When <i>true</i>, a new ParamSpec instance is created and loaded, otherwise this instance is loaded.</param>
            <returns>The ParamSpec instance loaded is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.ParamSpec.name">
            <summary>
            Specifies the name of this parameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ParamSpec.share_mode">
            <summary>
            Specifies whether to require shared weights to have the same shape, or just the same count - defaults to STICT (same shape).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ParamSpec.lr_mult">
            <summary>
            Specifies the multiplier used on the global learning rate for this parameter.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ParamSpec.decay_mult">
            <summary>
            Specifies the multiplier used on the global weight decay for this parameter.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.Clone">
            <summary>
            Creates a new copy of the ParamSpec.
            </summary>
            <returns>A new instance of the ParamSpec is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.ToProto(System.String)">
            <summary>
            Converts the ParamSpec into a RawProto.
            </summary>
            <param name="strName">Specifies a name given to the RawProto.</param>
            <returns>The new RawProto representing the ParamSpec is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ParamSpec.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses a new ParamSpec from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto representing the ParamSpec.</param>
            <returns>The new ParamSpec instance is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.PoolingParameter">
            <summary>
            Specifies the parameters for the PoolingLayer.
            </summary>
            <remarks>
            @see [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) by Vincent Dumoulin and Francesco Visin, 2016.
            @see [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150) by Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba, 2015.
            @see [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner, 1998.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.PoolingParameter.PoolingMethod">
            <summary>
            Defines the pooling method.
            </summary>
        </member>
        <member name="F:MyCaffe.param.PoolingParameter.PoolingMethod.MAX">
            <summary>
            Select the maximum value from the pooling kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.param.PoolingParameter.PoolingMethod.AVE">
            <summary>
            Select the average value from the pooling kernel.
            </summary>
        </member>
        <member name="F:MyCaffe.param.PoolingParameter.PoolingMethod.STOCHASTIC">
            <summary>
            Select the stochastic value from the pooling kernel.
            </summary>
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.#ctor">
            @copydoc KernelParameter 
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.PoolingParameter.pool">
            <summary>
            Specifies the pooling method.
            </summary>
        </member>
        <member name="P:MyCaffe.param.PoolingParameter.global_pooling">
            <summary>
            Specifies whether or not to enable global pooling.
            </summary>
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc KernelParameter::Load 
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc KernelParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.Clone">
            @copydoc KernelParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.ToProto(System.String)">
            @copydoc KernelParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.PoolingParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.PowerParameter">
            <summary>
            Specifies the parameters for the PowerLayer.
            </summary>
            <remarks>
            The PowerLayer computes outputs:
            @f$ y = (shift + scale * x)^power @f$
            
            @see [Optimizing a Shallow Multi-Scale Network for Tiny-Imagenet Classification](http://cs231n.stanford.edu/reports/2015/pdfs/dashb_CS231n_Paper.pdf) by Dash Bodington, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.PowerParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.PowerParameter.power">
            <summary>
            Specifies power value in the formula @f$ x = (shift + scale * x)^power @f$.
            </summary>
        </member>
        <member name="P:MyCaffe.param.PowerParameter.scale">
            <summary>
            Specifies scale value in the formula @f$ x = (shift + scale * x)^power @f$.
            </summary>
        </member>
        <member name="P:MyCaffe.param.PowerParameter.shift">
            <summary>
            Specifies shift value in the formula @f$ x = (shift + scale * x)^power @f$.
            </summary>
        </member>
        <member name="M:MyCaffe.param.PowerParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.PowerParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.PowerParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.PowerParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.PowerParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.PReLUParameter">
            <summary>
            Specifies the parameters for the PReLULayer.
            </summary>
            <remarks>
            @see [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853) by Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li, 2015.
            @see [Revise Saturated Activation Functions](https://arxiv.org/abs/1602.05980?context=cs) by Bing Xu, Ruitong Huang, and Mu Li, 2016.
            @see [Understanding Deep Neural Networks with Rectified Linear Units](https://arxiv.org/abs/1611.01491) by Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee, 2016.
            @see [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852v1) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.PReLUParameter.filler">
            <summary>
            Specifies initial value of @f$ a_i @f$.  Default is @f$a_i = 0.25 @f$ for all i.
            </summary>
        </member>
        <member name="P:MyCaffe.param.PReLUParameter.channel_shared">
            <summary>
            Specifies whether or not slope parameters are shared across channels.
            </summary>
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.PReLUParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.RecurrentParameter">
            <summary>
            Specifies the parameters used by the RecurrentLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <remarks>
            CAFFE is the DEFAULT engine.
            </remarks>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.num_output">
            <summary>
            The dimension of the output (and usually hidden state) representation --
            must be explicitly set to non-zero.
            </summary>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.weight_filler">
            <summary>
            The filler for the weights.
            </summary>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.bias_filler">
            <summary>
            The filler for the bias.
            </summary>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.debug_info">
            <summary>
            Whether to enable displaying debug info in the unrolled recurrent net.
            </summary>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.expose_hidden">
            <summary>
            Whether to add as additional inputs (bottoms) the initial hidden state
            blobs, and add as additional outputs (tops) the final timestep hidden state
            blobs.  The number of additional bottom/top blobs required depends on the
            recurrent architecture -- e.g., 1 for RNN's, 2 for LSTM's.
            </summary>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.num_layers">
            <summary>
            The number of LSTM layers to implement.
            </summary>
            <remarks>This parameter only applies to cuDnn.</remarks>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.dropout_ratio">
            <summary>
            Specifies the dropout ratio. (e.g. the probability that values will be dropped out and set to zero.  A value of 0.25 = 25% chance that a value is set to 0, and dropped out.)
            </summary>
            <remarks>The drop-out layer is only used with cuDnn when more than one layer are used.</remarks>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.dropout_seed">
            <summary>
            Specifies the seed used by cuDnn for random number generation.
            </summary>
            <remarks>The drop-out layer is only used with cuDnn when more than one layer are used.</remarks>
        </member>
        <member name="P:MyCaffe.param.RecurrentParameter.cudnn_enable_tensor_cores">
            <summary>
            Specifies to enable the CUDA tensor cores when performing the rnn operations which is faster but not supported by all GPU's.
            </summary>
            <remarks>
            When run on GPU's that do not support Tensor cores, the default math (non-tensor core) is used.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.RecurrentParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ReductionParameter">
            <summary>
            Specifies the parameters used by ReductionLayer.
            </summary>
        </member>
        <member name="T:MyCaffe.param.ReductionParameter.ReductionOp">
            <summary>
            Defines the reduction operation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ReductionParameter.ReductionOp.SUM">
            <summary>
            Sum the values.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ReductionParameter.ReductionOp.ASUM">
            <summary>
            Sum the absolute values.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ReductionParameter.ReductionOp.SUMSQ">
            <summary>
            Sum the squared values.
            </summary>
        </member>
        <member name="F:MyCaffe.param.ReductionParameter.ReductionOp.MEAN">
            <summary>
            Calculate the mean value.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ReductionParameter.operation">
            <summary>
            Specifies the reduction operation.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ReductionParameter.axis">
            <summary>
            The first axis to reduce to scalar -- may be negative index from the
            end (e.g., -1 for the last axis).
            (Currently, only reduction along ALL 'tail' axes is supported; reduction
            on axis M through N, where N less than num_axis - 1, is unsupported.)
            Suppose we have an n-axis bottom Blob with shape:
              (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).
             
            if (axis == m, the output Blob will have shape
              (d0, d1, d2, ..., d(m-1)),
              
            and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))
            times, each including (dm * d(m+1) * ... * d(n-1)) individual data.
            
            if axis == 0 (the default), the output Blob always has the empty shape
            (count 1), performing reduction across the entire input --
            often useful for creating new loss functions.
            </summary>
        </member>
        <member name="P:MyCaffe.param.ReductionParameter.coeff">
            <summary>
            Specifies the coefficient used to scale the output.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ReductionParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ReLUParameter">
            <summary>
            Specifies the parameters for the ReLULayer
            </summary>
            <remarks>
            @see [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853) by Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li, 2015.
            @see [Revise Saturated Activation Functions](https://arxiv.org/abs/1602.05980?context=cs) by Bing Xu, Ruitong Huang, and Mu Li, 2016.
            @see [Rectifier Nonlinearities Improve Neural Network Acoustic Models](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf) by Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng, 2013.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.ReLUParameter.negative_slope">
            <summary>
            Specifies the negative slope.  Allow non-zero slope for negative inputs to speed up optimization.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.ToProto(System.String)">
            @copydoc EngineParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.ReLUParameter.FromProto(MyCaffe.basecode.RawProto)">
            @copydoc EngineParameter::FromProto 
        </member>
        <member name="T:MyCaffe.param.ReshapeParameter">
            <summary>
            Specifies the parameters for the ReshapeLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ReshapeParameter.shape">
            <summary>
            Specifies the output dimensions.
            </summary>
            <remarks>
            If some of the dimensions are set to 0,
            the corresponding dimension from the bottom layer is used (unchanged).
            Exactly one dimension may be set to -1, in which case its value is
            inferred from the count of the bottom blob and the remaining dimensions.
            
            For example, suppose we want to reshape a 2D blob 'input' with shape 2 x 8:
            
            layer {
              type: 'Reshape' bottom: 'input' top: 'output'
              reshape_param { ... }
            }
            
            If 'input' is 2D with shape 2 x 8, then the following reshape_param
            specifications are all equivalent, producing a 3D blob 'output' with shape
            2 x 2 x 4:
            
              reshape_param { shape { dim:  2 dim: 2 dim:  4 } }
              reshape_param { shape { dim:  0 dim: 2 dim:  4 } }
              reshape_param { shape { dim:  0 dim: 2 dim: -1 } }
              reshape_param { shape { dim:  0 dim:-1 dim:  4 } }
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ReshapeParameter.axis">
            <summary>
            Specifies the axis portion of the bottom blob's shape that is
            replaced by (included in) the reshape.  By default (axis == 0
            and num_axes == -1), the entire bottom blob shape is included 
            in the reshape, and hence the shape field must specify the entire 
            output shape.
            </summary>
            <remarks>
            axis may be non-zero to retain some portion of the beginning of the 
            input shape (and may be negative to index from the end; e.g., -1 to
            begin the reshape after the last axis, including nothing in the reshape,
            -2 to icnlude only the last axis, etc.).
            
            For example, suppose, 'input' is the 2D blob with shape 2 x 8.
            Then the following RehsapeLayer specifications are all equivalent,
            producing a blob 'output' with shape 2 x 2 x 4:
            
              reshape_param { shape { dim: 2 dim: 2 dim: 4 } }
              resahpe_param { shape { dim: 2 dim: 4 } axis:  1 }
              reshape_param { shape [ dim: 2 dim: 4 } axis: -3 }
            </remarks>
        </member>
        <member name="P:MyCaffe.param.ReshapeParameter.num_axes">
            <summary>
            num_axes specifies the extent of the reshape.
            </summary>
            <remarks>
            If num_axes >= 0 (and axis >= 0), the reshape will be performed only on
            input axes in the range [axis, axis+num_axes].
            num_axes may also be -1, the default, to include all remaining axes
            (starting from axis).
            
            For example, suppose, 'input' is the 2D blob with shape 2 x 8.
            Then the following RehsapeLayer specifications are all equivalent,
            producing a blob 'output' with shape 1 x 2 x 8:
            
              reshape_param { shape { dim: 1 dim: 2 dim: 8 } }
              resahpe_param { shape { dim: 1 dim: 2 } num_axes: 1 }
              reshape_param { shape [ dim: 1 } num_axes: 0 }
              
            On the other hand, these would produce output blob shape 2 x 1 x 8:
            
              reshape_param { shape { dim: 2 dim: 1 dim: 8 } }
              resahpe_param { shape { dim: 1 } axis: 1 num_axes: 0 }
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.CalculateCopyAxes(MyCaffe.param.LayerParameter,System.Int32@,System.Int32@,MyCaffe.basecode.Log)">
            <summary>
            Calculate the Copy Axes, inferred axis and constant count.
            </summary>
            <param name="p">Specifies the layer parameters.</param>
            <param name="nInferredAxis">Returns the inferred axis.</param>
            <param name="nConstantCount">Returns the constant count.</param>
            <param name="log">Optionally, specifies the output log (default = null, which ignores this parameter).</param>
            <returns>The copy axes are returned.</returns>
        </member>
        <member name="M:MyCaffe.param.ReshapeParameter.Reshape(MyCaffe.param.LayerParameter,System.Collections.Generic.List{System.Int32},System.Collections.Generic.List{System.Int32},System.Int32,System.Int32,MyCaffe.basecode.Log)">
            <summary>
            Calculates the new shape.
            </summary>
            <param name="p">Specifies the layer parameters.</param>
            <param name="rgShape">Specifies the Bottom[0] shape.</param>
            <param name="rgCopyAxes">Specifies the copy axes.</param>
            <param name="nInferredAxis">Specifies the inferred axis (if any).</param>
            <param name="nConstantCount">Specifies the constant count.</param>
            <param name="log">Specifies the output log.</param>
            <returns>The new top shape is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ScaleParameter">
            <summary>
            Specifies the parameters for the ScaleLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ScaleParameter.bias_term">
            <summary>
            Whether to also learn a bias (equivalent to a ScalarLayer + BiasLayer, but
            may be more efficient).
            </summary>
        </member>
        <member name="P:MyCaffe.param.ScaleParameter.bias_filler">
            <summary>
            Filler used for bias filling.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ScaleParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SigmoidParameter">
            <summary>
            Specifies the parameters for the SigmoidLayer.
            </summary>
            <remarks>
            @see [eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys](https://arxiv.org/abs/1702.08568v1) by Joshua Saxe and Konstantin Berlin, 2017. 
            @see [Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904v1) by Fei Wang, Mengquing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang, 2017.
            @see [Attention and Localization based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging](https://arxiv.org/abs/1703.06052v1) by Yong Xu, Qiuqiang Kong, Qiang Huang, Wenwu Wang, and Mark D. Plumbley, 2017.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.SigmoidParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SliceParameter">
            <summary>
            Specifies the parameters for the SliceLayer.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SliceParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.SliceParameter.axis">
            <summary>
            Specifies the axis along wich to slice -- may be negative to index from the end
            (e.g., -1 for the last axis).
            By default, SliceLayer concatenates blobs along the 'channels' axis 1.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SliceParameter.slice_point">
            <summary>
            Specifies optional slice points which indicate the indexes in the selected dimensions (the number of indices must be equal to the number of top blobs minus one).
            </summary>
        </member>
        <member name="P:MyCaffe.param.SliceParameter.slice_dim">
            <summary>
            <b>DEPRECIATED:</b> alias for 'axis' -- does not support negative indexing.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SliceParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.SliceParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.SliceParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.SliceParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.SliceParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SoftmaxParameter">
            <summary>
            Specifies the parameters for the SoftmaxLayer
            </summary>
            <remarks>
            @see [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580v1) by Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov, 2012.
            @see [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144v2) by Wu, et al., 2016.
            @see [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538v1) by Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean, 2017.
            @see [Exploring the Limits of Language Modeling](https://arxiv.org/abs/1602.02410v2) by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="P:MyCaffe.param.SoftmaxParameter.axis">
            <summary>
            The axis along which to perform the softmax -- may be negative to index
            from the end (e.g., -1 for the last axis).
            Any other axes will be evaluated as independent softmaxes.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.ToProto(System.String)">
            @copydoc EngineParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.SoftmaxParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SolverParameter">
            <summary>
            The SolverParameter is a parameter for the solver, specifying the train and test networks.
            </summary>
            <remarks>
            Exactly one train net must be specified using one of the following fields:
              train_net_param, train_net, net_param, net
            
            One or more of the test nets may be specified using any of the following fields:
              test_net_param, test_net, net_param, net
              
            If more than one test net field is specified (e.g., both net and test_net
            are specified), they will be evaluated in the field order given above:
              (1) test_net_param, (2) test_net, (3) net_param/net
              
            A test_iter must be specified for each test_net.
            A test_level and/or test_stage may also be specified for each test_net.
            </remarks>
        </member>
        <member name="T:MyCaffe.param.SolverParameter.EvaluationType">
            <summary>
            Defines the evaluation method used in the SSD algorithm.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.EvaluationType.CLASSIFICATION">
            <summary>
            Specifies to run a standard classification evaluation.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.EvaluationType.DETECTION">
            <summary>
            Specifies detection evaluation used in the SSD algorithm.
            </summary>
        </member>
        <member name="T:MyCaffe.param.SolverParameter.SnapshotFormat">
            <summary>
            Defines the format of each snapshot.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SnapshotFormat.BINARYPROTO">
            <summary>
            Save snapshots in the binary prototype format.
            </summary>
        </member>
        <member name="T:MyCaffe.param.SolverParameter.SolverType">
            <summary>
            Defines the type of solver.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.SGD">
            <summary>
            Use Stochastic Gradient Descent solver with momentum updates weights by a linear combination of the negative gradient and the previous weight update.
            </summary>
            <remarks>
            @see [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) Wikipedia.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.NESTEROV">
            <summary>
            Use Nesterov's accelerated gradient, similar to SGD, but error gradient is computed on the weights with added momentum.
            </summary>
            <remarks>
            @see [Lecture 6c The momentum method](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin, 2012.
            @see [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981) by Botev, Alexandar and Lever, Guy and Barber, David, 2016.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.ADAGRAD">
            <summary>
            Use Gradient based optimization like SGD that tries to find rarely seen features
            </summary>
            <remarks>
            @see [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) by Duchi, John and Hazan, Elad, and Singer, Yoram, 2011.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.RMSPROP">
            <summary>
            Use RMS Prop gradient based optimization like SGD.
            </summary>
            <remarks>
            @see [Lecture 6e	rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Tieleman and Hinton, 2012,
            @see [RMSProp and equilibrated adaptive learning rates for non-convex optimization](https://arxiv.org/abs/1502.04390v1) by Dauphin, Yann N. and de Vries, Harm and Chung, Junyoung and Bengio, Yoshua, 2015.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.ADADELTA">
            <summary>
            Use AdaDelta gradient based optimization like SGD.
            </summary>
            <remarks>
            See [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) by Zeiler, Matthew D., 2012.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.ADAM">
            <summary>
            Use Adam gradient based optimization like SGD that includes 'adaptive momentum estimation' and can be thougth of as a generalization of AdaGrad.
            </summary>
            <remarks>
            @see [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v9) by Kingma, Diederik P. and Ba, Jimmy, 2014.
            </remarks>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.SolverType.LBFGS">
            <summary>
            Use the L-BFGS solver based on the implementation of minFunc by Marc Schmidt.
            </summary>
            <remarks>
            @see [minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html) by Marc Schmidt, 2005
            </remarks>
        </member>
        <member name="T:MyCaffe.param.SolverParameter.LearningRatePolicyType">
            <summary>
            Defines the learning rate policy to use.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.FIXED">
            <summary>
            Use a fixed learning rate which always returns base_lr.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.STEP">
            <summary>
            Use a stepped learning rate which returns @f$ base_lr * gamma ^ {floor{iter/step}} @f$
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.EXP">
            <summary>
            Use an exponential learning rate which returns @f$ base_lr * gamma ^ {iter} @f$
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.INV">
            <summary>
            Use an inverse learning rate which returns @f$ base_lr * {1 + gamma * iter}^{-power} @f$
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.MULTISTEP">
            <summary>
            Use a multi-step learning rate which is similar to INV, but allows for non-uniform steps defined by stepvalue.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.POLY">
            <summary>
            Use a polynomial learning rate where the effective learning rate follows a polynomial decay, to be zero by the max_iter.  Returns @f$ base_lr * {1 - iter/max_iter}^{power} @f$
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.LearningRatePolicyType.SIGMOID">
            <summary>
            Use a sigmoid learning rate where the effective learning rate follows a sigmoid decay.  Returns @f$ base_lr * {1/{1 + exp{-gamma * {iter - stepsize}}}} @f$
            </summary>
        </member>
        <member name="T:MyCaffe.param.SolverParameter.RegularizationType">
            <summary>
            Defines the regularization type.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.RegularizationType.L1">
            <summary>
            Specifies L1 regularization.
            </summary>
        </member>
        <member name="F:MyCaffe.param.SolverParameter.RegularizationType.L2">
            <summary>
            Specifies L2 regularization.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SolverParameter.#ctor">
            <summary>
            The SolverParameter constructor.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SolverParameter.Clone">
            <summary>
            Creates a new copy of the SolverParameter.
            </summary>
            <returns>A new instance of the SolverParameter is returned.</returns>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.output_average_results">
            <summary>
            Specifies to average loss results before they are output - this can be faster when there are a lot of results in a cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.custom_trainer">
            <summary>
            Specifies the Name of the custom trainer (if any) - this is an optional setting used by exteral software to 
            provide a customized training process.  Each custom trainer must implement the IDnnCustomTraininer interface
            which contains a 'Name'property - the named returned from this property is the value set here as the 
            'custom_trainer'.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.custom_trainer_properties">
            <summary>
            Specifies the custom trainer properties (if any) - this is an optional setting used by exteral software to 
            provide the propreties for a customized training process.
            </summary>
            <remarks>
            Note all spaces are replaced with '~' characters to avoid parsing errors.</remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.net_param">
            <summary>
            Inline train net param, possibly combined with one or more test nets.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.train_net_param">
            <summary>
            Inline train net param, possibly combined with one or more test nets.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_net_param">
            <summary>
            Inline test net params.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.train_state">
            <summary>
            The states for the train/test nets.  Must be unspecified or
            specified once per net.
            </summary>
            <remarks>
            By default, all states will have solver = true;
            train_state will have phase = TRAIN,
            and all test_state's will have phase = TESET.
            Other defaults are set according to NetState defaults.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_state">
            <summary>
            The states for the train/test nets.  Must be unspecified or
            specified once per net.
            </summary>
            <remarks>
            By default, all states will have solver = true;
            train_state will have phase = TRAIN,
            and all test_state's will have phase = TESET.
            Other defaults are set according to NetState defaults.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_iter">
            <summary>
            The number of iterations for each test.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_interval">
            <summary>
            The number of iterations between two testing phases.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_compute_loss">
            <summary>
            Test the compute loss.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.test_initialization">
            <summary>
            If true, run an initial test pass before the first iteration,
            ensuring memory availability and printing the starting value of the loss.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.base_lr">
            <summary>
            The base learning rate.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.display">
            <summary>
            The number of iterations between displaying info.  If display = 0, no info
            will be displayed.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.average_loss">
            <summary>
            Display the loss averaged over the last average_loss iterations.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.max_iter">
            <summary>
            The maximum number of iterations.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.iter_size">
            <summary>
            Accumulate gradients over 'iter_size' x 'batch_size' instances.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.LearningRatePolicy">
            <summary>
            The learning rate decay policy.
            </summary>
            <remarks>
            The currently implemented learning rate policies are as follows:
               - fixed: always return @f$ base_lr @f$.
               - step: return @f$ base_lr * gamma ^ {floor{iter / step}} @f$
               - exp: return @f$ base_lr * gamma ^ iter @f$
               - inv: return @f$ base_lr * {1 + gamma * iter} ^ {-power} @f$
               - multistep: similar to step but it allows non-uniform steps defined by stepvalue.
               - poly: the effective learning rate follows a polynomial decay, to be
                       zero by the max_iter.  return @f$ base_lr * {1 - iter/max_iter} ^ {power} @f$
               - sigmoid: the effective learning rate follows a sigmoid decay.
                       return @f$ base_lr * {1/{1 + exp{-gamma * {iter - stepsize}}}} @f$
                       
            where base_lr, max_iter, gamma, step, stepvalue and power are defined int the
            solver protocol buffer, and iter is the current iteration.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.lr_policy">
            <summary>
            The learning rate decay policy.
            </summary>
            <remarks>
            The currently implemented learning rate policies are as follows:
               - fixed: always return base_lr.
               - step: return base_lr * gamma ^ (floor(iter / step))
               - exp: return base_lr * gamma ^ iter
               - inv: return base_lr * (1 + gamma * iter) ^ (-power)
               - multistep: similar to step but it allows non-uniform steps defined by stepvalue.
               - poly: the effective learning rate follows a polynomial decay, to be
                       zero by the max_iter.  return base_lr * (1 - iter/max_iter) ^ (power)
               - sigmoid: the effective learning rate follows a sigmoid decay.
                       return base_lr * (1/(1 + exp(-gamma * (iter - stepsize))))
                       
            where base_lr, max_iter, gamma, step, stepvalue and power are defined int the
            solver protocol buffer, and iter is the current iteration.
            
            @see [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489) by Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying and Quoc V. Le, 2017. 
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.gamma">
            <summary>
            The 'gamma' parameter to compute the learning rate.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.power">
            <summary>
            The 'power' parameter to compute the learning rate.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.momentum">
            <summary>
            The momentum value.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.weight_decay">
            <summary>
            The weight decay.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.Regularization">
            <summary>
            The regularization type.
            </summary>
            <remarks>
            The regularization types supported are:
               - L1 and L2 controlled by weight_decay.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.regularization_type">
            <summary>
            The regularization type.
            </summary>
            <remarks>
            The regularization types supported are:
               - L1 and L2 controlled by weight_decay.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.stepsize">
            <summary>
            The stepsize for learning rate policy 'step'.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.stepvalue">
            <summary>
            The step values for learning rate policy 'multistep'.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.clip_gradients">
            <summary>
            Set clip_gradients to >= 0 to clip parameter gradients to that L2 norm,
            whenever their actual L2 norm is larger.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot">
            <summary>
            The snapshot interval.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_prefix">
            <summary>
            The prefix for the snapshot.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_diff">
            <summary>
            Whether to snapshot diff in the results or not.  Snapshotting diff will help
            debugging but the final protocol buffer size will be much larger.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_format">
            <summary>
            The snapshot format.
            </summary>
            <remarks>
            Currently only the Binary Proto Buffer format is supported.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_include_weights">
            <summary>
            Specifies whether or not the snapshot includes the trained weights.  The default = <i>true</i>.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_include_state">
            <summary>
            Specifies whether or not the snapshot includes the solver state.  The default = <i>false</i>.  Including the solver state will slow down the time of each snapshot.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.device_id">
            <summary>
            The device id that will be used when run on the GPU.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.random_seed">
            <summary>
            If non-negative, the seed with which the Solver will initialize the caffe
            random number generator -- useful for repoducible results.  Otherwise
            (and by default) initialize using a seed derived from the system clock.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.type">
            <summary>
            Specifies the solver type.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.delta">
            <summary>
            Numerical stability for RMSProp, AdaGrad, AdaDelta and Adam
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.momentum2">
            <summary>
            An additional momentum property for the Adam solver.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.rms_decay">
            <summary>
            RMSProp decay value.
            </summary>
            <remarks>
            MeanSquare(t) = rms_decay * MeanSquare(t-1) + (1 - rms_decay) * SquareGradient(t)
            </remarks>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.debug_info">
            <summary>
            If true, print information about the state of the net that may help with
            debugging learning problems.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.lbgfs_corrections">
            <summary>
            Specifies the number of lbgfs corrections used with the L-BGFS solver.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.snapshot_after_train">
            <summary>
            If false, don't save a snapshot after training finishes.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.eval_type">
            <summary>
            Specifies the evaluation type to use when using Single-Shot Detection (SSD) - (default = NONE, SSD not used).
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.ap_version">
            <summary>
            Specifies the AP Version to use for average precision when using Single-Shot Detection (SSD) - (default = INTEGRAL).
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverParameter.show_per_class_result">
            <summary>
            Specifies whether or not to display results per class when using Single-Shot Detection (SSD) - (default = false).
            </summary>
        </member>
        <member name="M:MyCaffe.param.SolverParameter.ToProto(System.String)">
            <summary>
            Converts the SolverParameter into a RawProto.
            </summary>
            <param name="strName">Specifies a name given to the RawProto.</param>
            <returns>The new RawProto representing the SolverParameter is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.SolverParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses a new SolverParameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto representing the SolverParameter.</param>
            <returns>The new SolverParameter instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.param.SolverParameter.DebugString">
            <summary>
            Returns a debug string for the SolverParameter.
            </summary>
            <returns>The debug string is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.SolverState">
            <summary>
            The SolverState specifies the state of a given solver.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SolverState.#ctor">
            <summary>
            The SolverState constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.iter">
            <summary>
            The current iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.start">
            <summary>
             Specifies the start used by L-BGFS
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.end">
            <summary>
            Specifies the end used by L-BGFS
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.history">
            <summary>
            The history for SGD solvers.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.current_step">
            <summary>
            The current step for learning rate.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.gradients">
            <summary>
            Gradients used with L-BFGS state.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.direction">
            <summary>
            Direction used with L-BFGS state.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.s_history">
            <summary>
            S history used with L-BFGS state.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SolverState.rho_history">
            <summary>
            rho history used with L-BFGS state.
            </summary>
        </member>
        <member name="T:MyCaffe.param.SPPParameter">
            <summary>
            The SPPParameter specifies the parameters for the SPPLayer.
            </summary>
            <remarks>
            @see [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2014.
            @see [Image-based Localization using Hourglass Networks](https://arxiv.org/abs/1703.07971v1) by Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, and Esa Rahtu, 2017.
            @see [Relative Camera Pose Estimation Using Convolutional Neural Networks](https://arxiv.org/abs/1702.01381v2) by Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, Esa Rahtu, 2017.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.SPPParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="P:MyCaffe.param.SPPParameter.pool">
            <summary>
            Specifies the pooling method to use.
            </summary>
        </member>
        <member name="P:MyCaffe.param.SPPParameter.pyramid_height">
            <summary>
            Specifies the pyramid height.
            </summary>
        </member>
        <member name="M:MyCaffe.param.SPPParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.SPPParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc EngineParameter::Copy 
        </member>
        <member name="M:MyCaffe.param.SPPParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.SPPParameter.ToProto(System.String)">
            @copydoc EngineParameter::ToProto 
        </member>
        <member name="M:MyCaffe.param.SPPParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.TanhParameter">
            <summary>
            Specifies the parameters for the TanhLayer
            </summary>
            <remarks>
            @see [ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks](https://arxiv.org/abs/1505.00393v3) by Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, and Yoshua Bengio, 2015.
            @see [Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction](https://arxiv.org/abs/1608.05267v2) by Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, and Ferdous Sohel, , 2016.
            @see [Applying Deep Learning to Answer Selection: A Study and An Open Task](https://arxiv.org/abs/1508.01585v2) by Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou, 2015.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.TanhParameter.#ctor">
            @copydoc EngineParameter 
        </member>
        <member name="M:MyCaffe.param.TanhParameter.useCaffeReason">
            <summary>
            Returns the reason that Caffe version was used instead of [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.param.TanhParameter.useCudnn">
            <summary>
            Queries whether or not to use [NVIDIA's cuDnn](https://developer.nvidia.com/cudnn).
            </summary>
            <returns>Returns <i>true</i> when cuDnn is to be used, <i>false</i> otherwise.</returns>
        </member>
        <member name="M:MyCaffe.param.TanhParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc EngineParameter::Load 
        </member>
        <member name="M:MyCaffe.param.TanhParameter.Clone">
            @copydoc EngineParameter::Clone 
        </member>
        <member name="M:MyCaffe.param.TanhParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.ThresholdParameter">
            <summary>
            Stores the parameters used by the ThresholdLayer
            </summary>
            <remarks>
            @see [Neural Networks with Input Specified Thresholds](http://cs231n.stanford.edu/reports/2016/pdfs/118_Report.pdf) by Fei Liu and Junyang Qian, 2016.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.ThresholdParameter.threshold">
            <summary>
            Specifies the threshold value which must be strictly positive values.
            </summary>
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.ThresholdParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.TileParameter">
            <summary>
            Specifies the parameters used by the TileLayer
            </summary>
        </member>
        <member name="M:MyCaffe.param.TileParameter.#ctor">
            @copydoc LayerParameterBase 
        </member>
        <member name="P:MyCaffe.param.TileParameter.axis">
            <summary>
            Specifies the index of the axis to tile.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TileParameter.tiles">
            <summary>
            Specifies the number of copies (tiles) of the blob to output.
            </summary>
        </member>
        <member name="M:MyCaffe.param.TileParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.TileParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.TileParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.TileParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.TileParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.param.TransformationParameter">
            <summary>
            Stores parameters used to apply transformation 
            to the data layer's data.
            </summary>
        </member>
        <member name="T:MyCaffe.param.TransformationParameter.COLOR_ORDER">
            <summary>
            Defines the color ordering used to tranform the input data.
            </summary>
        </member>
        <member name="F:MyCaffe.param.TransformationParameter.COLOR_ORDER.RGB">
            <summary>
            Orders the channels by 'R'ed, 'G'reen, then 'B'lue.
            </summary>
        </member>
        <member name="F:MyCaffe.param.TransformationParameter.COLOR_ORDER.BGR">
            <summary>
            Orders the channels by 'B'lue, 'G'reen, then 'R'ed.  This ordering is typically used by Native C++ Caffe.
            </summary>
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.#ctor">
            <summary>
            The constructor.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.forced_positive_range_max">
            <summary>
            Specifies whether or not to fit the data into a forced range 
            of [0, forced_positive_range_max].  
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.scale">
            <summary>
            For data pre-processing, we can do simple scaling and subtracting the
            data mean, if provided.  Note that the mean subtraction is always carried
            out before scaling.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.mirror">
            <summary>
            Specify if we want to randomly mirror the data.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.crop_size">
            <summary>
            Specify if we would like to randomly crop an image.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.use_imagedb_mean">
            <summary>
            Specifies whether to subtract the mean image from the image database,
            subtract the mean values, or neither and do no mean subtraction.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.mean_value">
            <summary>
            If specified can be repeated once (would subtract it from all the channels
            or can be repeated the same number of times as channels
            (would subtract them from the corresponding channel).
            </summary>
            <remarks>
            So for example if there are 3 channels, mean_value could have 3 values,
            one for each channel -- or just one value which would be applied to
            all channels.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.force_color">
            <summary>
            Force the decoded image to have 3 color channels.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.force_gray">
            <summary>
            Force the decoded image to have 1 color channel.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.random_seed">
            <summary>
            Only used during testing.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.mean_file">
            <summary>
            Specifies the path to file containing the image mean in the proto buffer format of a BlobProto.
            </summary>
            <remarks>
            The mean file is used when specified and the 'use_imagedb_mean' = <i>true</i>.  If the 'use_imagedb_mean' = <i>true</i> and
            the mean file is not set, the Caffe Image Database is queried for the calculated mean image.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.color_order">
            <summary>
            Specifies the color ordering to use.  Native Caffe models often uses COLOR_ORDER.BGR, whereas MyCaffe datasets often
            uses the COLOR_ORDER.RGB ordering.
            </summary>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.resize_param">
            <summary>
            Optionally, specifies the resize policy, otherwise this is <i>null</i>.
            </summary>
            <remarks>
            Currently, this parameter is only used by the AnnotatedDataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.noise_param">
            <summary>
            Optionally, specifies the noise policy, otherwise this is <i>null</i>.
            </summary>
            <remarks>
            Currently, this parameter is only used by the DataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.distortion_param">
            <summary>
            Optionally, specifies the distortion policy, otherwise this is <i>null</i>.
            </summary>
            <remarks>
            Currently, this parameter is only used by the AnnotatedDataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.expansion_param">
            <summary>
            Optionally, specifies the expansion policy, otherwise this is <i>null</i>.
            </summary>
            <remarks>
            Currently, this parameter is only used by the AnnotatedDataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.emit_constraint">
            <summary>
            Optionally, specifies the emit constraint on emitting annotation after transformation, otherwise this is <i>null</i>.
            </summary>
            <remarks>
            Currently, this parameter is only used by the AnnotatedDataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.mask_param">
            <summary>
            Optionally, specifies the image mask which defines the boundary area that is set to black on the image thus masking that area out.
            </summary>
            <remarks>
            The mask is applied last, after all other alterations are made.
            
            Currently, this parameter is only used by the DataLayer.
            </remarks>
        </member>
        <member name="P:MyCaffe.param.TransformationParameter.label_mapping">
            <summary>
            Optionally, specifies the label mapping which defines how to map lables when calling the DataTransformer.TransformLabel method.
            </summary>
            <remarks>
            Currently, this parameter is only used by the DataLayer.
            </remarks>
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.Load(System.IO.BinaryReader,System.Boolean)">
            @copydoc LayerParameterBase::Load 
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.Copy(MyCaffe.param.LayerParameterBase)">
            @copydoc LayerParameterBase::Copy 
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.Clone">
            @copydoc LayerParameterBase::Clone 
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.ToProto(System.String)">
            @copydoc LayerParameterBase::ToProto 
        </member>
        <member name="M:MyCaffe.param.TransformationParameter.FromProto(MyCaffe.basecode.RawProto)">
            <summary>
            Parses the parameter from a RawProto.
            </summary>
            <param name="rp">Specifies the RawProto to parse.</param>
            <returns>A new instance of the parameter is returned.</returns>
        </member>
        <member name="T:MyCaffe.Properties.Resources">
            <summary>
              A strongly-typed resource class, for looking up localized strings, etc.
            </summary>
        </member>
        <member name="P:MyCaffe.Properties.Resources.ResourceManager">
            <summary>
              Returns the cached ResourceManager instance used by this class.
            </summary>
        </member>
        <member name="P:MyCaffe.Properties.Resources.Culture">
            <summary>
              Overrides the current thread's CurrentUICulture property for all
              resource lookups using this strongly typed resource class.
            </summary>
        </member>
        <member name="P:MyCaffe.Properties.Resources.LICENSETXT">
             <summary>
               Looks up a localized string similar to Apache License
                                       Version 2.0, January 2004
                                    http://www.apache.org/licenses/
            
               TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
            
               1. Definitions.
            
                  &quot;License&quot; shall mean the terms and conditions for use, reproduction,
                  and distribution as defined by Sections 1 through 9 of this document.
            
                  &quot;Licensor&quot; shall mean the copyright owner or entity authorized by
                  the copyright owner that is granting the License.
            
                  &quot; [rest of string was truncated]&quot;;.
             </summary>
        </member>
        <member name="T:MyCaffe.solvers.AdaGradSolver`1">
            <summary>
            Use AdaGrad Solver based optimization like SGD that tries to find rarely seen features.
            </summary>
            <remarks>
            @see [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) by John Duchi, Elad Hazan, and Yoram Singer, 2011.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.AdaGradSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The NesterovSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.AdaGradSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the AdaGrad update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="T:MyCaffe.solvers.AdamSolver`1">
            <summary>
            Use Adam Solver which uses gradient based optimization like SGD that includes 'adaptive momentum estimation' and can be thougth of as a generalization of AdaGrad.
            </summary>
            <remarks>
            @see [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v9) by Diederik P. Kingma, and Jimmy Ba, 2014.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.AdamSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The AdamSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.AdamSolver`1.AdamPreSolve">
            <summary>
            Runs the AdamSolver pre-solve which parpares the Solver to start Solving.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.AdamSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the AdamSolver update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="T:MyCaffe.solvers.LBFGSSolver`1">
            <summary>
            Optimizes the parameters of a Net using L-BFGS.  This implementation
            is based on minFunc, by Marc Schmidt.
            </summary>
            <remarks>
            @see [minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html) by Marc Schmidt, 2005
            @see [ftokarev/caffe-neural-style Github](https://github.com/ftokarev/caffe-neural-style) by ftokarev, 2017. 
            @see [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, 2015 
            </remarks>
            <typeparam name="T">Specifies the base type of <i>double</i> or <i>float</i>.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The LBFGSSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.dispose">
            <summary>
            Releases all resources (GPU and Host) used by the Solver.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.PreSolve">
            <summary>
            Runs the pre-solve which parpares the Solver to start Solving.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.ApplyUpdate(System.Int32)">
            <summary>
            Apply the gradients to the network.
            </summary>
            <param name="nIterationOverride">Optionally, specifies an iteration override (default = -1, which ignores the override).</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.CollectGradients">
            <summary>
            Collect the gradients from the network learnable parameters.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.UpdateHistory">
            <summary>
            Update the history values with the gradients and direction.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.ComputeInitialHessianApprox">
            <summary>
            Compute the initial Hessian approximation.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.ComputeDirection">
            <summary>
            Compute the direction.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.ComputeStep">
            <summary>
            Compute the step.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.UpdateNet">
            <summary>
            Update the network.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.RestoreSolverState(System.Byte[])">
            <summary>
            Restore a previously saved solver state.
            </summary>
            <param name="rgState">Specifies the solver state to restore.</param>
        </member>
        <member name="M:MyCaffe.solvers.LBFGSSolver`1.SnapshotSolverState">
            <summary>
            Save the solver state.
            </summary>
            <returns>They byte stream of the solver state is returned.</returns>
        </member>
        <member name="T:MyCaffe.solvers.RmsPropSolver`1">
            <summary>
            Use RmsProp Solver which uses gradient based optimization like SGD.
            </summary>
            <remarks>
            @see [Lecture 6e	rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Tieleman and Hinton, 2012,
            @see [RMSProp and equilibrated adaptive learning rates for non-convex optimization](https://arxiv.org/abs/1502.04390v1) by Yann N. Dauphin, Harm de Vries, Junyoung Chung, and Yoshua Bengio, 2015.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.RmsPropSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The RmsPropSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.RmsPropSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the RmsProp update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="T:MyCaffe.solvers.AdaDeltaSolver`1">
            <summary>
            Use AdaDelta Solver which has gradient based optimization like SGD.
            </summary>
            <remarks>
            See [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) by Matthew D. Zeiler, 2012.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.AdaDeltaSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The SGDSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.AdaDeltaSolver`1.AdaDeltaPreSolve">
            <summary>
            Runs the AdaDleta pre-solve which parpares the Solver to start Solving.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.AdaDeltaSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the AdaDelta update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="T:MyCaffe.solvers.NesterovSolver`1">
            <summary>
            Use Nesterov's accelerated gradient Solver, which is similar to SGD, but the error gradient is computed on the weights with added momentum.
            </summary>
            <remarks>
            @see [Lecture 6c The momentum method](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) by Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky, 2012.
            @see [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981) by Alexandar Botev, Guy Lever, and David Barber, 2016.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="M:MyCaffe.solvers.NesterovSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The NesterovSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.NesterovSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the Nesterov update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="T:MyCaffe.solvers.SGDSolver`1">
            <summary>
            Use Stochastic Gradient Descent solver with momentum updates weights by a linear combination of the negative gradient and the previous weight update.
            </summary>
            <remarks>
            @see [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) Wikipedia.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.solvers.SGDSolver`1.m_colHistory">
            <summary>
            History maintains the historical momentum data.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.SGDSolver`1.m_colTemp">
            <summary>
            Temp maintains other information that might be needed in computation
            of gradients/updates and is not needed in snapshots.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The SGDSolver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.dispose">
            <summary>
            Releases all resources (GPU and Host) used by the Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.SGDSolver`1.history">
            <summary>
            Returns the history BlobCollection containing historical momentum data.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.PreSolve">
            <summary>
            Runs the pre-solve which parpares the Solver to start Solving.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.GetLearningRate(System.Int32)">
            <summary>
            Return the current learning rate. 
            </summary>
            <remarks>
            The currently implemented learning rate policies are as follows:
               - fixed: always return @f$ base_lr @f$.
               - step: return @f$ base_lr * gamma ^ {floor{iter / step}} @f$
               - exp: return @f$ base_lr * gamma ^ iter @f$
               - inv: return @f$ base_lr * {1 + gamma * iter} ^ {-power} @f$
               - multistep: similar to step but it allows non-uniform steps defined by stepvalue.
               - poly: the effective learning rate follows a polynomial decay, to be
                       zero by the max_iter.  return @f$ base_lr * {1 - iter/max_iter} ^ {power} @f$
               - sigmoid: the effective learning rate follows a sigmoid decay.
                       return @f$ base_lr * {1/{1 + exp{-gamma * {iter - stepsize}}}} @f$
                       
            where base_lr, max_iter, gamma, step, stepvalue and power are defined int the
            solver protocol buffer, and iter is the current iteration.
            </remarks>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
            <returns>The learning rate value.</returns>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.ApplyUpdate(System.Int32)">
            <summary>
            Compute the update values and apply them to the training Net.
            </summary>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
            <returns>The learning rate used is returned.</returns>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.RestoreSolverState(System.Byte[])">
            <summary>
            Restore the state of the Solver.
            </summary>
            <param name="rgState">Specifies the state of the Solver.</param>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.SnapshotSolverState">
            <summary>
            Take a snapshot of the Solver state.
            </summary>
            <returns>The Solver state snapshot is returned.</returns>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.Normalize(System.Int32)">
            <summary>
            Normalize a learnable Blob of the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.Regularize(System.Int32)">
            <summary>
            Regularize a learnable Blob of the training net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.ComputeUpdateValue(System.Int32,System.Double,System.Int32)">
            <summary>
            Compute the SGD update value that will be applied to a learnable blobs in the training Net.
            </summary>
            <param name="param_id">Specifies the id of the Blob.</param>
            <param name="dfRate">Specifies the learning rate.</param>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="M:MyCaffe.solvers.SGDSolver`1.ClipGradients">
            <summary>
            Clip the gradients of all learnable blobs in the training Net.
            </summary>
        </member>
        <member name="T:MyCaffe.solvers.Solver`1">
            <summary>
            An interface for classes that perform optimization on Nets
            </summary>
            <remarks>
            Requires implementation of ApplyUpdate to compute a parameter update
            given the current state of the Net parameters.
            </remarks>
            <typeparam name="T">Specifies the base type <i>float</i> or <i>double</i>.  Using <i>float</i> is recommended to conserve GPU memory.</typeparam>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_cuda">
            <summary>
            Specifies the instance of CudaDnn used by the Solver that provides a connection to Cuda.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_log">
            <summary>
            Specifies the Log for output.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_param">
            <summary>
            Specifies the SolverParameter that defines how the Solver operates.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_net">
            <summary>
            Specifies the training Net.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_rgTestNets">
            <summary>
            Specifies the testing Nets.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_nIter">
            <summary>
            Specifies the current iteration.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_nCurrentStep">
            <summary>
            Specifies the current step.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_rgLosses">
            <summary>
            Specifies the Losses used to calculate the smoothed Loss.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_nSolverCount">
            <summary>
            Specifies the Solver count in a multi-GPU training session.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_nSolverRank">
            <summary>
            Specifies the Solver rank of this solver, where rank == 0 is the root Solver.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_persist">
            <summary>
            Specifies the persistance object used to save weight and solver states.
            </summary>
        </member>
        <member name="F:MyCaffe.solvers.Solver`1.m_dfLearningRateOverride">
            <summary>
            Optionally, specifies a learning rate override (default = 0, which ignores this setting).
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnStart">
            <summary>
            The OnStart event fires at the start of each training iteration.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnAborted">
            <summary>
            The OnAborted event fires after aborting a training cycle.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnGradientsReady">
            <summary>
            The OnGradientsReady event fires after the gradients of a Solver are ready for distribution to other Solvers in a multi-GPU training session.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnSnapshot">
            <summary>
            The OnSnapshot event fires when the Solver detects that a snapshot is needed.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnTrainingIteration">
            <summary>
            The OnTrainingIteration event fires at the end of each training iteration.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnTestingIteration">
            <summary>
            The OnTestingIteration event fires at the end of each testing iteration.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnTest">
            <summary>
            When specifies, the OnTest event fires during a TestAll and overrides the call to Test.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnTestStart">
            <summary>
            The OnTestStart event fires at the start of each testing iteration.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnCustomForwardBack">
            <summary>
            The OnCustomForwardBack allows for overriding the forward/backward operations within
            the solver.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnGetWorkspace">
            <summary>
            Specifies the OnGetWorkspace event that fires when the getWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="E:MyCaffe.solvers.Solver`1.OnSetWorkspace">
            <summary>
            Specifies the OnSetWorkspace event that fires when the setWorkspace() function is called by a layer to get a shareable workspace to conserve GPU memory.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.#ctor(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            The Solver constructor.
            </summary>
            <param name="cuda">Specifies the instance of CudaDnn to use.</param>
            <param name="log">Specifies the Log for output.</param>
            <param name="p">Specifies teh SolverParameter.</param>
            <param name="evtCancel">Specifies a CancelEvent used to cancel the current operation (e.g. training, testing) for which the Solver is performing.</param>
            <param name="evtForceSnapshot">Specifies an automatic reset event that causes the Solver to perform a Snapshot when set.</param>
            <param name="evtForceTest">Specifies an automatic reset event that causes teh Solver to run a testing cycle when set.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPU session.</param>
            <param name="nSolverRank">Specifies the rank of this Solver in a multi-GPU session.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Dispose">
            <summary>
            Discards the resources (GPU and Host) used by this Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.LearningRateOverride">
            <summary>
            Get/set the learning rate override.  When 0, this setting is ignored.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.ForceOnTrainingIterationEvent">
            <summary>
            Force an OnTrainingIterationEvent to fire.
            </summary>
            <returns>When fired, <i>true</i> is returned, otherwise <i>false</i>.</returns>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TrainingTimeLimitInMinutes">
            <summary>
            Get/set the training time limit in minutes.  When set to 0, no time limit is imposed on training.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.SnapshotWeightUpdateMethod">
            <summary>
            Get/set the snapshot weight update method.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.Database">
            <summary>
            Returns the CaffeImageDatabase used.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.dispose">
            <summary>
            Override that allows discarding of resources (GPU and Host) used by this Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableTesting">
            <summary>
            When enabled, the training cycle calls TestAll periodically based on the SolverParameter.  Otherwise testing is not performed.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableBlobDebugging">
            <summary>
            When enabled, the OnTrainingIteration event is set extra debugging information describing the state of each Blob used by the Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableLayerDebugging">
            <summary>
            Enable/disable layer debugging which causes each layer to check for NAN/INF on each forward/backward pass and throw an exception when found.
            </summary>
            <remarks>
            This option dramatically slows down training and is only recommended during debugging.
            </remarks>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableBreakOnFirstNaN">
            <summary>
            When enabled (requires EnableBlobDebugging = <i>true</i>), the Solver immediately stop training upon detecting the first NaN withing the training Net.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableDetailedNanDetection">
            <summary>
            When enabled (requires EnableBlobDebugging = <i>true</i>), the detailed Nan (and Infinity) detection is perofmed on each blob when training Net.
            </summary>
            <remarks>
            IMPORTANT: The use of this setting is only recommended on TCC enabled drivers.
            @see [NVIDIA Tesla Compute Cluster (TCC) Help](http://docs.nvidia.com/gameworks/content/developertools/desktop/tesla_compute_cluster.htm)
            </remarks>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.EnableSingleStep">
            <summary>
            When enabled (requires EnableBlobDebugging = <i>true</i>), the Solver only runs one training cycle.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.WeightsUpdated">
            <summary>
            Get/set when the weights have been updated.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.Tag">
            <summary>
            Returns a generic tag associated with the Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TestingNet">
            <summary>
            Returns the testing Net used by the solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TrainingNet">
            <summary>
            Returns the training Net used by the solver.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Init(MyCaffe.param.SolverParameter,MyCaffe.common.Net{`0})">
            <summary>
            Initializes the Solver.
            </summary>
            <param name="p">Specifies the SolverParameters used to initialize the Solver.</param>
            <param name="shareNet">Optionally, specifies a network to share (default = null).</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Reset">
            <summary>
            Reset the iterations of the net.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.InitTrainNet(MyCaffe.common.Net{`0})">
            <summary>
            Initializes the Net used by the solver for training.
            </summary>
            <param name="shareNet">Optionally, specifies a network to share (default = null).</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.InitTestNets">
            <summary>
            Initializes the Net used by the Solver for testing.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.Cuda">
            <summary>
            Returns the CudaDnn instance used by the Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.ActiveLabelCounts">
            <summary>
            Returns a string describing the labels detected in the training along with the % that each label has participated in the training.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.LabelQueryHitPercents">
            <summary>
            Return the label query hit percentages for the active datasource.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.LabelQueryEpochs">
            <summary>
            Return the label query epochs for the active datasource.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.CurrentIteration">
            <summary>
            Returns the current training iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.MaximumIteration">
            <summary>
            Returns the maximum training iterations.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TrainingIterations">
            <summary>
            Returns the current training iterations remaining.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TestingIterations">
            <summary>
            Returns the current testing iterations remaining.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Solve(System.Int32,System.Byte[],System.Byte[],MyCaffe.common.TRAIN_STEP)">
            <summary>
            The main entry of the solver function.  In default, iter will be zero.  Pass
            in a non-zero iter number to resume training for a pre-trained net.
            </summary>
            <param name="nIterationOverride">Optionally, specifies an iteration override value to use for the number of iterations run.  The default is -1, which ignores the parameter.</param>
            <param name="rgWeights">Optionally, specifies weights to load via the Restore method.  The default is <i>null</i> which ignores the parameter.</param>
            <param name="rgState">Optionally, specifies the state to load via the Restore method.  The default is <i>null</i> which ignores the parameter.</param>
            <param name="step">Optionally, specifies to single step the training pass - typically this is used during debugging. The default = <i>TRAIN_STEP.NONE</i> which runs the solver in the normal manner.</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Step(System.Int32,MyCaffe.common.TRAIN_STEP,System.Boolean,System.Boolean,System.Boolean,System.Boolean,System.Nullable{System.Double},System.Nullable{System.Boolean})">
            <summary>
            Steps a set of iterations through a training cycle.
            </summary>
            <param name="nIters">Specifies the number of steps to iterate.</param>
            <param name="step">Optionally, specifies to single step the training pass - typically this is used during debugging. The default = <i>TRAIN_STEP.NONE</i> for no stepping.</param>
            <param name="bZeroDiffs">Optionally, specifies whether or not to zero out the gradient diffs (default = <i>true</i>).</param>
            <param name="bApplyUpdates">Optionally, specifies to apply the gradient updates to the weights (default = <i>true</i>).</param>
            <param name="bDisableOutput">Optionally, disable the output to the log.</param>
            <param name="bDisableProgress">Optionally, disables the progress updating to the log.</param>
            <param name="dfLossOverride">Optionally, specifies a loss override which can be useful when using a backward step only.</param>
            <param name="bAllowSnapshot">Optionally, specifies whether or not a snapshot is allowed even during TRAIN_STEP.</param>
            <returns></returns>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Restore(System.Byte[],System.Byte[],System.String)">
            <summary>
            The restore method simply calls the RestoreSolverState method of the inherited class.
            </summary>
            <param name="rgWeights">Specifies the weights to load, or <i>null</i> to ignore.</param>
            <param name="rgState">Specifies the state to load, or <i>null</i> to ignore.</param>
            <param name="strSkipBlobTypes">Specifies the blob types to ignore and not load, or <i>null</i> to ignore.</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Snapshot(System.Boolean,System.Boolean)">
            <summary>
            The snapshot function implements the basic snapshotting utility that stores the
            learned net.  This method calls the SnapshotSolverState method of the inherited class.
            </summary>
            <param name="bForced">Specifies whehter or not to force the snapshot.</param>
            <param name="bScheduled">Specifies whether or not the snapshot is a scheduled snapshot that occurs at regular intervals, or a snapshot based on an improving accuracy.</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.GetSnapshotArgs(System.Byte[],System.Byte[],System.Double,System.Double,System.Int32,MyCaffe.basecode.SNAPSHOT_WEIGHT_UPDATE_METHOD)">
            <summary>
            The GetSnapshotArgs method fills out a snapshot args structure.
            </summary>
            <param name="rgState">Specifies the state bytes or null.</param>
            <param name="rgWeights">Specifies the weight bytes or null.</param>
            <param name="dfAccuracy">Specifies the accuracy.</param>
            <param name="dfError">Specifies the error.</param>
            <param name="nIteration">Specifies the interation.</param>
            <param name="wtUpdt">Specifies the weight update method.</param>
            <returns>The args are returned.</returns>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TrainingIterationOverride">
            <summary>
            Get/set the training iteration override.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.TestingIterationOverride">
            <summary>
            Get/set the testing iteration override.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.CompletedEvent">
            <summary>
            Returns an auto reset event that is set upon training completion.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.CancelEvent">
            <summary>
            Returns the cancel event which when set cancels the current operation run by the Solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.smoothed_loss">
            <summary>
            Returns the smoothed loss.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.parameter">
            <summary>
            Returns the SolverParameter used.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.net">
            <summary>
            Returns the main training Net.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.test_nets">
            <summary>
            Returns the testing Nets.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.iter">
            <summary>
            Returns the current training iteration.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.type">
            <summary>
            Returns the type of solver.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.forceSnapshot">
            <summary>
            Returns whether or not a snapshot has been forced.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.forceTest">
            <summary>
            Returns whether or not a test has been forced.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.solver_count">
            <summary>
            Returns the solver count in a multi-GPU session.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.solver_rank">
            <summary>
            Returns this Solver's rank in a multi-GPU session.
            </summary>
        </member>
        <member name="P:MyCaffe.solvers.Solver`1.is_root_solver">
            <summary>
            Returns whether or not this is the root solver. 
            </summary>
            <remarks>
            The root solver has rank = 0.
            </remarks>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.TestAll(System.Int32)">
            <summary>
            Run a TestAll by running all test Nets.
            </summary>
            <remarks>
            Depending on the eval_type tests are run as Test (default), TestClassification (for SSD Classification),
            or TestDetection (for SSD Detection).
            </remarks>
            <param name="nIterationOverride">Specifies an override to the iterations to run.</param>
            <returns>The accuracy of the testing run is returned as a percentage value in the range [0, 1].</returns>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.TestDetection(System.Int32,System.Int32)">
            <summary>
            Run an SSD detection test on a given test Net by running it through its iterations.
            </summary>
            <param name="nIterationOverride">Specifies an override the the number of iterations to run.</param>
            <param name="nTestNetId">Specifies the ID of the test Net to run.</param>
            <returns>The accuracy of the test run is returned as a percentage in the range [0, 1].</returns>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.TestClassification(System.Int32,System.Int32)">
            <summary>
            Run a test on a given test Net by running it through its iterations.
            </summary>
            <param name="nIterationOverride">Specifies an override the the number of iterations to run.</param>
            <param name="nTestNetId">Specifies the ID of the test Net to run.</param>
            <returns>The accuracy of the test run is returned as a percentage in the range [0, 1].</returns>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.UpdateSmoothedLoss(System.Double,System.Int32,System.Int32)">
            <summary>
            Update the avaraged loss value.
            </summary>
            <param name="dfLoss">Specifies the new loss value to add into the average.</param>
            <param name="nStartIter">Specifies the starting iteration.</param>
            <param name="nAverageLoss">Optionally, specifies the number of iterations to average over (default = param.average_loss).</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.ApplyUpdate(System.Int32)">
            <summary>
            Make and apply the update value for the current iteration.
            </summary>
            <param name="nIterationOverride">Optionally, specifies an iteration override, or -1 which is ignored.</param>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.SnapshotSolverState">
            <summary>
            Save the current solver state.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.RestoreSolverState(System.Byte[])">
            <summary>
            Restore a solver state.
            </summary>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Create(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.basecode.ProjectEx,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            Create a new Solver based on the project containing the SolverParameter.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance that the new Solver will use.</param>
            <param name="log">Specifies the Log for output that the new Solver will use.</param>
            <param name="p">Specifies the project to use in initialzing the new Solver.</param>
            <param name="evtCancel">Specifies the CancelEvent that the new Solver will use.</param>
            <param name="evtForceSnapshot">Specifies the force snapshot event that the new Solver will use.</param>
            <param name="evtForceTest">Specifies the force test event that the new Solver will use.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase that the new Solver will use.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPu session.</param>
            <param name="nSolverRank">Specifies the rank of the new Solver.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
            <returns>A new Solver instance is returned.</returns>
        </member>
        <member name="M:MyCaffe.solvers.Solver`1.Create(MyCaffe.common.CudaDnn{`0},MyCaffe.basecode.Log,MyCaffe.param.SolverParameter,MyCaffe.basecode.CancelEvent,System.Threading.AutoResetEvent,System.Threading.AutoResetEvent,MyCaffe.basecode.IXImageDatabaseBase,MyCaffe.common.IXPersist{`0},System.Int32,System.Int32,MyCaffe.common.Net{`0},MyCaffe.common.onGetWorkspace,MyCaffe.common.onSetWorkspace)">
            <summary>
            Create a new Solver based on the project containing the SolverParameter.
            </summary>
            <param name="cuda">Specifies the CudaDnn instance that the new Solver will use.</param>
            <param name="log">Specifies the Log for output that the new Solver will use.</param>
            <param name="solverParam">Specifies the SolverParameter used to create the Solver.</param>
            <param name="evtCancel">Specifies the CancelEvent that the new Solver will use.</param>
            <param name="evtForceSnapshot">Specifies the force snapshot event that the new Solver will use.</param>
            <param name="evtForceTest">Specifies the force test event that the new Solver will use.</param>
            <param name="imgDb">Specifies the CaffeImageDatabase that the new Solver will use.</param>
            <param name="persist">Specifies the peristence used for loading and saving weights.</param>
            <param name="nSolverCount">Specifies the number of Solvers participating in a multi-GPu session.</param>
            <param name="nSolverRank">Specifies the rank of the new Solver.</param>
            <param name="shareNet">Optionally, specifies the net to share when creating the training network (default = null, meaning no share net is used).</param>
            <param name="getws">Optionally, specifies the handler for getting the workspace.</param>
            <param name="setws">Optionally, specifies the handler for setting the workspace.</param>
            <returns></returns>
        </member>
    </members>
</doc>
